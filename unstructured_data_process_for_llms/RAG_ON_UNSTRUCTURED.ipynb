{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d78a7d9f-d103-4923-85b9-208d505517f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unstructured_client import UnstructuredClient\n",
    "from unstructured_client.models import shared\n",
    "from unstructured_client.models.errors import SDKError\n",
    "\n",
    "from unstructured.chunking.title import chunk_by_title\n",
    "from unstructured.partition.md import partition_md\n",
    "from unstructured.partition.pptx import partition_pptx\n",
    "from unstructured.staging.base import dict_to_elements\n",
    "\n",
    "import chromadb\n",
    "\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b894010-f6d2-4224-9441-f40521c8bace",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DLAI_API_KEY = \"\"\n",
    "DLAI_API_URL = \"\"\n",
    "\n",
    "s = UnstructuredClient(\n",
    "    api_key_auth=DLAI_API_KEY,\n",
    "    server_url=DLAI_API_URL,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56a58239-2da9-4b2c-b871-1e67237abc11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CoT.pdf                                embedded-images-tables.pdf\n",
      "RAG_ON_UNSTRUCTURED.ipynb              image_in_pdf_preprocess.ipynb\n",
      "\u001b[34mchroma_tmp\u001b[m\u001b[m/                            medium_blog.html\n",
      "donut_paper.pdf                        msft_openai.pptx\n",
      "donut_readme.md                        process_unstructured_file_types.ipynb\n",
      "donut_slide.pptx                       unstructured_chromadb.ipynb\n",
      "el_nino.html                           winter-sports.epub\n",
      "el_nino.pdf\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcc10bd0-a1db-4a15-bb0c-88fdf2635e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"donut_paper.pdf\"\n",
    "with open(filename,\"rb\") as f:\n",
    "    files =  shared.Files(\n",
    "        content=f.read(),\n",
    "        file_name=filename\n",
    "    )\n",
    "req = shared.PartitionParameters(\n",
    "    files=files,\n",
    "    strategy=\"hi_res\",\n",
    "    hi_res_model_name=\"yolox\",\n",
    "    pdf_infer_table_structure=True,\n",
    "    skip_infer_table_types=[],\n",
    ")\n",
    "res = s.general.partition(req)\n",
    "pdf_elements= dict_to_elements(res.elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b27004c-4e64-4daf-837e-6399a547785e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<unstructured.documents.elements.NarrativeText at 0x16b408650>,\n",
       " <unstructured.documents.elements.Title at 0x1699aa310>,\n",
       " <unstructured.documents.elements.Title at 0x175ad5610>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_elements[2:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df73dbad-4c75-48f3-96ff-425115a0b6da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<table><tr><td>NAVER CLOVA</td><td>2NAVER Search</td><td>3SNAVER AI Lal</td></tr></table>'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tables = [el for el in pdf_elements if el.category=='Table']\n",
    "tables[0].metadata.text_as_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec7df2ff-b1f9-4748-9f60-c3089dffb3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_title = [\n",
    "    el for el in pdf_elements\n",
    "    if el.text == \"References\"\n",
    "    and el.category == \"Title\"\n",
    "][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "709cb2da-ea70-4f31-8705-723d48bcb7a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Afzal, M.Z., Capobianco, S., Malik, M.I., Marinai, S., Breuel, T.M., classification with Dengel, A., Liwicki, M.: Deepdocclassifier: Document deep convolutional neural network. In: 2015 13th International Conference on Document Analysis and Recognition (ICDAR). pp. 1111–1115 (2015). https://doi.org/10.1109/ICDAR.2015.7333933 1, 4, 14\n"
     ]
    }
   ],
   "source": [
    "reference_title.to_dict()\n",
    "references_id = reference_title.id\n",
    "\n",
    "for element in pdf_elements:\n",
    "    if element.metadata.parent_id == references_id:\n",
    "        print(element)\n",
    "        break\n",
    "\n",
    "pdf_elements = [el for el in pdf_elements if el.metadata.parent_id != references_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e84fae25-f064-43f7-81a5-202aca07c298",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = [el for el in pdf_elements if el.category == \"Header\"]\n",
    "pdf_elements = [el for el in pdf_elements if el.category != \"Header\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ab82800-6c2d-4928-977e-4dcf1812ef04",
   "metadata": {},
   "outputs": [],
   "source": [
    "###PPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e2d4da5e-7b83-4bfb-aaa7-2d34416c526b",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"donut_slide.pptx\"\n",
    "pptx_elements = partition_pptx(filename=filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec9919b0-370e-48b4-966c-116617529aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Reading document from string ...\n",
      "INFO: Reading document ...\n",
      "INFO: HTML element instance has no attribute type\n",
      "INFO: HTML element instance has no attribute type\n",
      "INFO: HTML element instance has no attribute type\n",
      "INFO: HTML element instance has no attribute type\n",
      "INFO: HTML element instance has no attribute type\n",
      "INFO: HTML element instance has no attribute type\n",
      "INFO: HTML element instance has no attribute type\n",
      "INFO: HTML element instance has no attribute type\n",
      "INFO: HTML element instance has no attribute type\n",
      "INFO: HTML element instance has no attribute type\n",
      "INFO: HTML element instance has no attribute type\n",
      "INFO: HTML element instance has no attribute type\n",
      "INFO: HTML element instance has no attribute type\n",
      "INFO: HTML element instance has no attribute type\n",
      "INFO: HTML element instance has no attribute type\n",
      "INFO: HTML element instance has no attribute type\n",
      "INFO: HTML element instance has no attribute type\n",
      "INFO: HTML element instance has no attribute type\n",
      "INFO: HTML element instance has no attribute type\n",
      "INFO: HTML element instance has no attribute type\n",
      "INFO: HTML element instance has no attribute type\n",
      "INFO: HTML element instance has no attribute type\n",
      "INFO: HTML element instance has no attribute type\n",
      "INFO: HTML element instance has no attribute type\n",
      "INFO: HTML element instance has no attribute type\n",
      "INFO: HTML element instance has no attribute type\n"
     ]
    }
   ],
   "source": [
    "filename = \"donut_readme.md\"\n",
    "md_elements = partition_md(filename=filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "af000169-4efc-48d4-96ee-f7452ff78bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "elements = chunk_by_title(pdf_elements + pptx_elements + md_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "99fc1d6e-e603-451b-ace5-d346b713fd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "for element in elements:\n",
    "    metadata= element.metadata.to_dict()\n",
    "    del metadata[\"languages\"]\n",
    "    metadata['source'] = metadata['filename']\n",
    "    documents.append(Document(page_content=element.text,metadata=metadata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "412c5094-4de6-482f-8010-e30022ba50bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: OPENAI_API_KEY=\n"
     ]
    }
   ],
   "source": [
    "%set_env OPENAI_API_KEY="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1df837af-a139-4d37-9bf0-3c296272e4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3b3859c1-96e0-4a5c-8c2c-674eea9784d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='OCR-free Document Understanding Transformer\\n\\n2 2 0 2\\n\\nGeewook Kim1∗, Teakgyu Hong4†, Moonbin Yim2†, Jeongyeon Nam1, Jinyoung Park5†, Jinyeong Yim6†, Wonseok Hwang7†, Sangdoo Yun3, Dongyoon Han3, and Seunghyun Park1\\n\\nt c O 6\\n\\n1NAVER CLOVA 4Upstage\\n\\n3NAVER AI Lab', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 1, 'orig_elements': 'eJzVVF1PFTEQ/SvNPl+1X9ttfSNoREQwcMEQJGTazq4b7rabvbvqlfjfnUUwxGCCTze+NOmZMx9nZtqLmwJX2GEar9pYvGSFKWUoQXPhnNYeldRYaQ9OCGu9FqJYsKLDESKMQPybom5XmKDD2TnmNI1XPfQ4PO9jPXNn87jpb83Q96s2wNjm9OLOvILUTNDgmuwXBaamuCS0J+QqTZ3HgXDxg6ARv41zjKPd42f1gMhe5TDNdbPTFHFYj5Bimxq2HCCt6zx05Dq73aVetuMKCwr0p17rVOViLI01wULQUgmrRFnZ0inqhd2G3ltk+IeZPGyQZJJxJh+KP01UBDZ5aL9jXM68RxohXBU4N0ogVLzmwVQViMo5W1tNnaj/u0a8Qfya8zV713bi0ySlqBZsiXDdbCa2l1OjCeSSL9j7nJNvEztvO3mP7SMxNnSwQ+iojv02bfJE+/UBhuvyN4vQmTi7mnvwY05rpLx7X0lidY+e0CXmzM6npBa0vBSd0rI9mK+0u+wEKfznzZRuU4iH8zuEYaDmfcG/zS4665Abj9yGUIcyaF6byAGDtKRbb/vRjiywI2ae9CBRld575VXUoABoIW0VYvTRADeVg21rEYc7Z6+P2e7B0dkO06c9/TwNPkmZ5kq5IOsylCZExem/AScrp2yAYLzZtjL1S9nOW3YA/hFFlz8BvgnrGg==', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='2NAVER Search 5Tmax 6Google 7LBox', metadata={'text_as_html': '<table><tr><td>NAVER CLOVA</td><td>2NAVER Search</td><td>3SNAVER AI Lal</td></tr></table>', 'filetype': 'application/pdf', 'page_number': 1, 'filename': 'donut_paper.pdf', 'orig_elements': 'eJxVkN1qwzAMhV/F+Hosaf4zQiEbYwzCBmvpTSlBsZU0YDsmdSGj9N1nJ2FjF7rQd3SOhI43igIlKlP3nD4RylLwo7DBCHwecR4l6G+ixkdMWJvFbU4fCJVogIMBO3+jbS9QgURn5oO6mlqDxvFR89bNOtl861kGrUXPwPSD8lZZgOqu0OHF6keKqqMnS7UltbrKBkfLNzMZ/26M/DDMWdDGLE4YD32WAeRBmocZA5Y0iQs2OJkaLvXZSOE8hYFG4LYwoy2+/SgPr1/kpfo8lIVne8eCBe4QRnb+peFuweU7qUAs2HMp3pJI7+s2t+VfBIn3EiaSvA1DJ5Ck1fMwzaet79g7P72ffgBIe3g4', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content=']\\n\\nG L . s c [\\n\\n5 v 4 6 6 5 1 . 1 1 1 2 : v i X r a', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 1, 'orig_elements': 'eJzNUdFKAzEQ/JUlz1Lvktwl1x/wxccKQillL9kcgbs0XFNRi//uXlEsIvio7NvMLDM7uz0LGmmiVPbRizUIF5Aqr6W2wdRaBtPXJljZOm+wqTspbkBMVNBjQdafRYgjJZxoWfaHdCr7jJnmVfZh0S50eckXGnMeo8MSD+n2gx4xDScc6Mj8VlAaxI7RzMg+naaeZsbrCzJ/ZdSVUp2ToXEN51KVs4idNJ2yDl3bt+KNNwo9l0W8W2w+EzwktqfhMMdX8ptFwdLvFfiAXmkdArFFT061CtGSMi11rjLVX1RwfdEd3MMKjuBYfnXbJpaRfrwHa7SKSwpSVtSg18ZK7Izhnxqy9j+89NfOrwto4Ak0tDwN1FxFfRkJa8YjPMIM+MvTd+/8u+kH', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='Abstract. Understanding document images (e.g., invoices) is a core but challenging task since it requires complex functions such as reading text and a holistic understanding of the document. Current Visual Document Understanding (VDU) methods outsource the task of reading text to off- the-shelf Optical Character Recognition (OCR) engines and focus on the understanding task with the OCR outputs. Although such OCR-based approaches have shown promising performance, they suffer from 1) high', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 1, 'orig_elements': 'eJxlVctu20gQ/JWGThYgUS/Hj+RkOJe9bIDAySUIjOawKc4uOcOdh2whyL9v9TCSV9mjpl9VXdXUtx8z6WUQl55tM3tPs7vWMF9vNu9u17vN/fr+pt3ypjbbbSM3d82aZwuaDZK44cTI/zFrbS+OB9HixrucnkceJVRj02quhtNxLGEex94aTta71a9wz26feS8R8W8zcfvZd7yOeHl2eagl4H1TXsIbxqblZnd93baybd/VYnY3O+Y72d3eyL1Z365nP1GR5DVp8kMdU2CTKvriGgkxsWus21PjTVbeZAedT1dS7asFWXfw1kick43EZHwQqnMi03EPonstTRz/pmidEbKJgvyTbUAH44exl1dqszPKMVLMpiOOSOEyUzER5qNx53sbkzWUL1D5llInZ3AVPeagzOmrjZl7+nhCfUnm6uvHL3OCLp1vIvmcos8B8LRXQYu+FyCSx1O71IRl7KRv6dMINJjw2LGuSwJ9FuP3zioVuvr0+HlOhT+YKoUWSDDKlRmXJMrEF5u6EkOlIhoBqqKHHhDzvptWg9Cy5ihYyDgGz6ZD844PQrHzL47wNtioLeGo1oeBsfOFdj2iAfQP1CKFNnPqLJqqAjkVg4GJ8TFF4AyUSw9M+0BbCOtayGRr29t01M0owsE30hc+Z0sSKtW6UXNOisQPtJuXCgkBCYAIt5aRulQlHHO9pAhXqE6Iw01g/uSJmwY+iZoUYZ0Ys0R1HB5gtnI2C3pBxKXgmwz9mJw/SF8W1QYRgs4TVNKba+AHXByKOot1FgEmxmef/KZMYBd1kThQeihIqLWIo1ZGhXIxCWiFg+kKKmXqo2KKVo1OGrFJTMq4kStbCc7n6W3AfLIAo1CWGGxdMXj9Fyrs4Vxhgo9xKUp5PFKPH/NqoqUXaDwUHxOs3x9Pc4+SSODddmqE1XaheArGFhe1t7xil7boNd0b/HCMuu2XyVtvLP6/2sVpq/CjlQP0x/6SLH271HPhkP5rx+KZAwfrcQ7aQ90fi6oShuKd2mMPcRT1uYIxJuPEjhX94dQT5cIKMl8sDWhHh0n6ddDPLO3FSeCkduw4Ec51nKSbnHCxX3iwhqbF4L1a6YztzdcKovEDKtSX6GPQZ0GlBzD+WoJm/QYEH2HiA9uetbdCSWmM71erPZTOdYX7W5neH5jtqvwXVPqNP33+/+QAFpDnST/NP7//C/IBSnE=', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='computational costs for using OCR; 2) inflexibility of OCR models on languages or types of documents; 3) OCR error propagation to the sub- sequent process. To address these issues, in this paper, we introduce a novel OCR-free VDU model named Donut, which stands for Document understanding transformer. As the first step in OCR-free VDU research, we propose a simple architecture (i.e., Transformer) with a pre-training objective (i.e., cross-entropy loss). Donut is conceptually simple yet ef-', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 1, 'orig_elements': 'eJxlVctu20gQ/JWGThYgUS/Hj+RkOJe9bIDAySUIjOawKc4uOcOdh2whyL9v9TCSV9mjpl9VXdXUtx8z6WUQl55tM3tPs7vWMF9vNu9u17vN/fr+pt3ypjbbbSM3d82aZwuaDZK44cTI/zFrbS+OB9HixrucnkceJVRj02quhtNxLGEex94aTta71a9wz26feS8R8W8zcfvZd7yOeHl2eagl4H1TXsIbxqblZnd93baybd/VYnY3O+Y72d3eyL1Z365nP1GR5DVp8kMdU2CTKvriGgkxsWus21PjTVbeZAedT1dS7asFWXfw1kick43EZHwQqnMi03EPonstTRz/pmidEbKJgvyTbUAH44exl1dqszPKMVLMpiOOSOEyUzER5qNx53sbkzWUL1D5llInZ3AVPeagzOmrjZl7+nhCfUnm6uvHL3OCLp1vIvmcos8B8LRXQYu+FyCSx1O71IRl7KRv6dMINJjw2LGuSwJ9FuP3zioVuvr0+HlOhT+YKoUWSDDKlRmXJMrEF5u6EkOlIhoBqqKHHhDzvptWg9Cy5ihYyDgGz6ZD844PQrHzL47wNtioLeGo1oeBsfOFdj2iAfQP1CKFNnPqLJqqAjkVg4GJ8TFF4AyUSw9M+0BbCOtayGRr29t01M0owsE30hc+Z0sSKtW6UXNOisQPtJuXCgkBCYAIt5aRulQlHHO9pAhXqE6Iw01g/uSJmwY+iZoUYZ0Ys0R1HB5gtnI2C3pBxKXgmwz9mJw/SF8W1QYRgs4TVNKba+AHXByKOot1FgEmxmef/KZMYBd1kThQeihIqLWIo1ZGhXIxCWiFg+kKKmXqo2KKVo1OGrFJTMq4kStbCc7n6W3AfLIAo1CWGGxdMXj9Fyrs4Vxhgo9xKUp5PFKPH/NqoqUXaDwUHxOs3x9Pc4+SSODddmqE1XaheArGFhe1t7xil7boNd0b/HCMuu2XyVtvLP6/2sVpq/CjlQP0x/6SLH271HPhkP5rx+KZAwfrcQ7aQ90fi6oShuKd2mMPcRT1uYIxJuPEjhX94dQT5cIKMl8sDWhHh0n6ddDPLO3FSeCkduw4Ec51nKSbnHCxX3iwhqbF4L1a6YztzdcKovEDKtSX6GPQZ0GlBzD+WoJm/QYEH2HiA9uetbdCSWmM71erPZTOdYX7W5neH5jtqvwXVPqNP33+/+QAFpDnST/NP7//C/IBSnE=', 'is_continuation': True, 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='fective. Through extensive experiments and analyses, we show a simple OCR-free VDU model, Donut, achieves state-of-the-art performances on various VDU tasks in terms of both speed and accuracy. In addition, we offer a synthetic data generator that helps the model pre-training to be flexible in various languages and domains. The code, trained model, and synthetic data are available at https://github.com/clovaai/donut.', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 1, 'orig_elements': 'eJxlVctu20gQ/JWGThYgUS/Hj+RkOJe9bIDAySUIjOawKc4uOcOdh2whyL9v9TCSV9mjpl9VXdXUtx8z6WUQl55tM3tPs7vWMF9vNu9u17vN/fr+pt3ypjbbbSM3d82aZwuaDZK44cTI/zFrbS+OB9HixrucnkceJVRj02quhtNxLGEex94aTta71a9wz26feS8R8W8zcfvZd7yOeHl2eagl4H1TXsIbxqblZnd93baybd/VYnY3O+Y72d3eyL1Z365nP1GR5DVp8kMdU2CTKvriGgkxsWus21PjTVbeZAedT1dS7asFWXfw1kick43EZHwQqnMi03EPonstTRz/pmidEbKJgvyTbUAH44exl1dqszPKMVLMpiOOSOEyUzER5qNx53sbkzWUL1D5llInZ3AVPeagzOmrjZl7+nhCfUnm6uvHL3OCLp1vIvmcos8B8LRXQYu+FyCSx1O71IRl7KRv6dMINJjw2LGuSwJ9FuP3zioVuvr0+HlOhT+YKoUWSDDKlRmXJMrEF5u6EkOlIhoBqqKHHhDzvptWg9Cy5ihYyDgGz6ZD844PQrHzL47wNtioLeGo1oeBsfOFdj2iAfQP1CKFNnPqLJqqAjkVg4GJ8TFF4AyUSw9M+0BbCOtayGRr29t01M0owsE30hc+Z0sSKtW6UXNOisQPtJuXCgkBCYAIt5aRulQlHHO9pAhXqE6Iw01g/uSJmwY+iZoUYZ0Ys0R1HB5gtnI2C3pBxKXgmwz9mJw/SF8W1QYRgs4TVNKba+AHXByKOot1FgEmxmef/KZMYBd1kThQeihIqLWIo1ZGhXIxCWiFg+kKKmXqo2KKVo1OGrFJTMq4kStbCc7n6W3AfLIAo1CWGGxdMXj9Fyrs4Vxhgo9xKUp5PFKPH/NqoqUXaDwUHxOs3x9Pc4+SSODddmqE1XaheArGFhe1t7xil7boNd0b/HCMuu2XyVtvLP6/2sVpq/CjlQP0x/6SLH271HPhkP5rx+KZAwfrcQ7aQ90fi6oShuKd2mMPcRT1uYIxJuPEjhX94dQT5cIKMl8sDWhHh0n6ddDPLO3FSeCkduw4Ec51nKSbnHCxX3iwhqbF4L1a6YztzdcKovEDKtSX6GPQZ0GlBzD+WoJm/QYEH2HiA9uetbdCSWmM71erPZTOdYX7W5neH5jtqvwXVPqNP33+/+QAFpDnST/NP7//C/IBSnE=', 'is_continuation': True, 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='Keywords: Visual Document Understanding, Document Information Extraction, Optical Character Recognition, End-to-End Transformer', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 1, 'orig_elements': 'eJxFj8FqAjEQhl8l5Ly20dVd9dp6KIUWiu1FRMbMZBvYnYRstlXEd2+yFTwlfP8/wze7i6SWOuJ4sCjXQtKs1qhIVYvlFJagFvMSqlqtAOtSzwzKQsiOIiBESP2LNLYlho7yMDoe4sGDp/Dg0eRujuPZjzF431oN0Tp+vMUtcDNAQ33Kd5K4kftEfSIHHrojhcSnIwl3RzSA5XxuDM3M4ki6rEqAJZV1RSutaiWvaSLSKebyK51/XcB+Lb5sP0Arnp0e8sHik5FCH4HRclPc+QsbF7pRU2xOMYDO30K8+5jsW/H0DZlREB+kXcP2P94wTqKbpEdsA3CflyT/rHK7/w1CSGt/aJvdrvs/YPd+jw==', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='1 Introduction\\n\\nDocument images, such as commercial invoices, receipts, and business cards, are easy to find in modern working environments. To extract useful informa- tion from such document images, Visual Document Understanding (VDU) has not been only an essential task for industry, but also a challenging topic for re- searchers, with applications including document classification [27,1], information extraction [22,42], and visual question answering [44,57].', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 1, 'orig_elements': 'eJzFUj1v2zAQ/SsHTQ2guJYsVXbnLFk6OVkMwziRR5kIRar8sGsE+e89ym4aZOvUTXj3+D7utHstyNBINh60LL5DoUhVrdwIKZZN3S+bql7V2FedorXqvq26ooRipIgSIzL/tVDakMWR8mPpbIqHCSfyi0mqzM3jeJnmMU6T0QKjdvbrbWzQDgkHCjzfFWSHYs/oxMjBprEnz3j1xlCkXzFrVPBoo3cyiSyTJf6ob3U0VDD3c6V6IxvqVN2s601bCazWbb3q18u242JKbP5HpRnx/7D2jzt4cCLlfqDH7FNCSOIIGEC4cSQvNBrQ9uS0yENPgvQU+QuthD4FbSkwF73MmCcgDBeIDpRmgrYwOknewtn5F20HIHvS3tnsGBawdcAxPIoIKZBK2Uo5P+I95C2A8m68BpKfYz7rkDjae/wnyz4hcqxs8+X54ekOjlzDugg9kQVnzYVTA+dlfq4VMbwA27GpTCH6S8mNIqAJDhDEEQ2fbshy0U1azFRP9xAIvTiyWwlnHXlZf+8WWEuYNGd4jywMhqDVjQK7uisrPuOt6ozdtnAd12VT768bPl1b/kwU5iHacCaf1XdNU7bdfvHxr/2B3rPeibb5uG/730GEM5M=', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='∗ Corresponding author: gwkim.rsrch@gmail.com † This work was done while the authors were at NAVER CLOVA.\\n\\n2  G. Kim et al. ', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 1, 'orig_elements': 'eJzFkN1KxDAUhF/lkGupbfqTdq8UERFFQWRvdFnOJqdtsElDmrqK7LubiuLiC3iZmTNhvnn6YDSQIRu2WrEVsKKseEZtVjR13tZtkwkumqwpq6qgUqFkJ8AMBVQYMN5/sFYPZNHQElajncPWoSOfONUut4sd3t2Xjc4NWmLQoz39tge03YwdTdF/YmQ7tomqi8rWzmZHPurZl+J/O7axX6kaqWRa8F1aZDznuMtES3UrqlywQ0wEegvL8fPMeSbgYvSeJjdapW0HOId+9Cvo9i/aJH7ysj/rDOohkaOBGEl5Co+9nmA/+hfY4wSRjWDfRxwIPX3/EH3y8RHg7nx9+QAXt/fr82QB+2G+Q+8j8Ss9Ln1isb+DV6mMU9cFr6SsUAiel1jnSsmqLmsl6n8YnB/vxwGuErjRBigADgkc012bGGSHzSc657bR', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='Fig. 1. The schema of the conventional document information extraction (IE) pipeline. (a) The goal is to extract the structured information from a given semi- structured document image. In the pipeline, (b) text detection is conducted to obtain text locations and (c) each box is passed to the recognizer to comprehend characters. (d) Finally, the recognized texts and its locations are passed to the following module to be processed for the desired structured form of the information ', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 2, 'orig_elements': 'eJxdUtFO4zAQ/JWVnxqpF2iPXFXeD6nvvCFUbexNYsn2WrZzwCH+/dYJFeWekszOzngmfnpX5MhTKGdr1D2ou2OnjzvTHe76vtsfu0Nnhn4/3P7c3+7oV3dQW1CeChosKPx3NVhHAT3VZcNhLueIkVIbzVC5dVze4jLGGJ3VWCyHm8+xwzDOOFKW+ZOiMKpnQaMg5zD7npLg+w+BCr2WqvFgxxZ2LTxOBFlP5BF4gCJfmsMfySHi6MCwnmsqsGHg5BdPEImEenndnH43EG0kZwO1sMFmURxZdm2GwhfyIp1LmnWZE5lvekNiDwijFV/I5O2Pa+bXEbzEaeEUFq2L6RY2fQM1FhgqtB5LrCWGEQkRkENwX9CGleV4rS4DBgMb3QChnqDn17oWMed1p5ok0jwG+5dSRTT7mGgiWdMT1lSUsoQ2DTxYacu9bb9vmcVxNbLyvLJO9J/VwM7xiw0jeDazo4r3QkqsaeFJXwvRULa1l6uKapWX33ddbL0Zl0tzqu2pj+d/3xXsdA==', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='(a) Pipeline Overview.\\n\\nFig. 2. The pipeline overview and benchmarks. The proposed end-to-end model, Donut, outperforms the recent OCR-dependent VDU models in memory, time cost and accuracy. Performances on visual document IE [45] are shown in (b). More results on various VDU tasks are available at Section 3 showing the same trend\\n\\n(b) System Benchmarks.', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 2, 'orig_elements': 'eJzFUktrGzEQ/ivDnhKwN9633WOTFnooCU3bizFmVppdi6we6OHUhP73SmuHQg+BnoIOkmbm+2a+mdm+ZDSRJOX3gmcfIGvbjur1pqGyqKmJh5VtXW76dt321YpW2QIySR45eozxL9kgJlIoKYG5VsHvDRqyueFDik1ufzKzG42ZBEMvtLq5uCdUY8CRXPRvM1JjtotWEy17FWRPNtrL39Hk6ZdPHFd4DQ/C0CQUwf2R7FHQc56oXrN8FmOwdIsm5cki9l+JHWP9uhtw2NRlUdUDYdkUjKjveVNVbfPeEqOAHMocvh8IzKtUfZEKqDj0pNhBon1ylyirjXbEgRRfer2MF0jNaVrAXap3ATr4WPGgrXTgI8ISiw2B+9tvS04mxqffz7sfZ5gDoUCS1Pa0AC8kAdPOz6mRsWCRnXJ4OPOhYuRAKzgKF3ACrllIzYYvn2BbNztAS+AO+lkl0qv+Ooev2qYKXJj8GYlW6ODm/B7dk5sxeEQxYT/Fl4dHYqmnUM1UQo2zCheHAt7G8v9rAdgqjr2rsS066jveIav4Zmi6pi2KFfHuvRcgNgkeT86ThI9/J/2GxN0f7WQ95A==', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='Current VDU methods [22,24,65,64,18] solve the task in a two-stage manner: 1) reading the texts in the document image; 2) holistic understanding of the doc- ument. They usually rely on deep-learning-based Optical Character Recognition (OCR) [4,3] for the text reading task and focus on modeling the understanding part. For example, as shown in Figure 1, a conventional pipeline for extracting structured information from documents (a.k.a. document parsing) consists of three separate modules for text', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 2, 'orig_elements': 'eJxVVE2P2zgM/StETh3ATjP5GEzb02IWxe5hW6Bot4cgGDAWHQuVJVeSJ5MW/e/7KCeZ7SEGIpKPj49P2v6ciZNefH60ZvaWZveLxZuNWfNdu2wX68W6XSxXvLldre73m/WKm1lFs14yG86M/J+z1jrx3IsWm+DH/DjwIHE+mFZzNZxPQwnzMDjbcLbBvz6HHfvDyAdJiG9n4g+zHU4HnDz6sd9LxPnyF46yPGfFeBhjBFv6988vBB5dMIm2y2W1XFd3m+puXd3e7ygF9ySUO/w4fSPriSkfQ50ygKln7yW+pdsbisLG+sOUig5Jc/WPCc2oqpDtUfKOljfUBWdTtg2N3kgElC+lob0U1FRK5vS5kxONaWTnTmiBT/BkRIbaCUePqnrPSQx9HIDHjh46jtxkifRJmnDwViWiVx8fPt3Qdl2tdtSGeCX5wlqHAw1EmzFpkz4YcZeBfuc5cAS198CRZ+4HJxVxotSFo9eh39vDGIVucUpN8E8YBBzAbbCDQkqhgO5KVPFSjmOTUWNQjlhf9kptDP1VvUSveP5tzvMXPUEjofxGmyTomSYBowglQZCz6BSjkzQNrQMbydIofHUR4KpSVQQ4o56dsJvTX+EoTxKrIgR0rI0MAj3AAC6MgZuOOszfRDutAGd7XIQEiSxEU1ZYX4U11qTIwFC9GIlSI7mRVDpOHiSboA06JPskc/oq1LCnMVtnf0gpgW4Q0QC3rZVUnTpxbYGF6xFK7+Cw/5FuQj+Mmc9baAI4qR7QWnABGqFjGJ2hvbw0LgmdPXT1d3jP5lOBj5JGlzHYPyFKuOIrkd956Cg16YWKXFzbsWI6ebZ7W+CMcHHX0eaOjG0LlUzXS0zob0KPSanpcCipomNnoXXPJ4L5DeVAQ0DaQXAHgfZjsg1PHeb0d0tH2FzVwmYnXmrqiixW51LAPN9Hi6HUjOe5S7oSU5lScYTjeMBwWC3uJt6qJAjoXapWb6r1HSzyhw+YP142jxVW0zpiDDGd5UUQj5GaEsRVNn130illFGgbLwiCAtTCZtxYNpPGfQLJYvfJKWC73ayr5WqnrwPMcmm6h5N7RHvsBhcA2xEA6ct4eTQ/cIylxWd9AX/t/gNri//b', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='detection, text recognition, and parsing [22,24]. However, the OCR-dependent approach has critical problems. First of all, us- ing OCR as a pre-processing method is expensive. We can utilize pre-trained off- the-shelf OCR engines; however, the computational cost for inference would be expensive for high-quality OCR results. Moreover, the off-the-shelf OCR meth- ods rarely have flexibility dealing with different languages or domain changes, which may lead to poor generalization ability. If we', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 2, 'orig_elements': 'eJxVVE2P2zgM/StETh3ATjP5GEzb02IWxe5hW6Bot4cgGDAWHQuVJVeSJ5MW/e/7KCeZ7SEGIpKPj49P2v6ciZNefH60ZvaWZveLxZuNWfNdu2wX68W6XSxXvLldre73m/WKm1lFs14yG86M/J+z1jrx3IsWm+DH/DjwIHE+mFZzNZxPQwnzMDjbcLbBvz6HHfvDyAdJiG9n4g+zHU4HnDz6sd9LxPnyF46yPGfFeBhjBFv6988vBB5dMIm2y2W1XFd3m+puXd3e7ygF9ySUO/w4fSPriSkfQ50ygKln7yW+pdsbisLG+sOUig5Jc/WPCc2oqpDtUfKOljfUBWdTtg2N3kgElC+lob0U1FRK5vS5kxONaWTnTmiBT/BkRIbaCUePqnrPSQx9HIDHjh46jtxkifRJmnDwViWiVx8fPt3Qdl2tdtSGeCX5wlqHAw1EmzFpkz4YcZeBfuc5cAS198CRZ+4HJxVxotSFo9eh39vDGIVucUpN8E8YBBzAbbCDQkqhgO5KVPFSjmOTUWNQjlhf9kptDP1VvUSveP5tzvMXPUEjofxGmyTomSYBowglQZCz6BSjkzQNrQMbydIofHUR4KpSVQQ4o56dsJvTX+EoTxKrIgR0rI0MAj3AAC6MgZuOOszfRDutAGd7XIQEiSxEU1ZYX4U11qTIwFC9GIlSI7mRVDpOHiSboA06JPskc/oq1LCnMVtnf0gpgW4Q0QC3rZVUnTpxbYGF6xFK7+Cw/5FuQj+Mmc9baAI4qR7QWnABGqFjGJ2hvbw0LgmdPXT1d3jP5lOBj5JGlzHYPyFKuOIrkd956Cg16YWKXFzbsWI6ebZ7W+CMcHHX0eaOjG0LlUzXS0zob0KPSanpcCipomNnoXXPJ4L5DeVAQ0DaQXAHgfZjsg1PHeb0d0tH2FzVwmYnXmrqiixW51LAPN9Hi6HUjOe5S7oSU5lScYTjeMBwWC3uJt6qJAjoXapWb6r1HSzyhw+YP142jxVW0zpiDDGd5UUQj5GaEsRVNn130illFGgbLwiCAtTCZtxYNpPGfQLJYvfJKWC73ayr5WqnrwPMcmm6h5N7RHvsBhcA2xEA6ct4eTQ/cIylxWd9AX/t/gNri//b', 'is_continuation': True, 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='train an OCR model, it also requires extensive training costs and large-scale datasets [4,3,39,46]. Another problem is, OCR errors would propagate to the VDU system and negatively influence subsequent processes [54,23]. This problem becomes more severe in', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 2, 'orig_elements': 'eJxVVE2P2zgM/StETh3ATjP5GEzb02IWxe5hW6Bot4cgGDAWHQuVJVeSJ5MW/e/7KCeZ7SEGIpKPj49P2v6ciZNefH60ZvaWZveLxZuNWfNdu2wX68W6XSxXvLldre73m/WKm1lFs14yG86M/J+z1jrx3IsWm+DH/DjwIHE+mFZzNZxPQwnzMDjbcLbBvz6HHfvDyAdJiG9n4g+zHU4HnDz6sd9LxPnyF46yPGfFeBhjBFv6988vBB5dMIm2y2W1XFd3m+puXd3e7ygF9ySUO/w4fSPriSkfQ50ygKln7yW+pdsbisLG+sOUig5Jc/WPCc2oqpDtUfKOljfUBWdTtg2N3kgElC+lob0U1FRK5vS5kxONaWTnTmiBT/BkRIbaCUePqnrPSQx9HIDHjh46jtxkifRJmnDwViWiVx8fPt3Qdl2tdtSGeCX5wlqHAw1EmzFpkz4YcZeBfuc5cAS198CRZ+4HJxVxotSFo9eh39vDGIVucUpN8E8YBBzAbbCDQkqhgO5KVPFSjmOTUWNQjlhf9kptDP1VvUSveP5tzvMXPUEjofxGmyTomSYBowglQZCz6BSjkzQNrQMbydIofHUR4KpSVQQ4o56dsJvTX+EoTxKrIgR0rI0MAj3AAC6MgZuOOszfRDutAGd7XIQEiSxEU1ZYX4U11qTIwFC9GIlSI7mRVDpOHiSboA06JPskc/oq1LCnMVtnf0gpgW4Q0QC3rZVUnTpxbYGF6xFK7+Cw/5FuQj+Mmc9baAI4qR7QWnABGqFjGJ2hvbw0LgmdPXT1d3jP5lOBj5JGlzHYPyFKuOIrkd956Cg16YWKXFzbsWI6ebZ7W+CMcHHX0eaOjG0LlUzXS0zob0KPSanpcCipomNnoXXPJ4L5DeVAQ0DaQXAHgfZjsg1PHeb0d0tH2FzVwmYnXmrqiixW51LAPN9Hi6HUjOe5S7oSU5lScYTjeMBwWC3uJt6qJAjoXapWb6r1HSzyhw+YP142jxVW0zpiDDGd5UUQj5GaEsRVNn130illFGgbLwiCAtTCZtxYNpPGfQLJYvfJKWC73ayr5WqnrwPMcmm6h5N7RHvsBhcA2xEA6ct4eTQ/cIylxWd9AX/t/gNri//b', 'is_continuation': True, 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='languages with complex character sets, such as Korean or Chinese, where the quality of OCR is relatively low [50]. To deal with this, post-OCR correction module [51,50,10] is usually adopted. However, it is not a practical solution for real application environments since it increases the entire system size and main- tenance cost.', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 3, 'orig_elements': 'eJxNUclu4zAM/RXCZzfjvelcexlggBYoeguCgJboWIAsaSQ6aVr034fyrFe+hY+Ph4+CLC3k+GR08RWKodLVQz+qYaqHRmGrxvuRFDVT1/XDvqWihGIhRo2Mwv8oJmPJ4UJZrL1b+RQwUNwFPWVuhvkWNhhDsEYhG+++/IYtuvOKZ0qCHwpy5+Io0yCTk1uXkaLM220S/2W8r3TTT42qu2avSYLuq061Y9sNRHtdt8WnKJjeOJP/boCr4RmUX4KlN1AzRlRMERJxKiGtagZM8N1HQgc+wuNsHCUq4TpTJOCZ4MeK1vAN/ATPjy9gEkSycs+F7A2sv8Khr447ePWgCe2vhTwbsQ8+8V3WKB8jqVwBLF6vlkRTl31V1tUxG65Jdogbah+Y9A6++StdKJZgOOPOMyCEnF2qtJC8XTe3SSLHvPW/loHcxUTv8n8TJOMUbTZOCTNJJfkowYzcl26JaRHSOwE6DQsadwcsv80qJfl3+WF/fvmEMW6Xv+aeP48/AQBAxWA=', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='We go beyond the traditional framework by modeling a direct mapping from a raw input image to the desired output without OCR. We introduce a new OCR-free VDU model to address the problems induced by the OCR-dependency. Our model is based on Transformer-only architecture, referred to as Document understanding transformer (Donut), following the huge success in vision and language [8,9,29]. We present a minimal baseline including a simple architecture and pre-training method. Despite its', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 3, 'orig_elements': 'eJxVUstuGzEM/BViTy3guH7FWfdao8cGKNL2EAQGV+J6hWolgZLsGkH+veQ6QZCbMCSHw9E8PjfkaaRQDs42X6GhzWbX3uJ2t9suFnZrNt1ia+yiN9iuu9Vd28ygGamgxYLS/9z0zlPAkXTYxlDLIWEinifba6+WyyVNZUzJO4PFxfDltewxHCseKUv9saFwbJ4ETYIcQh07YsHXE8LvGu8WdnXbr8xys2ot9cttu9iYdbfebIlau1w3LzJR6F/R5j8ExwgdXWKwUAaCwmidakAPPYvyc+S/0F1gjJa8C0dAsI7JFBhFsQI9x1FQxjO4kGoBN4pCKHEitJSl3UKsRWtnVwZ5wv23n3OQ7S4UjrYaEoZAZ8VveiaC3/tf151KhNYy5TwRJo6dfEqWUZ2zKk5xnbSUKFgK5jKH+8qvBC5Dh1k1BHhgDLmPPBLfxOAvgGwGV+SeyjQDpp5Y5erSDPtoqn4/VGHlXDBYvbi8k8CnvX7r5xn00ft4nsqiZqhiQa7GqGwX4OSymApCAG+/Co/tbDdb7Z4mI5IcqJsQRhfEQT9pFsfVI+OrvXqf3Zg8fVA9kcr4jciSUWmTCA7RzmFPOUkXuJKvg864cpnBJBnyEM8ZTBwlPiiWQmRJQilyVDwRo/cgUdUzMcj/lAGDrjm5WPPriqwmKU/QG7+7o8pZzTW7b7H+gcwS6hM9aORenv4DnzEnvA==', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='simplicity, Donut shows comparable or better overall performance than previous methods as shown in Figure 2.', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 3, 'orig_elements': 'eJxVUstuGzEM/BViTy3guH7FWfdao8cGKNL2EAQGV+J6hWolgZLsGkH+veQ6QZCbMCSHw9E8PjfkaaRQDs42X6GhzWbX3uJ2t9suFnZrNt1ia+yiN9iuu9Vd28ygGamgxYLS/9z0zlPAkXTYxlDLIWEinifba6+WyyVNZUzJO4PFxfDltewxHCseKUv9saFwbJ4ETYIcQh07YsHXE8LvGu8WdnXbr8xys2ot9cttu9iYdbfebIlau1w3LzJR6F/R5j8ExwgdXWKwUAaCwmidakAPPYvyc+S/0F1gjJa8C0dAsI7JFBhFsQI9x1FQxjO4kGoBN4pCKHEitJSl3UKsRWtnVwZ5wv23n3OQ7S4UjrYaEoZAZ8VveiaC3/tf151KhNYy5TwRJo6dfEqWUZ2zKk5xnbSUKFgK5jKH+8qvBC5Dh1k1BHhgDLmPPBLfxOAvgGwGV+SeyjQDpp5Y5erSDPtoqn4/VGHlXDBYvbi8k8CnvX7r5xn00ft4nsqiZqhiQa7GqGwX4OSymApCAG+/Co/tbDdb7Z4mI5IcqJsQRhfEQT9pFsfVI+OrvXqf3Zg8fVA9kcr4jciSUWmTCA7RzmFPOUkXuJKvg864cpnBJBnyEM8ZTBwlPiiWQmRJQilyVDwRo/cgUdUzMcj/lAGDrjm5WPPriqwmKU/QG7+7o8pZzTW7b7H+gcwS6hM9aORenv4DnzEnvA==', 'is_continuation': True, 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='We take pre-train-and-fine-tune scheme [8,65] on Donut training. In the pre-training phase, Donut learns how to read the texts by predicting the next words by conditioning jointly on the image and previous text contexts. Donut is pre-trained with document images and their text annotations. Since our pre- training objective is simple (i.e., reading the texts), we can realize domain and language flexibility straightforwardly pre-training with synthetic data. During fine-tuning stage, Donut learns', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 3, 'orig_elements': 'eJxtU0tv00AQ/isjn0CKQ+q4jumVXriABAUOVVSNveN42/WutbuOm1b978xsEgISN3ue32P2/jUjQwPZ+KBVdgMZFauuLD+ualUXm+ui3NRUl1VTbajqOqqKbAHZQBEVRuT616zThiwOJM3K2Sk+jDiSX46qk1pJx8OY0jiORrcYtbMfTmmDdjfhjgLn7zOyu2zL0ZEjD3YaGvIcX6eIv2DcrFRx3RXtVVnUirqrql6V7bpZlxVRra7W2Rt3RHqOUvyLIOITwegpjx61zdGqvNOWfydLENqe+cN9vaiut+As3AoJSKXa7pbw2ULs/+rnIIw9BlqcSg2htwF6N0N04AlVahAAAZqDdCrdRumTuOU4zM6rlGydVVoUkfSj0zaag6CQSj2wDsBwZcReuymkodKThi9PAHS4oCMFs449KNdOYutxSEhTeKb2xxForYvJCZ7yXduWwE0+jflDHVzzSIx7T7Ih6GE0BO/0kpaLxPJMKGF5v4CZoEUrKaNfiBEMPCctPrsMnaFn3Wij4wGC7Nn1sXN+Rq/M4V+JE4twsLwh6hbk3pjv5CV3dk++Q+TB/7disoo854/UYe6doYsw2LZsQg6JhksVys2WYREOfDPhaQl8PIoGJ0GMdFrC3gMKfMedlx0yB0/cYu/dtOuBlSEbREHao5mS4OLuHn2y8+ftj7Tp6I9wDCS+3jEYeuZnpAUqiy+M8OzB10/f8s4TpfbBKTJJeGx7TbyK0UTKXZczpRx9BJ7DIg8oLrMjkfwQwHXQOJF4JL4ZWc+CTB7bw3G9HJnXzZRuBPj5QZiGgXG/SHmAzhnj5nAjr/j8wL+gZ52Y7p08vrftb+MZinQ=', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='how to understand the whole document accord- ing to the downstream task. We demonstrate Donut has a strong understanding ability through extensive evaluation on various VDU tasks and datasets. The experiments show a simple OCR-free VDU model can achieve state-of-the-art performance in terms of both speed and accuracy. The contributions are summarized as follows:', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 3, 'orig_elements': 'eJxtU0tv00AQ/isjn0CKQ+q4jumVXriABAUOVVSNveN42/WutbuOm1b978xsEgISN3ue32P2/jUjQwPZ+KBVdgMZFauuLD+ualUXm+ui3NRUl1VTbajqOqqKbAHZQBEVRuT616zThiwOJM3K2Sk+jDiSX46qk1pJx8OY0jiORrcYtbMfTmmDdjfhjgLn7zOyu2zL0ZEjD3YaGvIcX6eIv2DcrFRx3RXtVVnUirqrql6V7bpZlxVRra7W2Rt3RHqOUvyLIOITwegpjx61zdGqvNOWfydLENqe+cN9vaiut+As3AoJSKXa7pbw2ULs/+rnIIw9BlqcSg2htwF6N0N04AlVahAAAZqDdCrdRumTuOU4zM6rlGydVVoUkfSj0zaag6CQSj2wDsBwZcReuymkodKThi9PAHS4oCMFs449KNdOYutxSEhTeKb2xxForYvJCZ7yXduWwE0+jflDHVzzSIx7T7Ih6GE0BO/0kpaLxPJMKGF5v4CZoEUrKaNfiBEMPCctPrsMnaFn3Wij4wGC7Nn1sXN+Rq/M4V+JE4twsLwh6hbk3pjv5CV3dk++Q+TB/7disoo854/UYe6doYsw2LZsQg6JhksVys2WYREOfDPhaQl8PIoGJ0GMdFrC3gMKfMedlx0yB0/cYu/dtOuBlSEbREHao5mS4OLuHn2y8+ftj7Tp6I9wDCS+3jEYeuZnpAUqiy+M8OzB10/f8s4TpfbBKTJJeGx7TbyK0UTKXZczpRx9BJ7DIg8oLrMjkfwQwHXQOJF4JL4ZWc+CTB7bw3G9HJnXzZRuBPj5QZiGgXG/SHmAzhnj5nAjr/j8wL+gZ52Y7p08vrftb+MZinQ=', 'is_continuation': True, 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='1. We propose a novel OCR-free approach for VDU. To the best of our knowl- edge, this is the first method based on an OCR-free Transformer trained in end-to-end manner.', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 3, 'orig_elements': 'eJxFkN1qwzAMRl9F+Lrpmjhrmt1uN4PBYHTbRSlFieXULJGN4+yH0nefUjYKBsN3joT4didFPQ3E6eCMugNVV5XVpItyXZOmCqmsN6sSc93qojCVUQtQAyU0mFD8k7KuJ8aB5mHjeUqHgIHiMhg7uzNOP+GCMYTetZic55s/3CN3E3Y0Ct8p4k7tJQ2SHHgaGoqS60sSrzdWK1Pc2qLNy2JjyOZrubDVjS7XRBuTa3WWiUTfaZbzJbwThOiDHwkQ2H9SD8/3L5mNJEEQhO0RrI/w9vC6hK2HdCRoaEzgLfgpwgf7rz4DMh0tBLoR5M2SdVEs6ePoDTQ4kgHPgHzdv43Io+weKEKK6FgUx0BssuQz+WBAZulrbuO/qCc3psdEgzrvfwHQXIYV', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='2. We introduce a simple pre-training scheme that enables the utilization of synthetic data. By using our generator SynthDoG, we show Donut can easily be extended to a multi-lingual setting, which is not applicable for the conventional approaches that need to retrain an off-the-shelf OCR engine. 3. We conduct extensive experiments and analyses on both public benchmarks and private industrial datasets, showing that the proposed method achieves not only state-of-the-art performances on benchmarks', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 3, 'orig_elements': 'eJxNkltr3DAQhf/K4KcW1u5es0sf20ApFArtQx9CCGNpbIvKkpHGm25D/nuPvAnpg8Ge63eO5+6pEi+jBH1wtvpI1eF4MLzdM7enHa/t8bgVsRs5nMyp5X17rFZUjaJsWRn1T1XnvAQepTTbGGZ9mHiS1Ey2K7UlrZdpSfM0eWdYXQwfXtKeQz9zLxn5u0pCX90jOiHyEOaxlYT4bomkN8bj2m4P3dZs9tuTlW5zc1rvza7d7W9ETnazq57RofJHS/G2oV9CLmiKdjZCTNmNkxeaktSa2AUXespmgAukAytBTusl40NoVufd3wWZYkf5EhBVZ6job+jTheZc+uOcqJcgiTUm+lnKbuOXFT0K5SE+0m1xhgwHEs7OX6gVAqAEK5Y0gmqcvbrau+KHpyyqeMWAwZmBXKYQlV4MBB11WFMATQxnOAM+dCGfIkNKvioJcp2eZBFKXER0NfrqPIjv6PvnH5DbuyAN7RajMA826RUuu3PBxO905UQyBlg87C8ZK2BJG3WgaW5BBUXBDCOn39eyKbkza3HezlmTA17xDMLyavGk2LZQFhngnmIGLW5riFhiBidnucqOAYZlxbQ6Xuk5KYEKJowczAvL2/4WXrPPkQbOhIoL5rPBbyse2TMHLSdH76TpmxUkZ62l6wQVZ3kPYhjGvn6MyVv672hzU0729Zq/uaxfVcbq+f4f46wkpg==', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='but also has many practical advantages (e.g., cost-effective) in real-world applications.', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 3, 'orig_elements': 'eJxNkltr3DAQhf/K4KcW1u5es0sf20ApFArtQx9CCGNpbIvKkpHGm25D/nuPvAnpg8Ge63eO5+6pEi+jBH1wtvpI1eF4MLzdM7enHa/t8bgVsRs5nMyp5X17rFZUjaJsWRn1T1XnvAQepTTbGGZ9mHiS1Ey2K7UlrZdpSfM0eWdYXQwfXtKeQz9zLxn5u0pCX90jOiHyEOaxlYT4bomkN8bj2m4P3dZs9tuTlW5zc1rvza7d7W9ETnazq57RofJHS/G2oV9CLmiKdjZCTNmNkxeaktSa2AUXespmgAukAytBTusl40NoVufd3wWZYkf5EhBVZ6job+jTheZc+uOcqJcgiTUm+lnKbuOXFT0K5SE+0m1xhgwHEs7OX6gVAqAEK5Y0gmqcvbrau+KHpyyqeMWAwZmBXKYQlV4MBB11WFMATQxnOAM+dCGfIkNKvioJcp2eZBFKXER0NfrqPIjv6PvnH5DbuyAN7RajMA826RUuu3PBxO905UQyBlg87C8ZK2BJG3WgaW5BBUXBDCOn39eyKbkza3HezlmTA17xDMLyavGk2LZQFhngnmIGLW5riFhiBidnucqOAYZlxbQ6Xuk5KYEKJowczAvL2/4WXrPPkQbOhIoL5rPBbyse2TMHLSdH76TpmxUkZ62l6wQVZ3kPYhjGvn6MyVv672hzU0729Zq/uaxfVcbq+f4f46wkpg==', 'is_continuation': True, 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='4. The codebase, pre-trained model, and synthetic data are available at GitHub.1\\n\\n2 Method\\n\\n2.1 Preliminary: background\\n\\nThere have been various visual document understanding (VDU) methods to un- derstand and extract essential information from the semi-structured documents such as receipts [20,25,18], invoices [49], and form documents [14,6,43].\\n\\n1https://github.com/clovaai/donut.\\n\\n4  G. Kim et al. ', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 3, 'orig_elements': 'eJzVVctu1DAU/ZWrrECazviVh7tGKhUPsShsRqPqxr6ZsUicyHZGVFX/HWegYkCVgBV0F5378D3n+sTb+4J6GsinW2eLSyiwKhkJbZWtZEkVYlMZVTa1NorrRstiBcVACS0mzPn3Red68jjQUmxHP6fbCScK68l2S+4STnfTKYzT1DuDyY1+8z3co9/PuKeY49uC/L7YZXTKyK2fh5ZCxuUJCT9mrJkVZScMV6Kx1PGqYcrIVqqKqLFcFg+5ItGXtCSrNdwcCMxoqcVIK5gCXaSAzpOFIaP9CtBbiHc+HSg5Aws1yOcBHtH12Pb5K8GVS6/nds2XqR8JvXUxXScalgN/FVIT1lrILitHxnTIlRSCi1IqoRvOnp+QAt5ROoz2XIAbl3p6ir1iWjW8rhhqI5U2XVnnnsaKrm6JJD4/9msOHwL1bnAew90ltGg+78M4+z8TBJmxpqZGa1OpzjKBJVctq0i2Vclr9j8I8tulnQuSTZUtcsAjQQ56OGJw4xzh6OKMPdjRzAt9yAJRiClbzPk9vPj06uNLGE4XKUIac/gCHhNOPsztA5oEFGMud7mV890YhhM96MI4QPYpRBrcRUxhNmkO2cmP50WIszkARghkyE0Z2Aq2EuWKN1kC54+jM5RBpXffjL80PyvfcrWqVkru1ud7fY8h5AmOdLPQf2K/FTO1ImyRtGBl15JqW5RNXdXEZEP82e2XH1Ka4uVms3fpkH98Zhw2ph+PiG5zmvfv9MnEO0tcoCy17hhnylrG80PTlaS5Lv+BPuqnZwLgag1v3ACUAPs1nLO7HnJh8bD7CmALRMM=', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='Fig. 3. The pipeline of Donut. The encoder maps a given document image into embeddings. With the encoded embeddings, the decoder generates a sequence of tokens that can be converted into a target type of information in a structured form', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 4, 'orig_elements': 'eJxNUM1OwzAMfhUr56kUtq7AFcQTIHGYpspL3CyidULqTqBp747TAeIWfb/xtzsbGmgkli448wimXt83m7a1ZNtme+eah6buLbXbFulwW9dbswIzkqBDQdWfTR8GYhypmF3kWbqEiXKVXF+0hZavtNCY0hAsSoh880MPyH5GT5PyO0PszV7RpEjH83igrPjmopDQp5SMl+ArWFfweiRIIdEQmCD28FyqrzCxjY4yjJgmQPDhRAwu2rmcCWHUcAgsEUgLnAvspwreghxB/tzuH7lacEfXVE9MGYVK9EQfs+qXD0h8J55UigIWGQ4ENvKJsmjYUocgmD0JlD2KJXAf87jsoe+SJ3m2Mmd1FKYM9Lud3q34E6aiNpf9N4VnlYU=', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='Earlier VDU attempts have been done with OCR-independent visual back- bones [27,1,15,12,31], but the performances are limited. Later, with the remark- able advances of OCR [4,3] and BERT [8], various OCR-dependent VDU models have been proposed by combining them [22,24,23]. More recently, in order to get a more general VDU, most state-of-the-arts [64,18] use both powerful OCR en- gines and large-scale real document image data (e.g., IIT-CDIP [32]) for a model pre-training. Although they showed', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 4, 'orig_elements': 'eJxdUktv2zAM/iuETxsge7Wd5rHb1vYQYC8U2S5GUNAWbQuTJU+PdEHR/14qydBtFx3IT/weZPOUkaaJTHhQMnsPWbfZLFcVdrTscLGsFvXV5rpfrcvV1bIqcVlnArKJAkoMyPinrFeaDE6UPktrYniYcSZXzLJP2NQOx/nUxnnWqsOgrHl3aWs0Q8SBPPebjMyQ7bk6c+XBxKklx/XFM5cC/Q5pxh06rcjBj9vvgCHQNAcPIx4IWiIDrIDgUYURvt7c58pImokfE+CgfEQNLXY/c2gZ5qGpVqIU5bUoK1GXTNzGAGEkYP29dROajlHoCLSaVCBZwCcM5MSZISEdTeh4ILaaAOXh/MX2iR6ahaj3gEbCx7v7HTRrpjigUzb6k7xXccnNZCXpv73Mzs7Wk4T2CJ2dWmWUGRLrxMorUS1EVe8L+Gxd0tHxHH0UoAxYJzmhYGGgAMiDGTCQIcf+mUlwxQfwgb3kts95Yo6OY2yWC1Gu9xA9K7DscLaPnETUJzdkchhUyi050ugGyn2HOpHzYGm7mO4I1MTbg3Qf8IaKoRCw3e7ym9vtN2jqav8WONqTKrbLHikPDk/WCvigw2jjcIr2CH5kenmJ+N+A2eTZMRwJnRfA1+EQqOfZrMBz91dUjn9zCmR85AT+W2xaEhpuBsa9LiCFHf0laMac4/Ej6f6SQsqgSLf756y/oHN81AfapRt93r8AuF0eRA==', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='remarkable advances in recent years, extra effort is required to ensure the performance of an entire VDU model by using the off-the-shelf OCR engine.', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 4, 'orig_elements': 'eJxdUktv2zAM/iuETxsge7Wd5rHb1vYQYC8U2S5GUNAWbQuTJU+PdEHR/14qydBtFx3IT/weZPOUkaaJTHhQMnsPWbfZLFcVdrTscLGsFvXV5rpfrcvV1bIqcVlnArKJAkoMyPinrFeaDE6UPktrYniYcSZXzLJP2NQOx/nUxnnWqsOgrHl3aWs0Q8SBPPebjMyQ7bk6c+XBxKklx/XFM5cC/Q5pxh06rcjBj9vvgCHQNAcPIx4IWiIDrIDgUYURvt7c58pImokfE+CgfEQNLXY/c2gZ5qGpVqIU5bUoK1GXTNzGAGEkYP29dROajlHoCLSaVCBZwCcM5MSZISEdTeh4ILaaAOXh/MX2iR6ahaj3gEbCx7v7HTRrpjigUzb6k7xXccnNZCXpv73Mzs7Wk4T2CJ2dWmWUGRLrxMorUS1EVe8L+Gxd0tHxHH0UoAxYJzmhYGGgAMiDGTCQIcf+mUlwxQfwgb3kts95Yo6OY2yWC1Gu9xA9K7DscLaPnETUJzdkchhUyi050ugGyn2HOpHzYGm7mO4I1MTbg3Qf8IaKoRCw3e7ym9vtN2jqav8WONqTKrbLHikPDk/WCvigw2jjcIr2CH5kenmJ+N+A2eTZMRwJnRfA1+EQqOfZrMBz91dUjn9zCmR85AT+W2xaEhpuBsa9LiCFHf0laMac4/Ej6f6SQsqgSLf756y/oHN81AfapRt93r8AuF0eRA==', 'is_continuation': True, 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='2.2 Document Understanding Transformer', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 4, 'orig_elements': 'eJwtjkGOwjAMRa8SeT0C2oLazpojMCuEKpPYVaTEjVJXGoS4Owli+97/374+gQJFEp28g18D7Ji7bmQ7NDwgN+19HPqj7U9db8dmPMCPgUiKDhVL/gnsAwlGqmW3yKZTwkR5lxzXbNX6SB+NKQVvUf0i+68OKPOGM63FX4FkhluhqZBJtninXPjxVZDSv9aNdtea82K3+rL5E0d5VRTnZTaXjLLykmNp1cb36sVrIHjd3l2RS9o=', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='Donut is an end-to-end (i.e., self-contained) VDU model for general understand- ing of document images. The architecture of Donut is quite simple, which con- sists of a Transformer [58,9]-based visual encoder and textual decoder modules. Note that Donut does not rely on any modules related to OCR functionality but uses a visual encoder for extracting features from a given document im- age. The following textual decoder maps the derived features into a sequence of subword tokens to construct a', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 4, 'orig_elements': 'eJxlU01v2zAM/SuETy2QePlsnV3X7bBDC2zdLkER0BLlCJMlVx/NgqL/fWTcrl13svH4SD4+UtvHihz15PPO6uojVLP1bL7BRdM2s8Wc2kY1C6L5bIEXtFLr9UU1gaqnjBozMv+xMtaRx54kWQdf8m7AgWI9aCNcCefjcArjMDirMNvgPzyHHfquYEeJ49uKfFfdMTowsvOlbykyvjoh8VWj0cYslxujmrlp0MwX7aa5XKnL9fJSbeabWfXEGZl+ZyFfiSawCdADeT3NYcofOLM11RNI5MxUBZ/RetLn8PPqB/RBkwMTInTkKaKD4jXFlJHTwfoOggEdVBHbwPYiv4bbPQFGtbeZVC6RhPS3931hGJLtB0cTOOyt2gN3nTKUchIqwm1En7hrTxG262ayuZu2mEjDg02FRZBXLCzyHBpkOME0jRhLLk5UXAfuk/eYn3vrQAl8yBDJHSF4zj6+sAXDzA1ygJtP38AUr2Q36Gw+QsvZJTEL3wsQZ7h/RGazGYZQ5k1gYuiZ3dkH8m/9mQI7NBpkgnPhIFn/TYBDYuHEQOQC+rWs9awPeVP3hRWcfE2lPYQown+RT6KfzUw5FpWZqSnZyBVGoMiv2MqenFHd8dK/fr+5Pq/hM/IWxmWr0A/Bn+Smt4sYNzAZPd+XUeGYwkS2QK4GCJNld61/d2M9er6fcfLwIJfkYIhBUUr/XId1rrDY0y64yBfbyf0sa3kgL2/nGiMT2Jlbueunuz9fb0ln', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='desired structured format (e.g., JSON). Each model component is Transformer-based, and thus the model is trained easily in an end-to-end manner. The overall process of Donut is illustrated in Figure 3.', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 4, 'orig_elements': 'eJxlU01v2zAM/SuETy2QePlsnV3X7bBDC2zdLkER0BLlCJMlVx/NgqL/fWTcrl13svH4SD4+UtvHihz15PPO6uojVLP1bL7BRdM2s8Wc2kY1C6L5bIEXtFLr9UU1gaqnjBozMv+xMtaRx54kWQdf8m7AgWI9aCNcCefjcArjMDirMNvgPzyHHfquYEeJ49uKfFfdMTowsvOlbykyvjoh8VWj0cYslxujmrlp0MwX7aa5XKnL9fJSbeabWfXEGZl+ZyFfiSawCdADeT3NYcofOLM11RNI5MxUBZ/RetLn8PPqB/RBkwMTInTkKaKD4jXFlJHTwfoOggEdVBHbwPYiv4bbPQFGtbeZVC6RhPS3931hGJLtB0cTOOyt2gN3nTKUchIqwm1En7hrTxG262ayuZu2mEjDg02FRZBXLCzyHBpkOME0jRhLLk5UXAfuk/eYn3vrQAl8yBDJHSF4zj6+sAXDzA1ygJtP38AUr2Q36Gw+QsvZJTEL3wsQZ7h/RGazGYZQ5k1gYuiZ3dkH8m/9mQI7NBpkgnPhIFn/TYBDYuHEQOQC+rWs9awPeVP3hRWcfE2lPYQown+RT6KfzUw5FpWZqSnZyBVGoMiv2MqenFHd8dK/fr+5Pq/hM/IWxmWr0A/Bn+Smt4sYNzAZPd+XUeGYwkS2QK4GCJNld61/d2M9er6fcfLwIJfkYIhBUUr/XId1rrDY0y64yBfbyf0sa3kgL2/nGiMT2Jlbueunuz9fb0ln', 'is_continuation': True, 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='Encoder. The visual encoder converts the input document image x∈RH×W ×C into a set of embeddings {zi|zi∈Rd, 1≤i≤n}, where n is feature map size or the number of image patches and d is the dimension of the latent vectors of the encoder. Note that CNN-based models [17] or Transformer-based models [9,40] can be used as the encoder network. In this study, we use Swin Transformer [40] because it shows the best performance in our preliminary study in document parsing. Swin Transformer first splits the', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 4, 'orig_elements': 'eJx1U8tu2zAQ/JWFznZq2U5k9xoUaIHWKNoAPThGsBKXNhGKFPiw46T59+5Kdpo+cpK4j+FwZ3b9VJCllly6M6p4D8ViuZhhOZ0T1WVzdVlNS31Vl7WaTKtqtpxOihEULSVUmJDrnwptLDlsSZqVdzndddhRuOiUllpJp2PXp7HrrGkwGe/endIW3TbjliLn1wW5bbHhaMeRO5fbmgLH530k/Oaoldaz2VI3i1IvUJfTermo5k11OauaZbmcFM/ckeghSfEH13jFfOBmR7A3MaMFGmLQeLenkCIkzhnX5QTKN1nGAaZlEvBwm6fTyeLbx9s8majqBwzfa65OHhAiJfAaiKkqZdw2wtOj+floTm1qBKX8Xs3N8HFM7bCjQODARNCEKfOhxQ6ieSTwoecyvF2QBxodpmZHEdApUNIoRcow0cjDlDoJWEzCfE9N8iGeo3QewMon4ggmuF6txjVGUtByykZYl9VG7r4J6KL2oaXwV8FyNJ9soEEHNUGWDMbX8OAoHXy4v4BPjuNMMaasjvzavhy+H4x7DQ9rwaupQcmaBHHnDwNiTTEBW0gK0TWiDPgcoAtkTWschuMALokXvdghkQW4+PcmbQIDRvbeH0qfBB6UdN6NPXvBskcZ5Tzw/6DV1jf3cSTeiSb26rMNdkYnngkXK384jy7bZMY7QsU2sXqMSeQRvXio2VKvJgKPbWzxyNBfPn8dARsd+kVhBCYmfF/IsIXdaDgCc9kK0741vtnGh3t2CbDqhHyMiV89LIPPSeZwsonmwdo3nsumfh7sqoZx9Q3eWn8QCrJqslaKBqfJYp93foUh8Mbv6Ub28XnzC5q0ib8=', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='input image x into non-overlapping patches. Swin Transformer blocks, consist of a shifted window-based multi-head self-attention module and a two-layer MLP, are applied to the patches. Then, patch merging layers are applied to the patch tokens at each stage. The output of the final Swin Transformer block {z} is fed into the following textual decoder.', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 4, 'orig_elements': 'eJx1U8tu2zAQ/JWFznZq2U5k9xoUaIHWKNoAPThGsBKXNhGKFPiw46T59+5Kdpo+cpK4j+FwZ3b9VJCllly6M6p4D8ViuZhhOZ0T1WVzdVlNS31Vl7WaTKtqtpxOihEULSVUmJDrnwptLDlsSZqVdzndddhRuOiUllpJp2PXp7HrrGkwGe/endIW3TbjliLn1wW5bbHhaMeRO5fbmgLH530k/Oaoldaz2VI3i1IvUJfTermo5k11OauaZbmcFM/ckeghSfEH13jFfOBmR7A3MaMFGmLQeLenkCIkzhnX5QTKN1nGAaZlEvBwm6fTyeLbx9s8majqBwzfa65OHhAiJfAaiKkqZdw2wtOj+floTm1qBKX8Xs3N8HFM7bCjQODARNCEKfOhxQ6ieSTwoecyvF2QBxodpmZHEdApUNIoRcow0cjDlDoJWEzCfE9N8iGeo3QewMon4ggmuF6txjVGUtByykZYl9VG7r4J6KL2oaXwV8FyNJ9soEEHNUGWDMbX8OAoHXy4v4BPjuNMMaasjvzavhy+H4x7DQ9rwaupQcmaBHHnDwNiTTEBW0gK0TWiDPgcoAtkTWschuMALokXvdghkQW4+PcmbQIDRvbeH0qfBB6UdN6NPXvBskcZ5Tzw/6DV1jf3cSTeiSb26rMNdkYnngkXK384jy7bZMY7QsU2sXqMSeQRvXio2VKvJgKPbWzxyNBfPn8dARsd+kVhBCYmfF/IsIXdaDgCc9kK0741vtnGh3t2CbDqhHyMiV89LIPPSeZwsonmwdo3nsumfh7sqoZx9Q3eWn8QCrJqslaKBqfJYp93foUh8Mbv6Ub28XnzC5q0ib8=', 'is_continuation': True, 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='Decoder. Given the {z}, the textual decoder generates a token sequence (yi)m i=1, where yi∈Rv is an one-hot vector for the i-th token, v is the size of token vo- cabulary, and m is a hyperparameter, respectively. We use BART [33] as the decoder architecture. Specifically, we initialize the decoder model weights with those from the publicly available2 pre-trained multi-lingual BART model[38].', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 5, 'orig_elements': 'eJxNksGOm0AMhl/F4tRKSUpCSKFSD60q9dbDdqUe0igygwGrw8x0mEnKRvvu9ZBdaQ8gZH/+/dvmeMtI00gmnLnNPkGGVaHKsqJuj0WnDjXhYZcfdnVJ1VbRxzJbQTZSwBYDCn/LOtZkcKRU3FoTw9mhI79xbZfYlA6zW9LonGaFga358JLWaPqIPU2SP2Zk+uwkUSeRs4ljQ17i5RLxbzyS2uc5FXQoW5XXTbEtt8Wh3jeqruqiqbNnqQj0LyT4Gynbih/4zhcyEAaC25MA6SMxETW0dwZ6MuQx0AQIwf4RfKK/kYwieDfz+xH483YF14E8wcy/426XVw8XYOENWEPrwQa4kArWQydP6sHrMNzFVrCgKTjxE4HtXppc7BoUNlGjn1ci1cK4aMIgi/Myuqw3kF+Bp8mJugyi5w38IogTwdcvD49wLIoT4F38dRr0auAgfPS0gZ9SyZ2sX2tpchVjhgOjTk7eVo3y1pLnfggTXDm5H6z06bwdF9LFRs6oZ8ALssZG0w6cp3XwyIbEfNSB15rTZfXd3iJ6LKrTJh399X/4gV62LdM8pls9n/4DY27bvw==', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='Model Input. Following the original Transformer [58], we use a teacher-forcing scheme [62], which is a model training strategy that uses the ground truth as input instead of model output from a previous time step. In the test phase, inspired by GPT-3 [5], the model generates a token sequence given a prompt. We add new special tokens for the prompt for each downstream task in our experiments. The prompts that we use for our applications are shown with the desired output sequences in Figure 3.', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 5, 'orig_elements': 'eJxNUsuO2zAM/BXC5103iZ0g7m0vW/TQoocAPQSLgLZoW1hbUvXIA4v995LKo70YFjkcDofcfxQ00UwmHrQqvkLRNv1io5oG+y1V/L9aNtWqrlbrdV1TvVkWT1DMFFFhRMZ/FL2eyOBMUqysSfHg0JEvneoFK+l4cTmNzk26w6it+XJLT2iGhAMFzu8LMkPxxlHHkYNJc0ue4+sc8f80InX1YkEVbdaqWzRttVwvq01Tt12zbaq2KT65ItI5CviHVTTBd+NSLOHVTpM9aTNAHAms14M2OMHOowm99TN52K+3LOFEkAIBQiTsRvLPnO2kLvBrJthvVoIadTeCDoybc5voUZsM479Iw4X7YBSqkDsO3iajGJbiCBhAiyz+Bm6jwPY3GpuixHtvZ6Z2no7aJmbQ3JmhruR5Ml+kEMGNGOhJWJz2pKC9wLdfu+eKR2GNAruyDmRIVIncaN/JQKA/iUzHuvSRn9LKzo59+s2jKwWGThAcdZo9yhUB2IdMeUXmpzgEyp54Ck84Q8Twzmp4Cs6d+Ra03FcoYfeoC1dfbi4LiYD/uw/W6HnWkVnhpNks6ako5AFv9tzVi43wqofEFRVbM00p288zSX8+sRvlXft9p/BY6n1bKMvJnTq2zD8WwafBckUSHlFP2E4kTV+cI6P0GV7KupRzvl/6T/RXATu5ws+3v5xVLSA=', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='Illustrative explanations for the teacher- forcing strategy and the decoder output format are available in Appendix A.4.', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 5, 'orig_elements': 'eJxNUsuO2zAM/BXC5103iZ0g7m0vW/TQoocAPQSLgLZoW1hbUvXIA4v995LKo70YFjkcDofcfxQ00UwmHrQqvkLRNv1io5oG+y1V/L9aNtWqrlbrdV1TvVkWT1DMFFFhRMZ/FL2eyOBMUqysSfHg0JEvneoFK+l4cTmNzk26w6it+XJLT2iGhAMFzu8LMkPxxlHHkYNJc0ue4+sc8f80InX1YkEVbdaqWzRttVwvq01Tt12zbaq2KT65ItI5CviHVTTBd+NSLOHVTpM9aTNAHAms14M2OMHOowm99TN52K+3LOFEkAIBQiTsRvLPnO2kLvBrJthvVoIadTeCDoybc5voUZsM479Iw4X7YBSqkDsO3iajGJbiCBhAiyz+Bm6jwPY3GpuixHtvZ6Z2no7aJmbQ3JmhruR5Ml+kEMGNGOhJWJz2pKC9wLdfu+eKR2GNAruyDmRIVIncaN/JQKA/iUzHuvSRn9LKzo59+s2jKwWGThAcdZo9yhUB2IdMeUXmpzgEyp54Ck84Q8Twzmp4Cs6d+Ra03FcoYfeoC1dfbi4LiYD/uw/W6HnWkVnhpNks6ako5AFv9tzVi43wqofEFRVbM00p288zSX8+sRvlXft9p/BY6n1bKMvJnTq2zD8WwafBckUSHlFP2E4kTV+cI6P0GV7KupRzvl/6T/RXATu5ws+3v5xVLSA=', 'is_continuation': True, 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='Output Conversion. The output token sequence is converted to a desired structured format. We adopt a JSON format due to its high representation capacity. As shown in Figure 3, a token sequence is one-to-one invertible to a JSON data. We simply add two special tokens [START ∗] and [END ∗], where ∗ indicates each field to extract. If the output token sequence is wrongly structured, we simply treat the field is lost. For example, if there is only [START name] exists but no [END name], we assume the', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 5, 'orig_elements': 'eJx9U8Fu2zAM/RXC5zSz4ySrdyu2FdgOKbAG2CENAlqibWGypElys6Lov4+S03XAgJ1sk498j4/04bkgTSOZeFKy+ADFarPuUFJZNvW63KzqRmybdlPV67a73sqaigUUI0WUGJHxz0WnNBkcKRVLa6Z4cujIL53sEjal45PLaXROK4FRWfPuktZo+gl7Cpw/FGT64shRx5GTmcaWPMc3OeLfNCKJdVlSTduNFGXT1hUL3DbrVjTXTd02xQtXRPoVE/huim6K8NGaR/KBqZewHwjsHI72BxkI9HMiIwhUAJGBkSTnAEFSUJ4/QvSTiFN67awfMS7hOwFK6yKjvt7f7S5xkBOlUhUDDKofwJPzFFh9HhwEOhQqPi3hJkAY7NmAMnCreu4N9YKb/avJGrqK9oofjE3qVKtp1peZ0zKynqBGp59YFqs/WwiOhEI9dwxwuN/ffNvDw7RaVe+PgEbC4fPu02tgAeeBWMT8yUwyLYsCEIoBOkU6e8K+ehQ8/5cO4v+cPHtrelbzZh0z/NEYPbFZqcHcmQu0Ddz21nrmQAbRAlTm8BcXuOwyQ7q4I8NUYJtbpjd2niUnMg+GMI2UGUYrSUOHSoe/JkiDlpVIFflNzkrSfTAb6t56FYeRN2bYgqCYvWUhSVj6YfgSzpyf5yFecz9pTNLTutOhseFVdVymM3/9A3boPZ/BI+3Tdb4cfwP5djCa', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='model fails to extract “name” field. This algorithm can easily be implemented with simple regular expressions [11].', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 5, 'orig_elements': 'eJx9U8Fu2zAM/RXC5zSz4ySrdyu2FdgOKbAG2CENAlqibWGypElys6Lov4+S03XAgJ1sk498j4/04bkgTSOZeFKy+ADFarPuUFJZNvW63KzqRmybdlPV67a73sqaigUUI0WUGJHxz0WnNBkcKRVLa6Z4cujIL53sEjal45PLaXROK4FRWfPuktZo+gl7Cpw/FGT64shRx5GTmcaWPMc3OeLfNCKJdVlSTduNFGXT1hUL3DbrVjTXTd02xQtXRPoVE/huim6K8NGaR/KBqZewHwjsHI72BxkI9HMiIwhUAJGBkSTnAEFSUJ4/QvSTiFN67awfMS7hOwFK6yKjvt7f7S5xkBOlUhUDDKofwJPzFFh9HhwEOhQqPi3hJkAY7NmAMnCreu4N9YKb/avJGrqK9oofjE3qVKtp1peZ0zKynqBGp59YFqs/WwiOhEI9dwxwuN/ffNvDw7RaVe+PgEbC4fPu02tgAeeBWMT8yUwyLYsCEIoBOkU6e8K+ehQ8/5cO4v+cPHtrelbzZh0z/NEYPbFZqcHcmQu0Ddz21nrmQAbRAlTm8BcXuOwyQ7q4I8NUYJtbpjd2niUnMg+GMI2UGUYrSUOHSoe/JkiDlpVIFflNzkrSfTAb6t56FYeRN2bYgqCYvWUhSVj6YfgSzpyf5yFecz9pTNLTutOhseFVdVymM3/9A3boPZ/BI+3Tdb4cfwP5djCa', 'is_continuation': True, 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='2.3 Pre-training\\n\\nTask. The model is trained to read all texts in the image in reading order (from top-left to bottom-right, basically). The objective is to minimize cross-entropy loss of next token prediction by jointly conditioning on the image and previous contexts. This task can be interpreted as a pseudo-OCR task. The model is trained as a visual language model over the visual corpora, i.e., document images.', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 5, 'orig_elements': 'eJzNkk9v2zAMxb8K4dMGJF4c/0m16+7bMORWFAEl0alaWRIkOZhX9LuPcjMgGDD0uqP5fjQfH3X/UpGliVw+GV19hgplh7LBRgzDcDgMJJux27d915ESnZZUbaCaKKPGjMy/VKOx5HCi0qy9m/MpYKBYBz0Wtsh5CauMIVijMBvvPl1li+4845kS6/cVuXP1wNXAlZObJ0mR6/1aiTceSXW7HbU09FrthGybvmkH0Ukl7kQrRfXKHZl+5gLv6xa+R9rmiMYZHlC0q6GjyZYK/XcKzdCIVuimH9pu33T6oER/EIT7uz3uaNf+Fym8d6nbFI6Ynms4PhJMXpMFk2ANhDRkD5FQA1oLBU9gHGQmzcQGykeROTnwUVOED2P0E3eFraUxl3bpc/bTNprzY96AxMTrWbt8fBvo5ROpbC60DvUw8Rkm84tARZ/SlheKPixg+QP8CI4tMPZMDkIkbVQJCuQCT964bBdQ3mlTiqulW6vodOm5GD+ngq3bFBNlMAcACvlPZaVMkcHM22MChJBo1n777cuPlftHUCt6MWlGC38udqX8hYMpRq6y8jH4iBswNdUb0F7N5XW9+Uz17Rv8ijFiiedYTvX68Bub5y3n', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='Visual Corpora. We use IIT-CDIP [32], which is a set of 11M scanned english document images. A commercial CLOVA OCR API is applied to get the pseudo text labels. As aforementioned, however, this kind of dataset is not always avail- able, especially for languages other than English. To alleviate the dependencies, we build a scalable Synthetic Document Generator, referred to as SynthDoG. Using the SynthDog and Chinese, Japanese, Korean and English Wikipedia, we generated 0.5M samples per language.', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 5, 'orig_elements': 'eJxNUl1v2zAM/CuEn1MvTuxk3luQDEX21WLL2oegCGiLtoXKkiDJyYKi/72UkxZ7E8jj3fHE/UtCinrS4SBF8gUSnCIWi7pcFpnI5uVsWmKJOVFZFuXnaZEnE0h6CigwIONfkkYq0thTHBZGD+Fg0ZJLrWgiNrbD2Y5ttFbJGoM0+tO1rVC3A7bkub9PSLfJE1ctVw566CtyXC/GivvPY5VjlWFWLhaL5XJBVdbks3mR51SXuagoeeWJQP9CBD9IP6CCtXHWOEzhkWDwBNvt7ma92d7Dfj5jyVMn6w6kBwRPAUwDWfYTfI1akwD2paTvQJh6iFmB7KPnFFZQm74nV8so8ePuYQV369+wut+OXHFfHg8GWiYNHYH1NAgD0RworEhFEkY2xo2/wNGQmEBnTnQkN+EZ5nmWWkRLMfPojkvaBEB1wjPPHlGqG8BK0QTIW4pm1BmYEj7iBcPqjulQw9fLNinsDHMoOkoMNLoTZEkL0rUkz5kQVINUImZSo4oC8OesGRhkDZv3LG5Jk8Ng2K2jhpy7bIz+At6Y2xT+eqnbUeJaawF5p3UnNXm2/Q0tXl7fOQj2GLtXn/Aon6UlIXG01F7kWGSaFvxF2FvF+/HJfWybxst6P7pf6Bguj7SLB/H69AaLZP2P', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='Synthetic Document Generator. The pipeline of image rendering basically follows Yim et al. [67]. As shown in Figure 4, the generated sample consists of\\n\\n2https://huggingface.co/hyunwoongko/asian-bart-ecjk.\\n\\nKim et al. e ey i\\n\\nFig. 4. Generated English, Chinese, Japanese, and Korean samples with SynthDoG. Heuristic random patterns are applied to mimic the real documents', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 5, 'orig_elements': 'eJzVVE1P3DAQ/SujnJdsPhxnw62ClrZIvbSXCiE0tieJS2JHsdPtCvHfO1lAUNRLT6g5WTPjvDdv5vnqLqGBRnLxxprkFJJC1YpylEIWWBV5LlBJmYtdZUjoUqlkA8lIEQ1G5Pq7pLUDORxpvWy8W+LNhBPN6WTatXZNx8N0TOM0DVZjtN5tH9MDum7BjgLnrxJyXXLN0YkjN24ZFc0cr46R+ZkjKmaVY95IKetakspbUZSVEKQbYRQl93wj0q+4Fn89uNhTtBrOvV7WTuGCHM0Y/ZzCt55gshMN1hH4FuzI0MBYhmbrOlAYmPEwHKD1w+D3Ab7bESgCDilcyfo6hXcBQu/3DqyDD7ZbZgKxAcaE7gGHDAQcp4FAexdsiIGR1uafdPmCM5fZn/Rt5czkXw9F6EKUuyKrqmJX5NJkom3LsjK7TPPXmP9uKEUf4xROt9t+6TrWuUVNqfbb/rC4vfeuu/VbVh7dicI5npD+cZv+k2JaS9mUzY7qulS1EE2hs1wUGZUZy2fqN1BMvhTg8nmLCOgA9mV3n9Yl/GtXrZKF1rwDTVa1TaOLnDLZ7LBguXWm36Kr13tQsOwNtpngx6JlcqRbk6MyGomEyqs/9oD9koJInxzJTnnvusGGfgNnPVsy0AY+44QPJ3QGLv1M6B4NFWBvYw9Hi5/7ixQ+0jKzwdjsMxf7ESaMkWYXgCnCsUXGiB5GO3LRalL+3QDm8WkIL8fwYOYznFZNkvvr36lmqsY=', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='several components; background, document, text, and layout. Background image is sampled from ImageNet [7], and a texture of document is sampled from the collected paper photos. Words and phrases are sampled from Wikipedia. Layout is generated by a simple rule-based algorithm that randomly stacks grids. In addition, several image rendering techniques [13,41,67] are applied to mimic real documents. The generated examples are shown in Figure 4. More details of SynthDoG are available in the code1', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 6, 'orig_elements': 'eJxlUktv2zAM/iuEz54Xx14yb6cOw4YCWy8r0EMQBJRE20L1mixnCYr+91FO02LYyQbJ70Vx91SQIUsuHbQqPkHRCiGbrt12q3aLW1WvsWllt26bjz2hoE1RQmEpocKEPP9U9NqQQ0sZrLyb0yFgoFgF1efZ3E7nsLQxBKMlJu3d+5e2QTfMONDE/V1Bbij2XA1cObjZCopc3yyV+OZx3TVdh/2qbYToV5uOZK9qFEoiUSvqD8UzIxKdUh6e6EgRDUhvg3dMMn0GgfJxiH52qgTl5Zzzl5ARJaBTYPDs51TBl9c50JY9gZ5gQhsMKeijt3Cbq3eUYLfdX6C40MyRwPev3P/h0khsyBiSiUvLwiCMPvmpggcf1bRwhTHiRPzPbP/AH/SjDqQ0VvBjsZoFBnIcNPOJM9uYdAZAnA29E0zD1szgo05jlscEkSW8NWeYEsdkfNSK5W8doFI6P1IJ1+Vd0vMTKIraDZxRjk7/ntncrm7Kti432/3ic3ljFkserLZaMojx10Uw/z1Hf7NKpyXXS8bR/3GgHXzTQ95gW8FPz1/F96bNlDf66+zS+NV/v2gduYyCUzLmslJF9bK6mxDYrD7BTbWu8qFdb/AOIyvrI93n+3je/wViLwf1', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='and Appendix A.2.', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 6, 'orig_elements': 'eJxlUktv2zAM/iuEz54Xx14yb6cOw4YCWy8r0EMQBJRE20L1mixnCYr+91FO02LYyQbJ70Vx91SQIUsuHbQqPkHRCiGbrt12q3aLW1WvsWllt26bjz2hoE1RQmEpocKEPP9U9NqQQ0sZrLyb0yFgoFgF1efZ3E7nsLQxBKMlJu3d+5e2QTfMONDE/V1Bbij2XA1cObjZCopc3yyV+OZx3TVdh/2qbYToV5uOZK9qFEoiUSvqD8UzIxKdUh6e6EgRDUhvg3dMMn0GgfJxiH52qgTl5Zzzl5ARJaBTYPDs51TBl9c50JY9gZ5gQhsMKeijt3Cbq3eUYLfdX6C40MyRwPev3P/h0khsyBiSiUvLwiCMPvmpggcf1bRwhTHiRPzPbP/AH/SjDqQ0VvBjsZoFBnIcNPOJM9uYdAZAnA29E0zD1szgo05jlscEkSW8NWeYEsdkfNSK5W8doFI6P1IJ1+Vd0vMTKIraDZxRjk7/ntncrm7Kti432/3ic3ljFkserLZaMojx10Uw/z1Hf7NKpyXXS8bR/3GgHXzTQ95gW8FPz1/F96bNlDf66+zS+NV/v2gduYyCUzLmslJF9bK6mxDYrD7BTbWu8qFdb/AOIyvrI93n+3je/wViLwf1', 'is_continuation': True, 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='2.4 Fine-tuning\\n\\nAfter the model learns how to read, in the application stage (i.e., fine-tuning), we teach the model how to understand the document image. As shown in Figure 3, we interpret all downstream tasks as a JSON prediction problem.', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 6, 'orig_elements': 'eJzNUT1v2zAQ/SsHTi3gqrQtWVC3LBkyJEOzBYFx1B1tohRFUKe6RZD/3pNgIEaXrgW48N4HH9+9vBmOPHCSYyDzDUzL1rsWHe4sdWSbQ31A1Iutm7bf7RqzATOwIKGg8t+MD5ETDryIaUyzHDNmLlUmv3AXWH7nFcacY+hRwpi+XuGI6TTjiSfFXwynk3nVadbJMc2D46LzwzopHxl33b7r0Nt675y3h457T1t01CNz7baNeVeF8C9ZyVUN9yHxF5lTUP8FuuZ5DhJ5If9dgue+U/d966nr2q31tm317/W2qa2jmv6HEv65qNsS7rxwATkzDCNxhMhY0gTn8QIyQmGkDYS0Em7ywSSaAT6FiqsN+I8SP2/gwiCM/fnG9Oo2J+KiykQrRmM/L9VCGNSsgrsJJmWm5b37cJoLw361C0kz5sICGKPKLmkSTTaA4PRjAtQDD9+fHkE5FPo1YC6j081Vt1t9xFI0/k9+Xj7//voH0b3t+g==', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='The decoder is trained to generate a token sequence that can be converted into a JSON that represents the desired output information. For example, in the document classification task, the decoder is trained to generate a token sequence [START class][memo][END class] which is 1-to-1 invertible to a JSON {“class”: “memo”}. We introduce some special tokens (e.g., [memo] is used for representing the class “memo”), if such replacement is available in the target task.', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 6, 'orig_elements': 'eJyVUk1v00AQ/Ssjn0BKje3WdtsbEnDgECQaiUOIovHu2FnV3l32IxRF+e/M2kZF4sTF8r558+bN0+wvGY00kQ5HJbNHyNqql20jqGy7/rYpO1nVbX1PZUn35UNf3WYbyCYKKDEg8y9Zr0bSOFFqlkbHcLRoyeVW9ombyuGXncto7agEBmX0u7U8oh4iDuS5vs9ID9mBUcvIUcepI8d4MyPuL49U9F2LHVaFfJBF3dw1iPwo7upWVFWdXbkj0EtI5N2JQJIwkhwoD8Gh0iQhGBhIk8NAgPx6Jg2efkTSgiCcMIBADR2BMPpMLnCL0tyE8Pnpy3ZhOLKOPPti2XmKV455JgYbA9N746Z52xw+GQf0gpMdacOVhW9ETMmDGNF71a/RQED/vFkV/8/3/mn3/utu0TvsJ5rMYf9x+2EF4OdJiVMSK2+CuSnZR9pMdSPB62aX77EqSjG3zL/yERYo6S3INYdvlPJwRkae683EH0tC4bh48vCG8iHfwOIiDY2e7XMkr7EpPcxrzrP+GfKWg+rBR7bMHSOK+UyTEp5RjZhsr0kGdAOFObg8XdWfg9ui46DUmXbpGK6H3woJ/i4=', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='3 Experiments and Analyses\\n\\nIn this section, we present Donut fine-tuning results on three VDU applications on six different datasets including both public benchmarks and private industrial service datasets. The samples are shown in Figure 5.', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 6, 'orig_elements': 'eJzNUU1vEzEQ/SujPZewn8maG1JB4sIpcKmqaGyPdy28Xsv2pqmq/nfGoYKIC1du9rz3Zt68eXipyNFCPp+srj5ApWgUezO0h7Y7qL5HIQU2dd30auwRpajuoFooo8aMzH+pjHXkcaEi1qvf8ilgoLgL2hRugfNzuMIYgrMKs139+zfYoZ82nCgx/lCRn6pHrgaunPy2SIpc318r8Y/HVnRCoKn7TkpT7wUpoxuUWiFRL5uhemVFpksu5A4+XdiPLTsmQK/ho0f3nHhkYb1ZO9rsqOj+zsO0B9l3eOhbGsZGt4IH1KYRahz39dCo/yGPf97sNo8vHvJsEyRSZfAdPBGESInbwX3xC8Z6epc3b/0EDGyOc1uLKhLB9/tvcOP7iiR7AW2NoeIJShKJWGO9cpsuXeSaZwibZBVI8mpeMP74dYwQ7RkzMVlvKUeLjp3Fs1X0u9EOjjNBwiU4YlHk97w+eZbAZztt/B92t7f8ijGyuTMdy8qvjz8BWCXyDg==', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='3.1 Downstream Tasks and Datasets\\n\\nDocument Classification. To see whether the model can distinguish across different types of documents, we test a classification task. Unlike other models that predict the class label via a softmax on the encoded embedding, Donut generate a JSON that contains class information to maintain the uniformity of the task-solving method. We report overall classification accuracy on a test set.', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 6, 'orig_elements': 'eJzNUj1v2zAQ/SsHzY5qSdZX13jqkA510SEIjBN5tAlTpEBSdowg/71HxUXTdOjahcO9e/fevePjS0aGRrJxr2X2GTIqCypaahtJZd227TCsOyXXTd+LumvqPltBNlJEiRG5/yVT2pDFkRJZOjvH/YQT+XySKvUmOF6nBcZpMlpg1M5+usEG7WHGAwXGHzOyh+yJqxNX9nYeB/Jcb5aK/+2x7Ku+R7XeVMOg2BoJJQscpECizVDU2SszIj3H1FzlBWzdxYboCUfYYTgFQCthyxsEiiHZ+OVwp6OhRP8rFonUDF1X1lVf9gpL0W0GVVR9t0bV1s3/EMs/T/c+lq0Tc9oP7g2GoNXNQA47B4EILkeKR/LAD4xOkgGBFqQOUbM1HY6AwrsQuKQUJReQ9gngFMjb7LCCC0GkEAFB/KEDnP0ph+/W6BOBW6QWmcCKGGHyJLWIi/zCBIMDmzhr5FnBqTjiM6Q53EBWMFUCcTJSsr8VX5wjhwNZ8hiJKV++fX14Gy2cjahtuM3VVjk/3kw5GBlK8DJ4tjqBOl7TWqmSbN8FZ86sAnzwo5M5/CDwNDkfwZ1Zz5iPy6IQs0dxTYbxLRD+efn7n/eAnp3qM+3SgV6ffgLY2zSD', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='RVL-CDIP. The RVL-CDIP dataset [16] consists of 400K images in 16 classes, with 25K images per class. The classes include letter, memo, email, and so on. There are 320K training, 40K validation, and 40K test images.\\n\\nOCR-free Document Understanding Transformer  7 \\n\\nFig. 5. Samples of the downstream datasets. (a) Document Classification. (b) Document Information Extraction. (c) Document Visual Question Answering', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 6, 'orig_elements': 'eJzFUstu2zAQ/JWFTi3guJQsklJvhdMCRoo+UjcXwzAocukQkCiBpOIUQf49pGw3RtF7jtqZFWdmZ/OUYYsd2rAzKvsIGSOFkrWoeak0ZURLxsqS1xWXeUPysspmkHUYhBJBRP5Tpk2LVnSYllVvx7AbxIBuPiiduAkOf4YJFsPQGimC6e2HE9wKux/FHn3ENxnafbaN0yFOdnbsGnRxzqaJe9WIRY45R84UFpRz3jSk0oqwupa0YrTOnuNGwMeQyLd3X6+W16sfc1jfI5y/IOn3GGCTsy3I3nrjg4deQ0nIDZguaQJjIWcgW+E9+hkcTLiHgv6Fo80jePz3iRe3ZDsqhGg8oJtBh10/A+yEaWcgrALfQ2+nHYcQjcGiiG8GJ4w1dj+LEm7gQbRGTVEdd9IsoA+nt+cpvHOu34RzkfqA6+Q5mv/3qETrnDZYSFZxxgkjSJWoF6SpdF4qKt/gqPzyRt+Xt1faIcJ1L8ekG35bhc6H6DwmAmsnrNe962LgwOHS+yrF8T/PnNWkxAXVTbEgKl/kWOesKXPCRM0o0rf2/MXs50Dn8Et0Q4tT9UIskeoP1geHojtXNLbrnXj/Gs0y1czo0/sRbC7AlU0xTQh8foyVkieSvCDdGT+KFn6OsU+J+Mn6A7oY9GWwUd/ocCmGRMmety+QC1Z2', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='Document Information Extraction. To see the model fully understands the complex layouts and contexts in documents, we test document information extraction (IE) tasks on various real document images including both public benchmarks and real industrial datasets. In this task, the model aims to map each document to a structured form of information that is consistent with the target ontology or database schema. See Figure 1 for an illustrative example. The model should not only read the characters', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 7, 'orig_elements': 'eJxVUk1PG0EM/SvWnooUpQlpA/RckLj0Um4IRd4Z7+6I+dLYA4kQ/72eDQ3hlM2z/fye3zy+deQpUJSds90v6Lbbnzerm/VmtUFaXW2H9eYae7NZr7c/LvH6su8W0AUStCio/W/d4DxFDNSGbYpVdhkzlWW2Q+ttZTnkuYw5e2dQXIrfP8oe41hxJNb6Y0dx7J4UzYrsYg09FcWv3hUS2kvj+J1MbXLhPg6phJkMbvdS0LTPJTwkYCKQiSAkSx6G6v0BarRUWDBanmsmhexpDx4PqQqDFhSLbQ2Di2A/9vACXpWNWE6Qlj9X02k1fLu/vQBBfmbQfy9YXKoMhdCfjYZmVhmMr9bFEfokE+Ta62Ggp2imgOX5KGeedNFWluIaiZ6cSXip3tWD43nZ4swquqBggoAZCM30uVdBBOWpRmohC80ApOGLFZlQBXI7AzuWNvbqVF3jFywjifqS5NN4gFRmOb0KAjYTBVzCXz37nRuVH9ZtgZoA532TrwteSG+F7eia0UkxT6l6CzE1bo1JPdtjPhO2u2pmen/vF9BXAfSczpKcG88DZNURxZn5CGqNytwyllTzsSNqkGp/clSwGP1ROCTNYTbZwl+2Z/n/xf7BctT+0J7f+9M/tvAhnA==', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='well, but also understand the layouts and semantics to infer the groups and nested hierarchies among the texts.', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 7, 'orig_elements': 'eJxVUk1PG0EM/SvWnooUpQlpA/RckLj0Um4IRd4Z7+6I+dLYA4kQ/72eDQ3hlM2z/fye3zy+deQpUJSds90v6Lbbnzerm/VmtUFaXW2H9eYae7NZr7c/LvH6su8W0AUStCio/W/d4DxFDNSGbYpVdhkzlWW2Q+ttZTnkuYw5e2dQXIrfP8oe41hxJNb6Y0dx7J4UzYrsYg09FcWv3hUS2kvj+J1MbXLhPg6phJkMbvdS0LTPJTwkYCKQiSAkSx6G6v0BarRUWDBanmsmhexpDx4PqQqDFhSLbQ2Di2A/9vACXpWNWE6Qlj9X02k1fLu/vQBBfmbQfy9YXKoMhdCfjYZmVhmMr9bFEfokE+Ta62Ggp2imgOX5KGeedNFWluIaiZ6cSXip3tWD43nZ4swquqBggoAZCM30uVdBBOWpRmohC80ApOGLFZlQBXI7AzuWNvbqVF3jFywjifqS5NN4gFRmOb0KAjYTBVzCXz37nRuVH9ZtgZoA532TrwteSG+F7eia0UkxT6l6CzE1bo1JPdtjPhO2u2pmen/vF9BXAfSczpKcG88DZNURxZn5CGqNytwyllTzsSNqkGp/clSwGP1ROCTNYTbZwl+2Z/n/xf7BctT+0J7f+9M/tvAhnA==', 'is_continuation': True, 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='We evaluate the models with two metrics; field-level F1 score [22,65,18] and Tree Edit Distance (TED) based accuracy [68,70,23]. The F1 checks whether the extracted field information is in the ground truth. Even if a single character is missed, the score assumes the field extraction is failed. Although F1 is simple and easy to understand, there are some limitations. First, it does not take into account partial overlaps. Second, it can not measure the predicted structure (e.g., groups and nested', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 7, 'orig_elements': 'eJxNU0tv2kAQ/isjn0ByCZACSXuqFCLl0kuReiAoGtaDvcrau9pdQ6wo/73frknbA5KZ1/eY2f17IUZa6eKLropvVBzXy9s7WX3lxZzv79VK1OLIclyf5psNH1VVlFS0ErniyKh/L07aSMetpObKdn18cezEz1x1SrUpHQeX0+yc0Yqjtt3NNW24q3uuJSC/L6SriwOiDpGXrm+P4hHffCAU5S2mGb+F5Mym5ygUG6HWVmICXXRsKF4sgZvXKnynkxZTfTFyFkOPCwrKeqH9clmuV+Xi7kDcVbTzIrStdKQHHSJ3Smiy2z5M6chBKmKles9qoP36rtzMy+XtYUY7gGKeakS9ArcRsPCZChiiOqIxY5PuTta3WS7pgL+5qva2B3T0fWxmtD0LkidiCrqrjWAu5yE+tbQ6gEeZ+0YBHELfSsiREeWKegU5MfyuZvTDxMb2dZOoIhx06zA8aRYOA0VLICE+iR7np9n4BdsKGd3qmHmHGT1qVJUEjyoL4M5GivwqkIMhcAhqIjn2UbMhexZv2KHtlyibRqNPcZfbWkD3flyb81Lp7FWAEyqm+ERm9azMBrmQuXYSUkmjxbNXzTCF/TZ5ICFkLDbm75ZKukCDCZAWklSbFzPew3Wh8AjrTes8JNE8cjtK6sDWrEfbAJ2qTy8ikBfwDPhMxwDTcS6Q9hSTpYqN6g2PqZJafpvMS1o898vlYgmUifPQEqc36fu5n99Wq/x/WqajgdwarqaaJHTMU15HyET+P5Pyn11jubQuDiMdcAxOsH/c+QDbsTrDn7JD3mkWh+vzoiCFLtbjcGHFp0562tJ+PO/0JD9f60/2ntPcXXp6H4c/JA1tSw==', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='hierarchy). To assess overall accuracy, we also use another metric based on TED [68], that can be used for any documents represented as trees. It is calculated as, max(0, 1−TED(pr, gt)/TED(ϕ, gt)), where gt, pr, and ϕ stands for ground truth, predicted, and empty trees respectively. Similar metrics are used in recent works on document IE [70,23]', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 7, 'orig_elements': 'eJxNU0tv2kAQ/isjn0ByCZACSXuqFCLl0kuReiAoGtaDvcrau9pdQ6wo/73frknbA5KZ1/eY2f17IUZa6eKLropvVBzXy9s7WX3lxZzv79VK1OLIclyf5psNH1VVlFS0ErniyKh/L07aSMetpObKdn18cezEz1x1SrUpHQeX0+yc0Yqjtt3NNW24q3uuJSC/L6SriwOiDpGXrm+P4hHffCAU5S2mGb+F5Mym5ygUG6HWVmICXXRsKF4sgZvXKnynkxZTfTFyFkOPCwrKeqH9clmuV+Xi7kDcVbTzIrStdKQHHSJ3Smiy2z5M6chBKmKles9qoP36rtzMy+XtYUY7gGKeakS9ArcRsPCZChiiOqIxY5PuTta3WS7pgL+5qva2B3T0fWxmtD0LkidiCrqrjWAu5yE+tbQ6gEeZ+0YBHELfSsiREeWKegU5MfyuZvTDxMb2dZOoIhx06zA8aRYOA0VLICE+iR7np9n4BdsKGd3qmHmHGT1qVJUEjyoL4M5GivwqkIMhcAhqIjn2UbMhexZv2KHtlyibRqNPcZfbWkD3flyb81Lp7FWAEyqm+ERm9azMBrmQuXYSUkmjxbNXzTCF/TZ5ICFkLDbm75ZKukCDCZAWklSbFzPew3Wh8AjrTes8JNE8cjtK6sDWrEfbAJ2qTy8ikBfwDPhMxwDTcS6Q9hSTpYqN6g2PqZJafpvMS1o898vlYgmUifPQEqc36fu5n99Wq/x/WqajgdwarqaaJHTMU15HyET+P5Pyn11jubQuDiMdcAxOsH/c+QDbsTrDn7JD3mkWh+vzoiCFLtbjcGHFp0562tJ+PO/0JD9f60/2ntPcXXp6H4c/JA1tSw==', 'is_continuation': True, 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='We use two public benchmark datasets as well as two private industrial datasets which are from our active real-world service products. Each dataset is explained in the followings.\\n\\nCORD. The Consolidated Receipt Dataset (CORD)3[45] is a public benchmark that consists of 0.8K train, 0.1K valid, 0.1K test receipt images. The letters of receipts is in Latin alphabet. The number of unique fields is 30 containing menu name, count, total price, and so on. There are complex structures (i.e.,', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 7, 'orig_elements': 'eJzFUsFu2zAM/RXCpw7I3MaOY3fXdqcOG1AU2CEIAlqmY2GypElUkqHYv49KUgzYgF13skE+PfI9vs1rQYZmsrzTQ/EBClz1PbXrqhpoWTftWDVV0y2bNY0t1UPXFwsoZmIckFHwr8WoDVmcKT8enE288+gplH4YMza3+Yc/t9F7oxWydvb22jZo9wn3FKW/Kcjui61UvVR2Ns09Bam3P6XEdOLM8ZUgRQI+OvCpFzroyappxvAN8k6ROAJGOJIx+XsGBn1AJtB2SJGDRvMbepy0mgADwRjcDC4FQMX6QBAIzfujC2aASOGgFQmRG5LiWMJHlFdXEtAR6OQNakuDDAGehM0Z447a7mOZdb5Z8BlDwEz/kvWIsD/9X3d1TTjWY9uN2NRttVqqZlxVqhvr+6pV/9v/hy/PjyW8iMIHZ6MzWhYR1c+kSHuGx6slNxn3rt6smm22B/8+Fk/IoIRDRzmDG+Gu7J6Ag7i4kP/lExxQ2K//TJHlIpches4bX7YQdUzhTHBtxzxQrvBJhFpA4yfsiS/oi6YMTlZ/T3ImTWY4v6jv8jYs4+VoIBdJkG1dSDVZXgA7ltxIlJTU0EooHDh7ppXw5AApN3tDJ5CISUhSoAg3uqRy8Y8AbH8B+u8tRA==', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='3https://huggingface.co/datasets/naver-clova-ix/cord-v1.\\n\\nnested groups and hierarchies such as items>item>{name, count, price}) in the information. See Figure 1 for more details.', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 7, 'orig_elements': 'eJzFkEFv4yAQhf/KiNtKSQxxbJweetzjXtpbVUVjGGwkGxDgqFXU/16IWu1qD3vdC4O+Nwxv3suN0UIruXyxmj0AM6cjYt8pY1DIkZvR6KFvNdeK67anju2ArZRRY8bSf2PGLuRwpfpYe7flS8BA8RC0qb1Vzu/hLmMIi1WYrXfNl7ygmzacKBX9hZGb2GuhoZCL29aRYuHyo6BMb7nOaOecQ3pomnmbJusmg4oOyjfVT6KcGodXinu1+Cvu7VujfNT7qzjUz759/MIYi4srPdehZfrfIYxSd/yMRrZjZ/S556PgrTRngxxJD6f/EMJwJ/G3x14K5EchJJpBqmMrxak/moFzPXS9ER37MzVHKZOGKfotJECnYbYUMapSEqRNzYAJbKY1Pdbz8Va32YHym8s7CNEq+vgB1kGeqRTj43pf4QBPRPDTTlskEFA4rL5cdUnHLulfsb9+Arzcz8E=', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='Ticket. This is a public benchmark dataset [12] that consists of 1.5K train and 0.4K test Chinese train ticket images. We split 10% of the train set as a validation set. There are 8 fields which are ticket number, starting station, train number, and so on. The structure of information is simple and all keys are guaranteed to appear only once and the location of each field is fixed.', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 8, 'orig_elements': 'eJxFUsGq2zAQ/JXF0FtwJTeOnXftsdBToIcQwsZaxSKybKT16wuP9+9dKQkFY6yZ2dlZrY+fFXmaKPDZmeoNKtNvd+3WqhabPbad0ts9ql6ZzjR7tUddbaCaiNEgo+g/K+s8BZyoFM9h5fOCC8V6MTZrM833pdC4LN4NyG4O35+0x3Bd8UpJ+GNF4VqdBF0EOYd1ulAUvC9I/J9x12lUjdYd2r4bmh+d3u4a2ytl+nZndVt9SQXTB2fxwQ034hoOo0sgD8KyXiQGXCgM44TxBnmWRAxH3ZyAR2QY5pBc4gSzBV23v4AjugAYDKh6K0dKDD9HFyjRk+PSB9yUp6nhD0GSaRm0+pZdeHwJcyfMOd7RO1NuI2M5IUUCGRR6sI68SfB3dMNYoKf941I2kBgju3DNH9lh8zR/8TlpmmEOxVZUcR14FR+J4oKd4/RoLBeS3LR4KhXoPdzonkpH2UvEwEQGeAbZHWEUQ3+X1/DQ56H8/NhodiaUtCV6Nrbug0ydt/z6AX5jjCJ+p0NeztfpHzja1AY=', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='Business Card (In-Service Data). This dataset is from our active products that are currently deployed. The dataset consists of 20K train, 0.3K valid, 0.3K test Japanese business cards. The number of fields is 11, including name, company, address, and so on. The structure of information is similar to the Ticket dataset.', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 8, 'orig_elements': 'eJxFkctuGzEMRX+F0KoFpu7M+DGTLttu2gDd1LsgMBiJcoRqJEGijBpB/r2U7TQ7go/Lc8mHF0WeFgp8cEZ9AWVo2JlxMw6DGacRdT/3dHe3xs1m6tfTtFUdqIUYDTJK/4uyzlPAhS7DMVQ+JEyUV8nY1tvKfE6XMqbknUZ2MXy+lT2GY8UjFak/KApH9SjZJJlDqMsTZcnPl0x+Z9xNA/ZCOKGdJz2up2GzG+3c92be7uywVa8ywfSXW/PXWlygUuAbZgMffoRPvymfnCb4LhY+rmD/7Ao0O4UYJLQ5LhBrBtTsTgQpR1M1F+BnZBAO0DU3Gn8GQ8nHM5mmQv9FdAzFFZmIFsb+HjijCx30q/U9nNA7c4uZCsNPTCh8BE9voFpAy1XxeoOmYx15UxrfMHTggvbVuHCEdvpONi6icu4AjcmiIUEwUCLEcBUqnMVDFXjRcsHGvFz+0ASLW5zHDBzFIsHe6T9i4mZm1b709sBfmDO2o+zbcV8f/wGYe70+', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='Receipt (In-Service Data). This dataset is also from one of our real products. The dataset consists of 40K train, 1K valid, 1K test Korean receipt images. The number of unique field is 81, which includes store information, payment information, price information, and so on. Each sample has complex structures compared to the aforementioned datasets. Due to industrial policies, not all samples can publicly be available. Some real-like high-quality samples are shown in Figure 5 and in the', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 8, 'orig_elements': 'eJxdUstu2zAQ/JWFTi1gu5YfstJzWqAw0EOTWxAYa3IlLUqRDB9OjCD/3qXspGhv5OzucGa4D68VGRrJpgPr6itUat2qtdJd22m12WFzc1yt8Wa9q7FuaNNQNYNqpIQaE0r/a9WxIYsjlWHtbE4Hj57Cwuuu9JZyOvupjN4bVpjY2S/XskHbZ+wpSv2hIttXj4J6QQ42j0cKgrcTEv5qbETNclXXO+zanVqJuE2z6trlUrfbpqu31ZtMJHpJpfkXKWKf4NMPO7+jcGJFcCviPy/gfuAIxUikBHJEEx10wY3gLIHrwOUAgdCAD05nlWKZoY8R5WzkmGJp3Sz3kAKynUG9hxMa1tMpUUywd8Jiheoihcfi+MJ1cVkYsuWnTNAxGV3UtPUMngdWA7BVJmuKEJMQybVzYZxinIHHc/m9/8BQXP4DodUg7pxdwDcUzoijNwQDRrFRji/CHsRjDnSBJHENyUESlShM05YIl6DXAMTCrSiWHrY6yziXqJz8MVOcgXVJIjXXp4RUIvD5KGVzhqOQnpANHg0t4M6NNCU9N/xbVHE/zJ+yhJjOH+OiB+Lgnq28Bt+5F6GwnXzJvYiMWfZrUonhDGKciqBFWbP3DfyJIUggJ7ov2/H2+Ad5UwaN', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='supplementary material.', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 8, 'orig_elements': 'eJxdUstu2zAQ/JWFTi1gu5YfstJzWqAw0EOTWxAYa3IlLUqRDB9OjCD/3qXspGhv5OzucGa4D68VGRrJpgPr6itUat2qtdJd22m12WFzc1yt8Wa9q7FuaNNQNYNqpIQaE0r/a9WxIYsjlWHtbE4Hj57Cwuuu9JZyOvupjN4bVpjY2S/XskHbZ+wpSv2hIttXj4J6QQ42j0cKgrcTEv5qbETNclXXO+zanVqJuE2z6trlUrfbpqu31ZtMJHpJpfkXKWKf4NMPO7+jcGJFcCviPy/gfuAIxUikBHJEEx10wY3gLIHrwOUAgdCAD05nlWKZoY8R5WzkmGJp3Sz3kAKynUG9hxMa1tMpUUywd8Jiheoihcfi+MJ1cVkYsuWnTNAxGV3UtPUMngdWA7BVJmuKEJMQybVzYZxinIHHc/m9/8BQXP4DodUg7pxdwDcUzoijNwQDRrFRji/CHsRjDnSBJHENyUESlShM05YIl6DXAMTCrSiWHrY6yziXqJz8MVOcgXVJIjXXp4RUIvD5KGVzhqOQnpANHg0t4M6NNCU9N/xbVHE/zJ+yhJjOH+OiB+Lgnq28Bt+5F6GwnXzJvYiMWfZrUonhDGKciqBFWbP3DfyJIUggJ7ov2/H2+Ad5UwaN', 'is_continuation': True, 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='Document Visual Question Answering. To validate the further capacity of the model, we conduct a document visual question answering task (DocVQA). In this task, a document image and question pair is given and the model predicts the answer for the question by capturing both visual and textual information within the image. We make the decoder generate the answer by setting the question as a starting prompt to keep the uniformity of the method (See Figure 3).', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 8, 'orig_elements': 'eJxNUsFq4zAQ/ZXBpxZCNk6a2O2tUAp7WShbuodSwlgaOyK2pJVHyYbSf9+RUrc5WX5v5r03I72+F9TTQJa3Rhd3UKxIlQpXtG6qurzZ4OJWteub+rZSVDdrtSlmUAzEqJFR6t+L1vRkcaDUrJ2NvPXoKcy9blNtovnkM43e90YhG2d/fNI92i5iR6PwrwXZrngT1AuytXFoKAheZyR8Z9xUJS6WZVlhW1dquaok6LKtFwtdrzdtuS4+pIPpH6fiB6dimg9ezBixh6dIY0oA93Y8UjC2m8OzgwP2RmYi4B1BG4N8Aij0qAyfwLUZH5ymfgZHAuWsjooBQU/6h7P+30kfJ31gHPdwJUFenu6v5/DTipgZMzy7VDCDzC19+lvEowkgtZ05kM3UVw7wgbRRPGbo7AatC/n3S6A5pSk45iCN492UM2vJjtLZWOkb8sXA0Ug4m0Vynjn8EUPcnzejSYl3gI4shWldn95iNRJznvgyAo4y5MgYMuWDGzwDO9gT+VwZrUn+l4sm3jkNV7+J4NF0MRCsrufpwUxv6RcG8ZetPKd7/nj7D2hS9Nk=', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='DocVQA. The dataset is from Document Visual Question Answering competi- tion4 and consists of 50K questions defined on more than 12K documents [44]. There are 40K train, 5K valid, and 5K test questions. The evaluation metric is ANLS (Average Normalized Levenshtein Similarity) which is an edit-distance- based metric. The score on the test set is measured via the evaluation site.\\n\\n3.2 Setups', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 8, 'orig_elements': 'eJzVUstu2zAQ/JWFTi1gu3pL7s1Abw4MBDZyMQxjRS4tAhKlkpTSNMi/d6m4SNA/6JEzu7Mzuzy/RtRRT8ZftYy+Q0RKFWq7zVRWiTqr8m3aNE1dFFIlVMZSRCuIevIo0SPXv0ZKd2Swp9AsBzP564gj2c0oVagNtH8ZFxrHsdMCvR7MtzvdoblNeCPH/Dkic4sujI6MXM3UN2QZrxfEfngsqwTjNEkqVHUl0qxK8jJVdRzLuihVUkRv3OHplw/FPwbx9LjbwKklCKYdedAOlB16YG4K2eFJuwk7eJzIBXewM+6ZrDY3EEM/ktdrCHgOaCRDxmnnHQwKingPP+9dDiQpbUgCK/SDJfAtGkjSPcj7IAfnPL8sZpjmTJCzgLeozQqKPczYablapvDLs+6H+nsE4pJpWSHwGawWIczu8HCEL7uZLC8ODoPtWec3G3mgmYxrPWkDR93rDq32L1/hudWiDZ3sj6T2a8mB0AhaQ8Mbknft95FOhCw80PNj8XTfYU/oJsvVs8aF/GTOaU+bcOG/xz+gtUzNdAqH4Qv9+/MyVWdFgVVZYp7lTVLWWZbEVJKgGhuh/rufl21SOJKfRvd5DyftO4reLn8AAbAhQw==', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='We use Swin-B [40] as a visual encoder of Donut with slight modification. We set the layer numbers and window size as {2, 2, 14, 2} and 10. In further consideration of the speed-accuracy trade-off, we use the first four layers of BART as a decoder. As explained in Section 2.3, we train the multi-lingual Donut using the 2M synthetic and 11M IIT-CDIP scanned document images. We pre-train the model for 200K steps with 64 A100 GPUs and a mini-batch size of 196. We use Adam [30] optimizer, the', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 8, 'orig_elements': 'eJxNU9uK2zAQ/ZXBz3HqW9ykb9kulFC2LN0tfQghTKRRIpBlIcmbTUP+vSM7vRiD7ZkzZ86ckbfXjAx1ZONey+wTZItayaZSslwdimapBC2WWNcLuar4qmWRzSDrKKLEiIy/ZkobsthRKpa9HeLeoSM/d1IlbErHixvT6JzRAqPu7Yd72qA9DnikwPltRvaY7TjqOLK3Q3cgz/HlGPH/NNZqWS8W+LFtsambQ9ku67osqCVBSzwIld24ItJ7TOCfBEMgeDlrmz/Atil2gAEQ3nQY0ABZ0Uvy0Ct4TOrhrOMJgtHHU4Sul1rdFc+BmQJFiCcCgxeumRQymZVcZmV/hqB/UeK/VjPgu2z4cRsBZTGHjQU1eCbwIHobNDceuVP3RBsckcxRiMGjuED0KCnvlZrBeZoigZT2IYLqBz/JCKn6Yf39dZpL0jjQHNYB6N0Z1JYkaAsvJMZe1bwe+Zido4mxG0zUudFpFeZuwxD4c8xWTxAult+iFtMk5RNsNq/558fNMwSBNjWQvRjSMQLdpXWObjlP+X9dWJZh3R6qovgKIZILk9ttA+uyKODL84/JTIROW50fMIrTZCmPWK7akTX5sJbYwbbmXfYu6o4RfjYthtDbpJyNJdCB9Z1IDoYVJuIEYeaoec6/CP4BRCSZjuOfk/oNfdrMG72mU3Tb/Qa4Iwsl', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='learning rate is scheduled and the initial rate is selected', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 8, 'orig_elements': 'eJxNU9uK2zAQ/ZXBz3HqW9ykb9kulFC2LN0tfQghTKRRIpBlIcmbTUP+vSM7vRiD7ZkzZ86ckbfXjAx1ZONey+wTZItayaZSslwdimapBC2WWNcLuar4qmWRzSDrKKLEiIy/ZkobsthRKpa9HeLeoSM/d1IlbErHixvT6JzRAqPu7Yd72qA9DnikwPltRvaY7TjqOLK3Q3cgz/HlGPH/NNZqWS8W+LFtsambQ9ku67osqCVBSzwIld24ItJ7TOCfBEMgeDlrmz/Atil2gAEQ3nQY0ABZ0Uvy0Ct4TOrhrOMJgtHHU4Sul1rdFc+BmQJFiCcCgxeumRQymZVcZmV/hqB/UeK/VjPgu2z4cRsBZTGHjQU1eCbwIHobNDceuVP3RBsckcxRiMGjuED0KCnvlZrBeZoigZT2IYLqBz/JCKn6Yf39dZpL0jjQHNYB6N0Z1JYkaAsvJMZe1bwe+Zido4mxG0zUudFpFeZuwxD4c8xWTxAult+iFtMk5RNsNq/558fNMwSBNjWQvRjSMQLdpXWObjlP+X9dWJZh3R6qovgKIZILk9ttA+uyKODL84/JTIROW50fMIrTZCmPWK7akTX5sJbYwbbmXfYu6o4RfjYthtDbpJyNJdCB9Z1IDoYVJuIEYeaoec6/CP4BRCSZjuOfk/oNfdrMG72mU3Tb/Qa4Iwsl', 'is_continuation': True, 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='4https://rrc.cvc.uab.es/?ch=17.\\n\\nTable 1. Classification results on the RVL-CDIP dataset. Donut achieves state-of-the-are performance with reasonable speed and model size efficiency. Donut is a general purpose backbone but does not rely on OCR while other recent backbones (e.g., LayoutLM) do. †# parameters for OCR should be considered for non-E2E models', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 8, 'orig_elements': 'eJzFUk1v1DAQ/SujcAGpm43jrJNUQhzaHpDKh6qKS6lWY3uyicjake20LFX/O5OlBdEDV27WezNv3nvJzUNGI+3Jpe1gs1PIqroUddc0QmxKpTpZFaZVqq1VJTU1ymYnkO0pocWEPP+QdcNIDve0LFvv5rSdcKKQT7ZbZhc6HaYjjdM0DgbT4N36iR7R7WbcUWT+JiO3y24ZnRjZunmvKTDeHJHwx6PsGrnZYK0UVrLSQjVSioIUGWpQmy575I1E39MxUJ/SFE/X6xBMbu5MPqPOKa7fmf6tqPPFxLO/jxgCu7uj62WXRV6W05imlaXVspAl1m1HakNtUQipUJIty/9QTvuyHLUpzQarQrRtVWmSZUV1pbEVoml0JcRf5VyjHglEDmcjxjh0TwYgUJzHFIGfqSe4+nK5Ojt//xmWYJFSDudLGEDTD3RHEWLCRCvfrXh6xW6AQ3Y+7NEZgvsh9ayI0bvjuTgRWUBnYe8tjRCHHwTU8fGBnDk8iw8REHbkKOAI0xwmHwk0mm/aO37whPV82vnE4uNh8frp7Aruey4VPBsJjBsu5vdShNeU7/ITuMSDn9PlhzcskcPXuSzK4hVwjfypEoUI7P0oFns/jxY0gfEuDpYCO19I593qorz4lSD+4ye6/Qn0UBgO', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='OCR #Params Time (ms) Accuracy (%) ✓ 110M + α† BERT ✓ 125M + α† RoBERTa ✓ 113M + α† LayoutLM LayoutLM (w/ image) ✓ 160M + α† ✓ 200M + α† LayoutLMv2 1392 1392 1396 1426 1489 89.81 90.06 91.78 94.42 95.25 Donut (Proposed) 143M 752 95.30', metadata={'text_as_html': '<table><thead><th>OCR</th><th>#Params</th><th>Time (ms)</th><th>Accuracy (%)</th></thead><tr><td>BERT</td><td>110M + «</td><td>1392</td><td>89.81</td></tr><tr><td>RoBERTa</td><td>125M + o</td><td>1392</td><td>90.06</td></tr><tr><td>LayoutLM</td><td>113M + « %</td><td>1396</td><td>91.78</td></tr><tr><td>LayoutLM (w/ image)</td><td>160M + «</td><td>1426</td><td>94.42</td></tr><tr><td>LayoutLMv2</td><td>200M + o</td><td>1489</td><td>05.25</td></tr><tr><td>Donut (Proposed)</td><td>143M</td><td>752</', 'filetype': 'application/pdf', 'page_number': 9, 'filename': 'donut_paper.pdf', 'orig_elements': 'eJx1U9tuozAQ/ZURq0qpdgW2MbcqitS9vLXaKspbt4rGMEmRuImY7kZV/31twgLbpA9g5ozn4Dlz/PjqUEElVXqbZ84NOMGOK0qDjAQiCkHRzlcsUmpHcciCOHG+gFOSxgw1mv2vzi4vqMKSbHFWV53eNthQ6zbZzu61aX1s+jQ2TZGnqPO68oZ0gdW+wz0dTP7RoWrvPBm0Mci26kpFrcGTHmmnM4aBSAOUjCeJlIp8ISmSChPO41hJzi2xpj96i4ftsy4LW7PUqApaLfUzYWaX1c9v66VnVvv96QFbLA9jvMlLgkV5uB6R2zTtWkyPsLgaQO8fVWuebPX1x3pjsKwPOGf38Bl+dYyhmlA/EWMQJ27MT5HXU5xo1rUlwqlGBJapvkySMJeF5yR3eKw7fXc/O48/nQeu5mThRMbdKP6YDBa/PchLM5rrqT682KcUM1bpSvEx68vUjGDsXasyTsaABa4Iznm+W8vB4qGtm/pA2exo0p/6j4KZZoHrsxmRd3KG8za4xrrFeAMGT8DoBZh7wPQsIu7DNGlfcYMxwcBOcMyf5vdffpjxROGfbRlFv6D+WBee/3pIDVJepHwRYD00vkKw8wIrNvSmhN5V0NsB+vFBYsWH91qD1RiMttBr2l+74apvrKbO29NfDF5FtQ==', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='', metadata={'text_as_html': 'td><td>95.30</td></tr></table>', 'filetype': 'application/pdf', 'page_number': 9, 'filename': 'donut_paper.pdf', 'orig_elements': 'eJx1U9tuozAQ/ZURq0qpdgW2MbcqitS9vLXaKspbt4rGMEmRuImY7kZV/31twgLbpA9g5ozn4Dlz/PjqUEElVXqbZ84NOMGOK0qDjAQiCkHRzlcsUmpHcciCOHG+gFOSxgw1mv2vzi4vqMKSbHFWV53eNthQ6zbZzu61aX1s+jQ2TZGnqPO68oZ0gdW+wz0dTP7RoWrvPBm0Mci26kpFrcGTHmmnM4aBSAOUjCeJlIp8ISmSChPO41hJzi2xpj96i4ftsy4LW7PUqApaLfUzYWaX1c9v66VnVvv96QFbLA9jvMlLgkV5uB6R2zTtWkyPsLgaQO8fVWuebPX1x3pjsKwPOGf38Bl+dYyhmlA/EWMQJ27MT5HXU5xo1rUlwqlGBJapvkySMJeF5yR3eKw7fXc/O48/nQeu5mThRMbdKP6YDBa/PchLM5rrqT682KcUM1bpSvEx68vUjGDsXasyTsaABa4Iznm+W8vB4qGtm/pA2exo0p/6j4KZZoHrsxmRd3KG8za4xrrFeAMGT8DoBZh7wPQsIu7DNGlfcYMxwcBOcMyf5vdffpjxROGfbRlFv6D+WBee/3pIDVJepHwRYD00vkKw8wIrNvSmhN5V0NsB+vFBYsWH91qD1RiMttBr2l+74apvrKbO29NfDF5FtQ==', 'is_continuation': True, 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='from 1e-5 to 1e-4. The input resolution is set to 2560×1920 and a max length in the decoder is set to 1536. All fine-tuning results are achieved by starting from the pre-trained multi-lingual model. Some hyperparameters are adjusted at fine-tuning and in ablation studies. We use 960×1280 for Train Tickets and Business Card parsing tasks. We fine-tune the model while monitoring the edit distance over token sequences. The speed of Donut is measured on a P40 GPU, which is much slower than A100. For', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 9, 'orig_elements': 'eJxVU11v2zAM/CuEnxPPduIk3lvWYcOAdS3WbHvogkK26FirLHn6SFsU/e8j1XbLXuKA5PGOR+r6MUONI5pwo2T2FrJFv5BNI2WHa4lNU9Rt3ZWrvhbletNUTZ/NIBsxCCmCoPrHrFcajRiRwdKaGG4mMaHLJ5lqOR0eppQW06RVJ4Ky5s1LWgtziOKAnvLXGZpDtqfoRJEbE8cWHcWbFHEnGtdVtymr1Wa5adclaes2jexWFF70bb3E7IkQAe8DF/fOjlDivIZg+bvMYTcgKDPFAA691ZEFgfLgMXBRVa+Kn7Eo5LpsqgKEkSBgFPdAgx7CQFAI1EFiZyW6E2BZL1Y5bLWGXhmch2iUOTBH1MEDTQCiGxQeUUL7AD4IF7ggKeSOkyOQE4SVMBJGzbViezSMxKRzuLIjwkBuOvKDPA/oXvrKX9EHgonwHzdrJ7mi1cl14oxSoc/hB0L0CM3fSatNAb11sGN+2KnuFlkz4d9FTx29hzPhJBCx585B+NvnPq98mGZISuFuoLXTf6OCdamcUihVAKlobtMh2CN5F+wtkir8HZFi/nk1fkKaxPbwns+J/R1R+Og4SLPA5bKAj5ffZszSDSkf6eu1veOWgzCwLYsihw/WJeKLs6/QCs+206/maWZsf0A/t/2cSua0ilRGG+Z0MpUMkjOyr9NkGs1wfpVKtpefUoqNvV4t98mks88X37ev6fpfvlrOqsWejoKEG6EfPKmlIU6pKCKOQmlaEp8lXGGXdrXI6VTPreNLC5T3bAkDmS+dCYui04vTyxGcdtlOExqp7mGblwmxzeucX9zrY/winKOjOOKOH8rT/g/N0FgN', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='the OCR based baselines, states-of-the-art OCR engines are used, including MS OCR API used in [64] and CLOVA OCR API5 used in [24,23]. An analysis on OCR engines is available in Section 3.4. More details of OCR and training setups are available in Appendix A.1 and A.5.', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 9, 'orig_elements': 'eJxVU11v2zAM/CuEnxPPduIk3lvWYcOAdS3WbHvogkK26FirLHn6SFsU/e8j1XbLXuKA5PGOR+r6MUONI5pwo2T2FrJFv5BNI2WHa4lNU9Rt3ZWrvhbletNUTZ/NIBsxCCmCoPrHrFcajRiRwdKaGG4mMaHLJ5lqOR0eppQW06RVJ4Ky5s1LWgtziOKAnvLXGZpDtqfoRJEbE8cWHcWbFHEnGtdVtymr1Wa5adclaes2jexWFF70bb3E7IkQAe8DF/fOjlDivIZg+bvMYTcgKDPFAA691ZEFgfLgMXBRVa+Kn7Eo5LpsqgKEkSBgFPdAgx7CQFAI1EFiZyW6E2BZL1Y5bLWGXhmch2iUOTBH1MEDTQCiGxQeUUL7AD4IF7ggKeSOkyOQE4SVMBJGzbViezSMxKRzuLIjwkBuOvKDPA/oXvrKX9EHgonwHzdrJ7mi1cl14oxSoc/hB0L0CM3fSatNAb11sGN+2KnuFlkz4d9FTx29hzPhJBCx585B+NvnPq98mGZISuFuoLXTf6OCdamcUihVAKlobtMh2CN5F+wtkir8HZFi/nk1fkKaxPbwns+J/R1R+Og4SLPA5bKAj5ffZszSDSkf6eu1veOWgzCwLYsihw/WJeKLs6/QCs+206/maWZsf0A/t/2cSua0ilRGG+Z0MpUMkjOyr9NkGs1wfpVKtpefUoqNvV4t98mks88X37ev6fpfvlrOqsWejoKEG6EfPKmlIU6pKCKOQmlaEp8lXGGXdrXI6VTPreNLC5T3bAkDmS+dCYui04vTyxGcdtlOExqp7mGblwmxzeucX9zrY/winKOjOOKOH8rT/g/N0FgN', 'is_continuation': True, 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='3.3 Experimental Results', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 9, 'orig_elements': 'eJxFj0FqwzAQRa9itC6pZcWW3H0vULoLwYykkRHIirDHkBBy945MS7fv/c/8uTwFJlww0xS9+GiE1K3ywaKSYwcO+rbVTo7KBNChH8CKt0YsSOCBgPNPEWLCDAvWsr/lnaYCBddT8aFmq6ZHOTSUkqIDirf8/qsT5HmHGTf2F4F5FlemhcmU98Xiynw8yPq/UenOGdkN5myslqEHZ0bvBsYq2P6M4sUNwjsd4ZNqPu+8KNYvITVfuO2Jtnr9b9h3pMSt6w+PrlPX', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='Document Classification. The results are shown in Table 1. Without re- lying on any other resource (e.g., off-the-shelf OCR engine), Donut shows a state-of-the-art performance among the general-purpose VDU models such as LayoutLM [65] and LayoutLMv2 [64]. In particular, Donut surpasses the Lay- outLMv2 accuracy reported in [64], while using fewer parameters with the 2x faster speed. Note that the OCR-based models must consider additional model parameters and speed for the entire OCR framework,', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 9, 'orig_elements': 'eJxdU8tu2zAQ/JWFTi0gqZId+dFbkaBAgSQFijQ9GEawIpcWEYpUSSqOEOTfu1ScNO3J8j5mZ2a5u6eMDPVk452W2WfIVFPVzWbZLGS1rddiU1fbRlbtqq1WJNcKsxyyniJKjMj1T5nShiz2lJqls2O8G3AgXw5SpdqUjtMwp3EYjBYYtbOfTmmD9jDigQLndxnZQ7bn6MCROzv2LXmOb+eI/8uxXldLqVpa1tsFCmyqai3q7XKjcK2aFbbZM3dEeoyp+MKJMemDc4MhaHUiUMJNR+ApjCYGYHQInTta0BZusDUEdQm/dOzcGLmqADNpewBnAe0ELnbkU7MbvSD4QOWhzMEpVXCiCB0ZBd/PfwDr0ZY+5nCRjJkn8CwIESMV7qUafQT2Sznfo2Uw7B0P4gwcyJJHUwyjH1wguL34Cb2TZAKEUXSAAS5xYoKXV7BbNXumJt8iDwuOne1L+GaBzYtajAb9GxOGZDcozIO4p4DXLhRi9Cgmljc4H0kmSxJUDseOlwljSE4oOrIFjMyrj+QDHNmsGW3xCAoDxyAMRLKEaxeJMxjnNPtStBgY96SlH0ME4WzQkntQSp32g+Yl/35E0jdjArs1g/FetZ8xQaWyo/P3M0+2RwewjqX2aEzSwHYWkAwt4Su30yP2g6Gc9+FJpAeC8iFtQP5PEXZn+XLPVb9HHsaEnZ/1WNhUV+/4lfDFBJdD9Kht8ijx7fk7nv7/Kz9MbFL/8vaEC9FMsFss2WZDKOdyB5bFhje18+4Ko++TcFlEV/AP8FV5h6Ir0z29nto1es/v/IFu0hk87/8AroNVqA==', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='which is not small in gen- eral. For example, a recent advanced OCR-based model [4,3] requires more than 80M parameters. Also, training and maintaining the OCR-based systems are costly [23], leading to needs for the Donut-like end-to-end approach.', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 9, 'orig_elements': 'eJxdU8tu2zAQ/JWFTi0gqZId+dFbkaBAgSQFijQ9GEawIpcWEYpUSSqOEOTfu1ScNO3J8j5mZ2a5u6eMDPVk452W2WfIVFPVzWbZLGS1rddiU1fbRlbtqq1WJNcKsxyyniJKjMj1T5nShiz2lJqls2O8G3AgXw5SpdqUjtMwp3EYjBYYtbOfTmmD9jDigQLndxnZQ7bn6MCROzv2LXmOb+eI/8uxXldLqVpa1tsFCmyqai3q7XKjcK2aFbbZM3dEeoyp+MKJMemDc4MhaHUiUMJNR+ApjCYGYHQInTta0BZusDUEdQm/dOzcGLmqADNpewBnAe0ELnbkU7MbvSD4QOWhzMEpVXCiCB0ZBd/PfwDr0ZY+5nCRjJkn8CwIESMV7qUafQT2Sznfo2Uw7B0P4gwcyJJHUwyjH1wguL34Cb2TZAKEUXSAAS5xYoKXV7BbNXumJt8iDwuOne1L+GaBzYtajAb9GxOGZDcozIO4p4DXLhRi9Cgmljc4H0kmSxJUDseOlwljSE4oOrIFjMyrj+QDHNmsGW3xCAoDxyAMRLKEaxeJMxjnNPtStBgY96SlH0ME4WzQkntQSp32g+Yl/35E0jdjArs1g/FetZ8xQaWyo/P3M0+2RwewjqX2aEzSwHYWkAwt4Su30yP2g6Gc9+FJpAeC8iFtQP5PEXZn+XLPVb9HHsaEnZ/1WNhUV+/4lfDFBJdD9Kht8ijx7fk7nv7/Kz9MbFL/8vaEC9FMsFss2WZDKOdyB5bFhje18+4Ko++TcFlEV/AP8FV5h6Ir0z29nto1es/v/IFu0hk87/8AroNVqA==', 'is_continuation': True, 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='Document Information Extraction. Table 2 shows the results on the four different document IE tasks. The first group uses a conventional BIO-tagging- based IE approach [22]. We follows the conventions in IE [65,18]. OCR extracts texts and bounding boxes from the image, and then the serialization module sorts all texts with geometry information within the bounding box. The BIO-tagging- based named entity recognition task performs token-level tag classification upon\\n\\n5https://clova.ai/ocr.', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 9, 'orig_elements': 'eJzVUstu2zAQ/JWFzn7qGefYNodcGqAI0INhGEtyJROhSIGkHLtB/71L2YmLoui9N3J3ODs7nO1bRoZ6snGvVXYPWV5SXcpVI1alkOKuKOpG1jJHqQQWUq6yGWQ9RVQYkfFvWasNWewpPVbOjnE/4EB+Mag2YVM7noepjcNgtMSonV1e2wZtN2JHgfvbjGyX7bg6cGVvx16Q5/pmqvibxnWzKlQrqFhvWBhWq1Uj15virsWmrWoU2U9+EekUE/iLk2PaDx5t63w/TYeHU/Qo03EBzygMQQ7h4F4DxAOBpzCaGICB6dq60YPSbUtJA6gPwgeIGF4CUySU9iFC5904wBgoAIJ09shAnoIGPj0+zSN2nbbdHAQGUomALfEO5QG2eb5bwPc0zZh3ITeCANom/LauZus7Rj59/gZ02YKxfOKBVoFwo1U8gg8n1tB6109MumdLZxOEr5e9AnmNRv+4WNI7NbIPwflEZcyV9FXHA3Tk+M/9mUXcPEwdfWH6ferFjb9sm0KiIK0Tz2yxdJ3VE1EyETgyiZp3cS9k54aOxBKwA2kwBN1ecwPj4GwKznumvqL33DnSc/pv/vg/A91QJepNJas1yVzU1OT5phWq3BRNWVa1/O8CXR1iHML9cimNO+IC9dJJv/iHJ7tfXKVOgA==', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='Table 2. Performances on various document IE tasks. The field-level F1 scores and tree-edit-distance-based accuracies are reported. Donut shows the best accuracies for all domains with significantly faster inference speed. †Parameters for vocabulary are omitted for fair comparisons among multi-lingual models. ‡# parameters for OCR should be considered. ∗Official multi-lingual extension models are used', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 10, 'orig_elements': 'eJxVkstuGzEMRX+FmG5jd2biZ7dpC3TTBIV3aWBQEmUT1WOgh1MjyL+XslO0wezIyzNXl3x86ciRp1D2bLpP0K3HFamxV9QvF/JtrVovBjUskMjozbDobqDzVNBgQdG/dJYdBfTUhk0MtewnnCjNJ2ObtrXLebq0cZocaywcw8e3tsNwqHigLP3HjsKhe5LqJJV9qF5RkvrQX0rpn8nVcrvpt7dK0Zq02ep+1fdqM6zWuFrYcXnbvcpEod+liXeoHME4hwdKNiaPQVOGGOCEiWPNYKKuLQH49gUK5l95DrsjgWVyZuboRA6+DpB1TDKHwUBJRDMyXGaGc2m8mcJMBlDrmlBz0yWCRFNMhcwcPrdgIB/jc4YibEW5/K8WX4DOiRWPHDI8czlC5kNgK4GF4s5gMRdKwMGSJKEJ8kQN/bOO/dg/YJIdiODKOkWNqjpM54uR6LmIj0vLIifQ0UugnKP8C30MB/DVFZ45butw4KMhl9/gwweY3uPv7360x1Rn5CUCC5mNuLq6GYf1vRXb3DjvqLIQEqlEf+VfvFUJrl3C3yP5jinJiZxo1/b3+vQHstPmSQ==', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='CORD [45] Ticket [12] Business Card Receipt OCR #Params Time (s) F1 Acc. Time (s) F1 Acc. Time (s) F1 Acc. Time (s) F1 Acc. ✓ 86† BERT∗ [22] ✓ 86† BROS [18] ✓ 89† LayoutLM [65] LayoutLMv2∗ [64,66] ✓ 179† Donut M + α‡ M + α‡ M + α‡ M + α‡ 143† M 1.6 1.7 1.7 1.7 1.2 73.0 65.5 74.7 70.0 78.4 81.3 78.9 82.4 84.1 90.9 1.7 1.8 0.6 74.3 82.4 87.2 90.1 94.1 98.7 1.5 1.6 1.4 40.8 72.1 52.2 83.0 57.8 84.4 2.5 2.6 1.9 70.3 54.1 72.9 78.0 78.6 88.6 SPADE∗ [25] WYVERN∗ [21] ✓ 93† ✓ 106† M + α‡ M + α‡ 4.0 1.2', metadata={'text_as_html': '<table><thead><th></th><th rowspan=\"2\">OCR</th><th rowspan=\"2\">#Params</th><th colspan=\"3\">CORD [45]</th><th colspan=\"3\">Ticket [12]</th><th colspan=\"3\">Business Card</th><th colspan=\"3\">Receipt</th></thead><thead><th></th><th></th><th></th><th>Time (s)</th><th>F1</th><th>Acc.</th><th>Time (s)</th><th>F1</th><th>Acc.</th><th>Time (s)</th><th>F1</th><th>Acc.</th><th>Time (s)</th><th>F1</th><th>Ace</th></thead><tr><td>BERT\" [22]</td><td></td><td>86y, + ot</td><td>1.6</td><td>73.0</td><td>65.5</td>', 'filetype': 'application/pdf', 'page_number': 10, 'filename': 'donut_paper.pdf', 'orig_elements': 'eJzFVm1v2zgM/iuc96VDC0eSZUu+6xXY2g4YsK5DFmw4xEWg2EobXOIYib2tGPbfJ/qVWRMcMNxhH9xKIiXxofg8zPSbZ1d2bfNytsy8P8CzwYIpaQznWohFGi2yyLJUGTE3LDBGe2fgrW1pMlMa5//NWyxXNjdri5uzTV6Vs8IUdusX2QJ90Vw+FrXZFMVqmZpyuclHrXll8vvK3Nuds089m997d261cCuzvFrP7datc1YvbYcgozDWLA7mc6tsmsUpixibax4pE8mFCAM8ubRfy5nZzR7K9Qr3nJdmvrIX5+WDNRn+uzgf4Z/yAbabL7vC5H8lnki8i9vL8RHL8/dma9a73ppuVq01cNbL2/EVTGV4d8Q+Wab/2BKmXBzzeFXtlrnd7eDSbLMjPmOb2mVRNtZRD+ZnTAcGk+XawsnuRb/wmvfDl2nq/zZH+xOYrfuyi1fX40niwVTU6crqtX6go8czOIVN2a9wP+rHKvBZP4lCPyReavCSfjAcKCRxGjZI5uthh/B5P0kqxowFAVeDnZETQ9k5j2pELarx7QdXAuGA6U8CKkm4Q2WOhavoTexpVv6TAY32rXncVOXbG5hG6tArnLyAySmc8MMBa3/IqeYkNf9/wJ+dmkTyLIoORM1VfJYkb+DUlMfijkld+LQwhlrQyhf9JGY+zcFQiaEgXpqWpaD1KsiN7nr2FNgVKusBLDJ4Ri4WTz20K0MX3nA+IxfHksQda5IETmCHiuKWewnZi3s4WPcTCuLD+5dX1zWnwwPvEiQJVtOAUpJ0qb1JSAKSlN6SBCRiEinleiAoTWlhKjKOaG7CoMstxfPp74/X43etEEwFP1RsrFGq2ml+8KlkQG6VEUGwJ0S0+Jy2aOKlKOaYQBsgk1QoeqzuS68GNmqapPe9baDYOPu+BqSDwV6vgrYrgeud0HZJ6EQfXnPAbvArC0klFA/AKWMlmGBQN4VKCK7qxvDU3sirHixxayFS5pD0OiG60xq16LY5jWj31cSDm+YJgzmvl/mvLDiutmfegNMI9ynyCcCuBditAKUeUOIBNRRQO3EUA6oRIKMBKd3u1OAojXuC1u6UCe3OqfbUtVfY3ikBWxpgKwNUJ0BVAqQ4HixBOE9Re8YYQgDYx9A9xhDqiCJAekPD5u4xXFI7OrQrvM9m3AHvssuiPhP/ljXH+yY7OEDiA/IFkCfOFjbIHO0B6Q7IEozYeTknzA+SHZDk6BDjyOXXzRwjACkOSG1ATmD24vqXY/tzdYJc8L7f/QC2viJi', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='74.0 75.8 43.3 46.9 4.5 1.5 14.9 29.4 41.8 54.8 4.3 1.7 32.3 51.3 29.9 51.5 7.3 3.4 64.1 53.2 71.5 82.9', metadata={'text_as_html': '<td>1.7</td><td>74.3</td><td>824</td><td>1.5</td><td>40.8</td><td>72.1</td><td>® 2 D</td><td>70.3</td><td>54.1</td></tr><tr><td>BROS [15]</td><td>;</td><td>86\\\\1 + a</td><td>1.7</td><td>74.7</td><td>70.0</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>LayoutLM [67]</td><td></td><td>8() T+ (1</td><td>1.7</td><td>78.4</td><td>81.3</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>LayoutLMv: [64,66]</td', 'filetype': 'application/pdf', 'page_number': 10, 'filename': 'donut_paper.pdf', 'orig_elements': 'eJzFVm1v2zgM/iuc96VDC0eSZUu+6xXY2g4YsK5DFmw4xEWg2EobXOIYib2tGPbfJ/qVWRMcMNxhH9xKIiXxofg8zPSbZ1d2bfNytsy8P8CzwYIpaQznWohFGi2yyLJUGTE3LDBGe2fgrW1pMlMa5//NWyxXNjdri5uzTV6Vs8IUdusX2QJ90Vw+FrXZFMVqmZpyuclHrXll8vvK3Nuds089m997d261cCuzvFrP7datc1YvbYcgozDWLA7mc6tsmsUpixibax4pE8mFCAM8ubRfy5nZzR7K9Qr3nJdmvrIX5+WDNRn+uzgf4Z/yAbabL7vC5H8lnki8i9vL8RHL8/dma9a73ppuVq01cNbL2/EVTGV4d8Q+Wab/2BKmXBzzeFXtlrnd7eDSbLMjPmOb2mVRNtZRD+ZnTAcGk+XawsnuRb/wmvfDl2nq/zZH+xOYrfuyi1fX40niwVTU6crqtX6go8czOIVN2a9wP+rHKvBZP4lCPyReavCSfjAcKCRxGjZI5uthh/B5P0kqxowFAVeDnZETQ9k5j2pELarx7QdXAuGA6U8CKkm4Q2WOhavoTexpVv6TAY32rXncVOXbG5hG6tArnLyAySmc8MMBa3/IqeYkNf9/wJ+dmkTyLIoORM1VfJYkb+DUlMfijkld+LQwhlrQyhf9JGY+zcFQiaEgXpqWpaD1KsiN7nr2FNgVKusBLDJ4Ri4WTz20K0MX3nA+IxfHksQda5IETmCHiuKWewnZi3s4WPcTCuLD+5dX1zWnwwPvEiQJVtOAUpJ0qb1JSAKSlN6SBCRiEinleiAoTWlhKjKOaG7CoMstxfPp74/X43etEEwFP1RsrFGq2ml+8KlkQG6VEUGwJ0S0+Jy2aOKlKOaYQBsgk1QoeqzuS68GNmqapPe9baDYOPu+BqSDwV6vgrYrgeud0HZJ6EQfXnPAbvArC0klFA/AKWMlmGBQN4VKCK7qxvDU3sirHixxayFS5pD0OiG60xq16LY5jWj31cSDm+YJgzmvl/mvLDiutmfegNMI9ynyCcCuBditAKUeUOIBNRRQO3EUA6oRIKMBKd3u1OAojXuC1u6UCe3OqfbUtVfY3ikBWxpgKwNUJ0BVAqQ4HixBOE9Re8YYQgDYx9A9xhDqiCJAekPD5u4xXFI7OrQrvM9m3AHvssuiPhP/ljXH+yY7OEDiA/IFkCfOFjbIHO0B6Q7IEozYeTknzA+SHZDk6BDjyOXXzRwjACkOSG1ATmD24vqXY/tzdYJc8L7f/QC2viJi', 'is_continuation': True, 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='', metadata={'text_as_html': '><td></td><td>179,\\\\I +at</td><td>1.7</td><td>78.9</td><td>82.4</td><td>1.8</td><td>87.2</td><td>90.1</td><td>1.6</td><td>52.2</td><td>83.0</td><td>2.6</td><td>72.9</td><td>78.0</td></tr><tr><td>Donut</td><td></td><td>143!</td><td>1.2</td><td></td><td>84.190.9</td><td>0.6</td><td>94.1</td><td>98.7</td><td>1.4</td><td>57.8</td><td>84.4</td><td>1.9</td><td>78.6</td><td>88.6</td></tr><tr><td>SPADE\" [25]</td><td></td><td>3\\\\ T+ t</td><td>4.0</td><td>74.0</td><td>75.8</td><td>4.5</td><td>14.9</td><td>2', 'filetype': 'application/pdf', 'page_number': 10, 'filename': 'donut_paper.pdf', 'orig_elements': 'eJzFVm1v2zgM/iuc96VDC0eSZUu+6xXY2g4YsK5DFmw4xEWg2EobXOIYib2tGPbfJ/qVWRMcMNxhH9xKIiXxofg8zPSbZ1d2bfNytsy8P8CzwYIpaQznWohFGi2yyLJUGTE3LDBGe2fgrW1pMlMa5//NWyxXNjdri5uzTV6Vs8IUdusX2QJ90Vw+FrXZFMVqmZpyuclHrXll8vvK3Nuds089m997d261cCuzvFrP7datc1YvbYcgozDWLA7mc6tsmsUpixibax4pE8mFCAM8ubRfy5nZzR7K9Qr3nJdmvrIX5+WDNRn+uzgf4Z/yAbabL7vC5H8lnki8i9vL8RHL8/dma9a73ppuVq01cNbL2/EVTGV4d8Q+Wab/2BKmXBzzeFXtlrnd7eDSbLMjPmOb2mVRNtZRD+ZnTAcGk+XawsnuRb/wmvfDl2nq/zZH+xOYrfuyi1fX40niwVTU6crqtX6go8czOIVN2a9wP+rHKvBZP4lCPyReavCSfjAcKCRxGjZI5uthh/B5P0kqxowFAVeDnZETQ9k5j2pELarx7QdXAuGA6U8CKkm4Q2WOhavoTexpVv6TAY32rXncVOXbG5hG6tArnLyAySmc8MMBa3/IqeYkNf9/wJ+dmkTyLIoORM1VfJYkb+DUlMfijkld+LQwhlrQyhf9JGY+zcFQiaEgXpqWpaD1KsiN7nr2FNgVKusBLDJ4Ri4WTz20K0MX3nA+IxfHksQda5IETmCHiuKWewnZi3s4WPcTCuLD+5dX1zWnwwPvEiQJVtOAUpJ0qb1JSAKSlN6SBCRiEinleiAoTWlhKjKOaG7CoMstxfPp74/X43etEEwFP1RsrFGq2ml+8KlkQG6VEUGwJ0S0+Jy2aOKlKOaYQBsgk1QoeqzuS68GNmqapPe9baDYOPu+BqSDwV6vgrYrgeud0HZJ6EQfXnPAbvArC0klFA/AKWMlmGBQN4VKCK7qxvDU3sirHixxayFS5pD0OiG60xq16LY5jWj31cSDm+YJgzmvl/mvLDiutmfegNMI9ynyCcCuBditAKUeUOIBNRRQO3EUA6oRIKMBKd3u1OAojXuC1u6UCe3OqfbUtVfY3ikBWxpgKwNUJ0BVAqQ4HixBOE9Re8YYQgDYx9A9xhDqiCJAekPD5u4xXFI7OrQrvM9m3AHvssuiPhP/ljXH+yY7OEDiA/IFkCfOFjbIHO0B6Q7IEozYeTknzA+SHZDk6BDjyOXXzRwjACkOSG1ATmD24vqXY/tzdYJc8L7f/QC2viJi', 'is_continuation': True, 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='', metadata={'text_as_html': '9.4</td><td>4.3</td><td>32.3</td><td>51.3</td><td>7.3</td><td>64.1</td><td>53.2</td></tr><tr><td>WYVERN® [21]</td><td></td><td>106y, + «</td><td>1.2</td><td>43.3</td><td>46.9</td><td>1.5</td><td>41.8</td><td>54.8</td><td>1.7</td><td>29.9</td><td>51</td><td>3.4</td><td>71.5</td><td>82.9</td></tr></table>', 'filetype': 'application/pdf', 'page_number': 10, 'filename': 'donut_paper.pdf', 'orig_elements': 'eJzFVm1v2zgM/iuc96VDC0eSZUu+6xXY2g4YsK5DFmw4xEWg2EobXOIYib2tGPbfJ/qVWRMcMNxhH9xKIiXxofg8zPSbZ1d2bfNytsy8P8CzwYIpaQznWohFGi2yyLJUGTE3LDBGe2fgrW1pMlMa5//NWyxXNjdri5uzTV6Vs8IUdusX2QJ90Vw+FrXZFMVqmZpyuclHrXll8vvK3Nuds089m997d261cCuzvFrP7datc1YvbYcgozDWLA7mc6tsmsUpixibax4pE8mFCAM8ubRfy5nZzR7K9Qr3nJdmvrIX5+WDNRn+uzgf4Z/yAbabL7vC5H8lnki8i9vL8RHL8/dma9a73ppuVq01cNbL2/EVTGV4d8Q+Wab/2BKmXBzzeFXtlrnd7eDSbLMjPmOb2mVRNtZRD+ZnTAcGk+XawsnuRb/wmvfDl2nq/zZH+xOYrfuyi1fX40niwVTU6crqtX6go8czOIVN2a9wP+rHKvBZP4lCPyReavCSfjAcKCRxGjZI5uthh/B5P0kqxowFAVeDnZETQ9k5j2pELarx7QdXAuGA6U8CKkm4Q2WOhavoTexpVv6TAY32rXncVOXbG5hG6tArnLyAySmc8MMBa3/IqeYkNf9/wJ+dmkTyLIoORM1VfJYkb+DUlMfijkld+LQwhlrQyhf9JGY+zcFQiaEgXpqWpaD1KsiN7nr2FNgVKusBLDJ4Ri4WTz20K0MX3nA+IxfHksQda5IETmCHiuKWewnZi3s4WPcTCuLD+5dX1zWnwwPvEiQJVtOAUpJ0qb1JSAKSlN6SBCRiEinleiAoTWlhKjKOaG7CoMstxfPp74/X43etEEwFP1RsrFGq2ml+8KlkQG6VEUGwJ0S0+Jy2aOKlKOaYQBsgk1QoeqzuS68GNmqapPe9baDYOPu+BqSDwV6vgrYrgeud0HZJ6EQfXnPAbvArC0klFA/AKWMlmGBQN4VKCK7qxvDU3sirHixxayFS5pD0OiG60xq16LY5jWj31cSDm+YJgzmvl/mvLDiutmfegNMI9ynyCcCuBditAKUeUOIBNRRQO3EUA6oRIKMBKd3u1OAojXuC1u6UCe3OqfbUtVfY3ikBWxpgKwNUJ0BVAqQ4HixBOE9Re8YYQgDYx9A9xhDqiCJAekPD5u4xXFI7OrQrvM9m3AHvssuiPhP/ljXH+yY7OEDiA/IFkCfOFjbIHO0B6Q7IEozYeTknzA+SHZDk6BDjyOXXzRwjACkOSG1ATmD24vqXY/tzdYJc8L7f/QC2viJi', 'is_continuation': True, 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='the ordered texts to generate a structured form. We test three general-purpose VDU backbones, BERT [8], BROS [18], LayoutLM [65], and LayoutLMv2 [64,66]. We also test two recently proposed IE models, SPADE [24] and WYVERN [23]. SPADE is a graph-based IE method that predicts relations between bounding boxes. WYVERN is an Transformer encoder-decoder model that directly gen- erates entities with structure given OCR outputs. WYVERN is different from Donut in that it takes the OCR output as its', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 10, 'orig_elements': 'eJxVUstu2zAQ/JWFzrYrPyTLvTW1DwXSpHDcBIVgGCtxJRGRSIKk4hhB/r1L2a7RgyByZjmzHG7+EVFLHSl/kCL6ChFWcTlNsMxEIuIkS9N5vCIhpsVyXi4oK6MRRB15FOiR6z+iSraksKNwWGjV+4NBQ3ZiRBVqA+1PZqDRmFaW6KVWXy50i6rusSbHfB6RqqM9o4aRg+q7gizj03iA7K3JNFll8WpeFLSkUqzKOI3jIpumS0wX1SyZR598wtO7D8W+IdBWkCUBAXPgNdSkyKInQHDe9qXvA11p203ghbjOefCNJbpUtmPTW6MdwfP6NxRYvhZakRvB3Wa7gzzjru+2j0+QT8PyHk+69/c/IU8T3qIS/6C3GYOLUZruByNsnb64HTVYKvmS7QmM1cFMwI8NdFpQy05Pv76tN5DPFvtB8OXP82b7wPs5K5056fg6tUXTjAu8nibfaL54g55VSciSA7DUDq/goCB/JFJQ6F4JqWpevJObXNWDooKdReVCNmSBVMn92LGg4X/u7iwvJPcfuufIxjDE67jeSy95cZS+uWUNtXxj28fvW+BYTO//8xSyqii8N1RWd7AOYwVSnW0kZ4WvrBge9iYA6JjiTw1qYbiuc/eAlpthw10Yic/9X3b//Rk=', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='inputs.', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 10, 'orig_elements': 'eJxVUstu2zAQ/JWFzrYrPyTLvTW1DwXSpHDcBIVgGCtxJRGRSIKk4hhB/r1L2a7RgyByZjmzHG7+EVFLHSl/kCL6ChFWcTlNsMxEIuIkS9N5vCIhpsVyXi4oK6MRRB15FOiR6z+iSraksKNwWGjV+4NBQ3ZiRBVqA+1PZqDRmFaW6KVWXy50i6rusSbHfB6RqqM9o4aRg+q7gizj03iA7K3JNFll8WpeFLSkUqzKOI3jIpumS0wX1SyZR598wtO7D8W+IdBWkCUBAXPgNdSkyKInQHDe9qXvA11p203ghbjOefCNJbpUtmPTW6MdwfP6NxRYvhZakRvB3Wa7gzzjru+2j0+QT8PyHk+69/c/IU8T3qIS/6C3GYOLUZruByNsnb64HTVYKvmS7QmM1cFMwI8NdFpQy05Pv76tN5DPFvtB8OXP82b7wPs5K5056fg6tUXTjAu8nibfaL54g55VSciSA7DUDq/goCB/JFJQ6F4JqWpevJObXNWDooKdReVCNmSBVMn92LGg4X/u7iwvJPcfuufIxjDE67jeSy95cZS+uWUNtXxj28fvW+BYTO//8xSyqii8N1RWd7AOYwVSnW0kZ4WvrBge9iYA6JjiTw1qYbiuc/eAlpthw10Yic/9X3b//Rk=', 'is_continuation': True, 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='For all domains, including public and private in-service datasets, Donut shows the best scores among the comparing models. By measuring both F1 and TED- based accuracy, we observe not only Donut can extract key information but also predict complex structures among the field information. We observe that a large input resolution gives robust accuracies but makes the model slower. For example, the performance on the CORD with 1280×960 was 0.7 sec./image and 91.1 accuracy. But, the large resolution', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 10, 'orig_elements': 'eJxdU8tu2zAQ/JWFTi3gKJLtyFaPbZpjCzQpekiDYEWuZcIUKfARRw3y710ycpMU8EHex+zs7PD2qSBNA5lwr2TxCQpqqF6LZrUUS9lsEFeybep23dbbZY242RYLKAYKKDEg1z8VO6XJ4ECpWVoTw/2II7lylLtUm9JhGnMax1ErgUFZcz6nNZo+Yk+e87cFmb644+jIkXsTh44cx+sqh9wryeai3VbtqutoQ0K2omqqqtvWzLdZ75YXq+KZOwI9hlR8ZR2g1iDtgMr4BSgjdJTK9DDGjgkBGgmjUw8YiJNnntyDEgRpRU+BOy7TXuD39ugh7Ak68vxXWEcecLCMlKLCDswy4Q5WkvYlfJ5gIPQxBzsb9nBV52k3Xy/PoGN0CShEdCimBRwJbJeGExgbwBo9zZMFGuBtuCzAgSYmubNuyEJCx3nU3vIGJBUXJBqaHsEHF0WI7znuFGn5tr+EX69jwx4ZDDS6PikxMjS3Wx3zpF49MJazXeTtZ9qKI4nBgAd60SbvDl7bI5sAkvj0iInRIqfZGnm2YYEZM4W+fP9xCUfF6tTLbfU7VpXctE0FR/RQlRvwJMpzNbAnsnZtXdb/VGONY3hBfmH9hm+6FwvcUQjk3g72p8lM8iw1RMdsvApxluSGc5JNzt7lAxnUk08quiQKXJPI8KtyXcJPo9WBN2E0lw+qlaH/HOMDdvr95o56dFKTZyq7TMWrP5S+T6bLq86nVGE6lXHy4OHDNRFcqZ6PCxcfE1/lgX/IML1RO35kJoBiO7If0M+dLE+YAdIqqB2hnFjKwIdlr8VkR15QGckXdnzaMj3R0+v9hs5hKr1JD+v57i8RWnf6', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='showed better performances on the low-resource situation. The detailed analyses are in Section 3.4. Unlike other baselines, Donut shows stable performance regardless of the size of datasets and complexity of the tasks (See Figure 5). This is a significant impact as the target tasks are already actively used in industries.', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 10, 'orig_elements': 'eJxdU8tu2zAQ/JWFTi3gKJLtyFaPbZpjCzQpekiDYEWuZcIUKfARRw3y710ycpMU8EHex+zs7PD2qSBNA5lwr2TxCQpqqF6LZrUUS9lsEFeybep23dbbZY242RYLKAYKKDEg1z8VO6XJ4ECpWVoTw/2II7lylLtUm9JhGnMax1ErgUFZcz6nNZo+Yk+e87cFmb644+jIkXsTh44cx+sqh9wryeai3VbtqutoQ0K2omqqqtvWzLdZ75YXq+KZOwI9hlR8ZR2g1iDtgMr4BSgjdJTK9DDGjgkBGgmjUw8YiJNnntyDEgRpRU+BOy7TXuD39ugh7Ak68vxXWEcecLCMlKLCDswy4Q5WkvYlfJ5gIPQxBzsb9nBV52k3Xy/PoGN0CShEdCimBRwJbJeGExgbwBo9zZMFGuBtuCzAgSYmubNuyEJCx3nU3vIGJBUXJBqaHsEHF0WI7znuFGn5tr+EX69jwx4ZDDS6PikxMjS3Wx3zpF49MJazXeTtZ9qKI4nBgAd60SbvDl7bI5sAkvj0iInRIqfZGnm2YYEZM4W+fP9xCUfF6tTLbfU7VpXctE0FR/RQlRvwJMpzNbAnsnZtXdb/VGONY3hBfmH9hm+6FwvcUQjk3g72p8lM8iw1RMdsvApxluSGc5JNzt7lAxnUk08quiQKXJPI8KtyXcJPo9WBN2E0lw+qlaH/HOMDdvr95o56dFKTZyq7TMWrP5S+T6bLq86nVGE6lXHy4OHDNRFcqZ6PCxcfE1/lgX/IML1RO35kJoBiO7If0M+dLE+YAdIqqB2hnFjKwIdlr8VkR15QGckXdnzaMj3R0+v9hs5hKr1JD+v57i8RWnf6', 'is_continuation': True, 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='Document Visual Question Answering. Table 3 shows the results on the DocVQA dataset. The first group is the general-purposed VDU backbones whose scores are from the LayoutLMv2 paper [64]. We measure the running time with MS OCR API used in [64]. The model in the third group is a DocVQA-specific- purposed fine-tuning model of LayoutLMv2, whose inference results are available in the official leader-board.6', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 10, 'orig_elements': 'eJxNUstu2zAQ/JWFzpEr24oc5WYklwJJ27SpczAMY0UuJaIUSfBhNwjy7yUVte6RszO7M7vcvxWkaCQdjpIXt1AsqV1SzTcVr1aCNogtrYjWLatZhVjXxRUUIwXkGDDx3wohFWkcKYu50TEcLVpyC8tF5uZyeLVTGa1VkmGQRn+aywp1H7Enn+r7gnRfHBJqE3LUcezIJXxZTZC7mGyu25uqXXcdbYjxllVNVXU3y2aDTS1W1+viPSkC/Q6ZfG9YzAFhJ31EBU+RfLYAW+3P5KTuF/CMnSJYgx/M2UMYCBz5qIKHxMvP1GT3tIWc2lNIgoQJ6XyA3ploQX6oetLkUJU2Oms8cdjd/4QO2a/OaPJwHhIInpnUHVIgEM6Mk/ABX00MD4+nFUzrg31THxbwQjAS+piok6modfILQY4EZxkGePwBX+++w/bbZ4h5ntSzMhscDSeVoawNg3T84hbnSKW3xKSQrIR/poXUVIY4jfroYcR/Dq/mHFILSkdhl2XlSHhCqaZ1zoONSN1lWrwi5OTKzqDjiyZf/+/H+ILOpW9xoud8s/fDH7O43v4=', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='As can be seen, Donut achieves competitive scores with the baselines that are dependent on external OCR engines. Especially, Donut shows that it is robust to the handwritten documents, which is known to be challenging to process. In the conventional approach, adding a post-processing module that corrects OCR\\n\\n6https://rrc.cvc.uab.es/?ch=17&com=evaluation&task=1.', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 10, 'orig_elements': 'eJzVUsGK2zAQ/RXhw55Sx05sKV4IpbQ99NJC6W1ZwkgaR2JtSUhy0mXpv3fkZlnoofeCTm/e07x5Mw8vFU44o8snq6t7VnWj4M2h7fTAh27XqL7r1U5Ap/aD2u/GttqwasYMGjIQ/6Ua7YQOZixi7d2STwECxjrosXBLOT+HtQwhTFZBtt5tb+UJ3HmBMyaqP1ToztUjoYGQk1tmiZHwtlmh+GaS98OhGfZSokClB9XwppGHlgvg3bjr99UvUmT8mQv5Q2IKHJPIEqLbsE/FJANlLF6Qan4OmG22FyIoHwm62mxYNsgkJJysIygbIE1EpjGg02SFeceoA0YHE/v28Tsj84Vas88poLIwTc+vzZLx19sfll5i0cslZZb92saA09doc0bHtFdLWUfasKuxyhT2k/NXV8g0gzL08drqXJAQvcJETb+49Svl3YXUlDC5oryjp0E3DLQuAmDBp/zuJirI7PUy4R9rNHxElVOZpuzmdW1fIUYo+fwoiVK0f99M38mDbDshuORtC2LsuWhRqH6PXKDW/9/NcJNzSPfbbYyqVhdVLyBrTNv3yhxbcUc3c8QLTMvq6y5Dejq29T8ye/wN5Eke/A==', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='Table 3. Average Normalized Levenshtein Similarity (ANLS) scores on DocVQA. Donut shows a promising result without OCR. ∗Donut shows a high ANLS score on the handwritten documents which are known to be challenging due to the difficulty of handwriting OCR (See Figure 6). †Token embeddings for English is counted for a fair comparison. ‡# parameters for OCR should be considered', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 11, 'orig_elements': 'eJxVUl1v2zAM/CuE99ICQxrHaezsLdjHU5FhS7CXrghoibaIypIhycmyov99VNpt6JOA4/F45On+qSBLA7l0YF18gKKkddutq2bZLFdrLOuyVM1i3tZr3dSEtCreQzFQQo0Jhf9UdGzJ4UC5WXs3pcOII4XZqLvMzeV0Hi9lHEfLChN7d/Natuj6CXuKUr8vyPXFg6CjIAc3DS0FwcvyAoX/JlWD64qWKyqxUu38tiJsyuYWG11jU1eqeJaORL9SJu+xtQTVDDZHCiIMWx8GtPybNNzRkVw0idjBjge2GDid4WqzvdtdQ1Q+UATv4JNXP75tZvLKghCNP0VAGIMfOLLrQWiTTXDiZLwQvn78PoOf02JR1m87DPcGsviLdpZOhsCg0yeZnMiB9mrKeUQ4GVYGZG94dP4kTA8tgTJo5eJ9HqsnymiW0Nx1rMTEGXz3TzCTxAxc7YjgC/eTiK2uL97mi/neP8pAkjNrLcwInQ/w2fWWowGOoPzkklwpwwgdchBokCQ4evcqUr4DAST/ROFFIM+TfSerL3a9i6wpkM5x//0JWwxB/sGR9jmk54c/wH/Ybg==', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='Fine-tuning set OCR #Params† Time (ms) ANLS test set ANLS∗ handwritten ✓ 110M + α‡ ✓ 113M + α‡ ✓ 200M + α‡ BERT [64] LayoutLM[65] LayoutLMv2[64] train set train set train set 1517 1519 1610 63.5 69.8 78.1 train set Donut 67.5 176M 782 LayoutLMv2-Large-QG[64] train + dev + QG ✓ 390M + α‡ 86.7 1698 n/a n/a n/a 72.1 67.3', metadata={'text_as_html': '<table><thead><th></th><th>Fine-tuning set</th><th>OCR</th><th>#Params’</th><th>Time (ms)</th><th>ANLS test set</th><th>ANLS* handwritten</th></thead><tr><td>BERT [64]</td><td>train set</td><td>v</td><td>110M + «</td><td>517</td><td>63.5</td><td></td></tr><tr><td>LayoutLM[65]</td><td>train set</td><td>v</td><td>113M + « z</td><td>1519</td><td>69.8</td><td>n/a\\nn/a\\nn/a</td></tr><tr><td>LayoutLMv2[64]</td><td>train set</td><td>v</td><td>200M + o</td><td>1610</td><td>78.1</td><td></td></tr><tr><td>D', 'filetype': 'application/pdf', 'page_number': 11, 'filename': 'donut_paper.pdf', 'orig_elements': 'eJyNVF1vmzAU/StX7GVb1wTjgGHKIu2zL8m2dn1LqugSblIkcBCYbF3V/z4biCFTW/XBxj7X3OPje+zlvUMZ5STVOk2c9+D4hBRvkEV+GHsTRrgVAd+KibeNeZhsY+cdODkpTFChXn/vbNOMJOZkfk72slbrAgsqR0WyNWtNWN0VTRiLIks3qNK9HHfhDOWuxh1VOr50SO6cG40WGlnLOo+p1DhjDVT2m9yEGHGaBMSQb2LX54QhC30ME4Gh4BuTWdEftcZqfavyzPwzVRhnNJuqW8LEfGbTsel0+5ZKOle1TOUOKlIW//H5yo5f/cQS82pVey6LLHqd5gSv8+qNRT5+n/8CRZU6yWTQt3CLMvldpkqRbCPj42ZK3ZLZp69X17AMJjc6kDSIKjGVXaYWOdgRY+4CzmBVuy7GFvWZsOOAj3w7aQfjhqqlm+PdvlbzxTLwX8rIe0b42+N+cyQdaTQK7USOcSWP3dM7OHgvVu25jep9Tx4w105EOGLPKP5i/PkMTZ9UBItBUq9XJwZHKrwj24AENvusKlB+WDneyhlIPJ9juaPzywujFVryM0jooPvLi0ek8qiVqgZao/5sw2Akhvvig62MW7M7D91FMBfgP5eDdjcMXO25YN0MJy5uZnqJx8TQwtoHnmAceh/ymDWZWB/iT4W6Op6ErP9haE04dQnYsj060l4UpovAGAPMFQBjSTDWGKxrrACmnGCKrcMevLRWRw1dgU40mKqAqRNox9tmnGLIePMydc/htamR83DzD167rb4=', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='', metadata={'text_as_html': 'onut</td><td>train set</td><td></td><td>176M</td><td>782</td><td>67.5</td><td>72.1</td></tr><tr><td colspan=\"2\">LayoutLMv2-Large-QG[64] train + dev + QG</td><td>v</td><td>390M + ot</td><td>1698</td><td>86.7</td><td>67.3</td></tr></table>', 'filetype': 'application/pdf', 'page_number': 11, 'filename': 'donut_paper.pdf', 'orig_elements': 'eJyNVF1vmzAU/StX7GVb1wTjgGHKIu2zL8m2dn1LqugSblIkcBCYbF3V/z4biCFTW/XBxj7X3OPje+zlvUMZ5STVOk2c9+D4hBRvkEV+GHsTRrgVAd+KibeNeZhsY+cdODkpTFChXn/vbNOMJOZkfk72slbrAgsqR0WyNWtNWN0VTRiLIks3qNK9HHfhDOWuxh1VOr50SO6cG40WGlnLOo+p1DhjDVT2m9yEGHGaBMSQb2LX54QhC30ME4Gh4BuTWdEftcZqfavyzPwzVRhnNJuqW8LEfGbTsel0+5ZKOle1TOUOKlIW//H5yo5f/cQS82pVey6LLHqd5gSv8+qNRT5+n/8CRZU6yWTQt3CLMvldpkqRbCPj42ZK3ZLZp69X17AMJjc6kDSIKjGVXaYWOdgRY+4CzmBVuy7GFvWZsOOAj3w7aQfjhqqlm+PdvlbzxTLwX8rIe0b42+N+cyQdaTQK7USOcSWP3dM7OHgvVu25jep9Tx4w105EOGLPKP5i/PkMTZ9UBItBUq9XJwZHKrwj24AENvusKlB+WDneyhlIPJ9juaPzywujFVryM0jooPvLi0ek8qiVqgZao/5sw2Akhvvig62MW7M7D91FMBfgP5eDdjcMXO25YN0MJy5uZnqJx8TQwtoHnmAceh/ymDWZWB/iT4W6Op6ErP9haE04dQnYsj060l4UpovAGAPMFQBjSTDWGKxrrACmnGCKrcMevLRWRw1dgU40mKqAqRNox9tmnGLIePMydc/htamR83DzD167rb4=', 'is_continuation': True, 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='BERT [64] LayoutLM[65] LayoutLMv2[64]  train set train set train set  1517 1519 1610  63.5 69.8 78.1  train set  Donut  67.5  176M  782  LayoutLMv2-Large-QG[64] train + dev + QG ✓ 390M + α‡  86.7  1698  n/a n/a n/a  72.1  67.3 ', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 11, 'orig_elements': 'eJxtUEtvwjAM/itWrhsladck3XHahCbBgWm3gpBpTVWpDVFJ0RDiv89hLw47xIq/h7/E5VlQRz25sGlr8Qgir6ja5lpajYYkckvGVrWVaAuLRot7ED0FrDEg689i13bksKdorvduDBuPnobE17uojXQ4+SuN3ndthaHdu+k33aFrRmzowHwpyDVizahnZOPGfksD40pdGAv0EeKQp5e3dyj1wxrmeNqPYb4odf7XHNMrB2HA1sGBwr83ULkysRSgtJIAOkty0EViwdhE3drhOX6KFYYVoIxeAGtSuEmczHFoaLKcXaO/rHdQ05HrcgarMTUqg6yQCwZWo8y2ijGZco7VieGpurAAboq/B0wan8GhGcQ9/azwtefViMv6E8Ihf2Q=', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='Fig. 6. Examples of Donut and LayoutLMv2 outputs on DocVQA. The OCR- errors make a performance upper-bound of the OCR-dependent baselines, e.g., Lay- outLMv2 (left and middle examples). Due to the input resolution constraint of the end-to-end pipeline, Donut miss some tiny texts in large-scale images (right example) but this could be mitigated by scaling the input image size (See Section 3.4)', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 11, 'orig_elements': 'eJxNkUtrG0EQhP9KsycJtBtWli2TW7CTk0NIHHIxRvTu1K6GzIt5BCvG/z09ygp8GqZrur6a7qfXBgYWLh+0aj5Sg5ur292W+3FCv9sr3o4K/fWwn7Db7q/3t82GGovMijPL+9dm0gaOLWqz8q7kQ+CA2AU11bdVzqdwljkEo0fO2rsPi2zYzYVnJNGfGri5eZZqkMrBFTsgSr3v36SW8ZKryRc9d3TT0ecXtsEgkZ/ovnKJnaIHPvmSH77+2ZKcoWTRnejjr++fOvp5BH27+9ESYvQxkeXfICaJO/lo2Y2gEuTWDr6ImTjnpUMhwCkZEw2cYLRD2hC6udtUZEsX6Mpg+p/EaqUMCEvMdUf3BZT92VI7iUYRyZtSx0GjdylH1gJYqIJrs2/loKDDGblZPmp1SpS8FTvtTlQnk8SSDMcZbRpZuNrWqdIq6vmYLynWNEh7PuokwGIUDRCzrGfOkMuJaq9287uMZx9K+i9o9QjQI8Zz4Ktut64LvOxW1lIi7jhUtXl7/gdER8u/', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='errors is an option to strengthen the pipeline [51,50,10] or adopting an encoder- decoder architecture on the OCR outputs can mitigate the problems of OCR errors [23]. However, this kind of approaches tend to increase the entire system size and maintenance cost. Donut shows a completely different direction. Some inference results are shown in Figure 6. The samples show the current strengths of Donut as well as the left challenges in the Donut-like end-to-end approach. Further analysis and', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 11, 'orig_elements': 'eJw1UsFu2zAM/RXC58Srl8Ubet1Q7LQBa29BENASbQuVJUGik2VF/32k0pwsPz4+ko88vDXkaaHAJ2ebR2h2Q49jP9Iw9HZ4+Erdbkff9h3uOjv0pu+bDTQLMVpkFP5bMzpPARfSZBvDyqeEiXKb7KhcDfM11TCm5J1BdjF8+gh7DNOKExWJHxoKU3MUNAlyCusyUBa8694FY/rLKkI5x1zAFcAAMakYcITCWbJ5JvmbCZJL5F0gOOy7zf5h0z0cIWZAqxlh0lwKJlrKW7BUH4DZzI7J8JoJ4k3n9/c/EFdOKxcwkrQ4dhMy3YrkOIh5BeJYiR+tHT7vji38jBc6U94IU3p9dcEqTSzIEc1MBZgEks5dMJmw3CRlEU6ql2thWqC4fyStWljQBeFjMAQmFm7hh1oNZY4XMUKwJYnP5K9g3TiSeMHyyjKM+NPCc1xICtWASGQqq5eJUEuJRJAYPLlJB+9beJFGCqpiqeHamVlzVb0bXae+dYEFLuS9fpXpaWQwM3o5DNmsaitcuVvvXnVKu+W4VQPuhrTwtGahyRoC+mup+5Xw4Ou91H2f0XkBdBJ4vo0Gu/ZLq4d0v7FfmLNknOlF7+X9+B+7hgCw', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='ablation is available in Section 3.4.', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 11, 'orig_elements': 'eJw1UsFu2zAM/RXC58Srl8Ubet1Q7LQBa29BENASbQuVJUGik2VF/32k0pwsPz4+ko88vDXkaaHAJ2ebR2h2Q49jP9Iw9HZ4+Erdbkff9h3uOjv0pu+bDTQLMVpkFP5bMzpPARfSZBvDyqeEiXKb7KhcDfM11TCm5J1BdjF8+gh7DNOKExWJHxoKU3MUNAlyCusyUBa8694FY/rLKkI5x1zAFcAAMakYcITCWbJ5JvmbCZJL5F0gOOy7zf5h0z0cIWZAqxlh0lwKJlrKW7BUH4DZzI7J8JoJ4k3n9/c/EFdOKxcwkrQ4dhMy3YrkOIh5BeJYiR+tHT7vji38jBc6U94IU3p9dcEqTSzIEc1MBZgEks5dMJmw3CRlEU6ql2thWqC4fyStWljQBeFjMAQmFm7hh1oNZY4XMUKwJYnP5K9g3TiSeMHyyjKM+NPCc1xICtWASGQqq5eJUEuJRJAYPLlJB+9beJFGCqpiqeHamVlzVb0bXae+dYEFLuS9fpXpaWQwM3o5DNmsaitcuVvvXnVKu+W4VQPuhrTwtGahyRoC+mup+5Xw4Ou91H2f0XkBdBJ4vo0Gu/ZLq4d0v7FfmLNknOlF7+X9+B+7hgCw', 'is_continuation': True, 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='3.4 Further Studies\\n\\nIn this section, we study several elements of understanding Donut. We show some striking characteristics of Donut through the experiments and visualization.\\n\\nOn Pre-training Strategy. We test several pre-training tasks for VDUs. Fig- ure 7(a) shows that the Donut pre-training task (i.e., text reading) is the most', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 11, 'orig_elements': 'eJzVUU1r3DAQ/SuDTwls3DW2rN2eQ6CXtpCkPYSwjKWRLWLLRhpvsgn975WcJSxpKfRUepJ4b958vHf3klFPAzneWZ19hMyIQshCFPW2qpQUSpWbWlZUrGXd4KaR2QqygRg1Msb6l8zYnhwOlMR6dDPvJpzI55M2qTbRfJgWGqeptwrZju7Dke7RtTO2FCJ/l5Frs/uIThHZuXloyEe8KH5EjOmJU5Myr+Bq9tyRh2uetY3aRB9n3FjuKYuC94dpXYtCSllXaPRWGLXWWKvSaNFsC43qnxy2QP4v3D914pMD7myAQCpNXsEjQYiOHCKyJ489HB0IMBqYnSYfGJ22roXLdFAO36OiGx8hjEPSevuQSNWhR8XkbWCrFvVSH8f5cW67+BLQUzTDvraPTWFvw4y9fV5cyE8j+YzeR3RPN2nx30SzLrERYl1vpDGNIl2WohFUxsS2pIoN/n/RfHHw1dMFe7QuOXodf0ztYXGcKfBbRNNpGWN4CGBGD98ub0MOV7a9gNkTyDM8X5IK0XvkJYDXSH7Rw5nNKV9BWgU8YYr7HGxYNMMY+A/R3P8ErS5RnQ==', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='() Pretrain Strategy (b) Backbone (c) Resolution 9071 mmm CORD (Accuracy) 80 3 N DocVQA Score (ANLS) 60 40 g 20 21035 yOA%Q g o preo S ERonn o rerECOa @) ResNeITRANAVZ TR 0 1008 34 6409640 960900, 504968, 60x1920\\n\\nAccuracy\\n\\nFig. 7. Analysis on (a) pre-training strategies, (b) image backbones, and (c) input resolutions. Performances on CORD [45] and DocVQA [44] are shown', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 12, 'orig_elements': 'eJzFkk2L2zAQhv/KYCgkkKayLX+op7q7W1goyW4S9tAQwlgeu6K2bGS53bDsf6/sJhBKLz3tQQi9o9HonXn2Lx7V1JC2R1V4H8GTUgRUloKHQgpRks8lk2EZCcqFTBl6C/AasligRXf/xStVTRobGpOLVg/22GFHZtkV5Xh3DNtTN4Wx62ol0apWfziHa9TVgBX1Lr73SFfewamdU456aHIyTveDV6dZerbjI7M5PBiyBpWGrdssVSeY5XP4jPJH3mqCmZzDhvq2HsZKIFjiQ9M0cLPe3MIsk3IwKE9zSBmEsILbVj49ZrCVrXG52errdg4xA86ggoBB4LMwgtM6e/fohBY6Qy1s4W7Tau2OhszdzRrh01RzRfe7TbbKnr7BbgMMfMZSCDnEnAm3QMRMMLaAiHERpwtX59kXARs7cWnSfePMe87x35NBNw+/KEIZFCJICxRRniRY5iLiRR77bz+ZS2ev3eyUrf/phrMySfIgDfM0TNMYOcvRD8OAl1GBcRK8iZtJMv/R8mv7X1S1hGQJmcb61KseHHsznI/AvJ9wVbqC/g+xivrFBK0apw35GV0noi4mgJXuBuvoumDcL+GBTNmaBrWk6fGJ5z2PDlPSGeM95+7sSO6/t7/09ShWaFxt9ZN244dfD78BxlA5uw==', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='simple yet effective approach. Other tasks that impose a general knowledge of images and texts on models, e.g., image captioning, show little gains in the fine-tuning tasks. For the text reading tasks, we verify three options, SynthDoG only, IIT-CDIP only, and both. Note that synthetic images were enough for the document IE task in our analysis. However, in the DocVQA task, it was impor- tant to see the real images. This is probably because the image distributions of IIT-CDIP and DocVQA are', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 12, 'orig_elements': 'eJxFkk9v2zAMxb8K4bPjNVkax7sNy/7k0m1Y0UsQBJRE2UJlyZDkZkbR7z7KSbqj+J7E3xN5eC3IUk8unYwqPkGxUbVYbZbyTtRab7VYb++WtVaN3Gjc1lIWJRQ9JVSYkP2vhTaWHPaULyvvxnQacKBQDUpnb5bTNMwyDoM1EpPx7sNVtujaEVuKrB8Kcm1x5OrAlZMbe0GB68vVXAr/IVE2zVKpj3KlmtVWYXMv6hq1aO7XSmyWWLzxjUR/UzZH0w+WYKIEpDXJZF4IGCV4lF0FP1NHARLG5wipwwRs95Ed0JKjgBaenT9bUi2B16xmWkCnIDeI4B30XpGNJVDVVuXFARKHnNO4toTY+TNYkxJjtGhcBOO4F4E2jhZpzK4LQQXffJil/DgEQvWulXAmeKFg9MSOQIwzt2Dhz+RSt/PfGcZOJez3j4svu/2v6zGzCp8464NPdAkZ8w1KRt4CnSkQkPNj24G+Migvx7wasP86I2RsPwZ+EO0UDdP+8GdipPIWaOfl0+/Ps5lrCc4Y5/8MC67xQ8lDJJqtnM1em1fw2Bk2RuChCBR2AkESx3hxXj5UmZiCEeMcOU/iPWXOd23MSwI8b2MxwGG9PlZ5x27r94AhYJ7+Y96Mt+M/cKQECg==', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='similar [44].', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 12, 'orig_elements': 'eJxFkk9v2zAMxb8K4bPjNVkax7sNy/7k0m1Y0UsQBJRE2UJlyZDkZkbR7z7KSbqj+J7E3xN5eC3IUk8unYwqPkGxUbVYbZbyTtRab7VYb++WtVaN3Gjc1lIWJRQ9JVSYkP2vhTaWHPaULyvvxnQacKBQDUpnb5bTNMwyDoM1EpPx7sNVtujaEVuKrB8Kcm1x5OrAlZMbe0GB68vVXAr/IVE2zVKpj3KlmtVWYXMv6hq1aO7XSmyWWLzxjUR/UzZH0w+WYKIEpDXJZF4IGCV4lF0FP1NHARLG5wipwwRs95Ed0JKjgBaenT9bUi2B16xmWkCnIDeI4B30XpGNJVDVVuXFARKHnNO4toTY+TNYkxJjtGhcBOO4F4E2jhZpzK4LQQXffJil/DgEQvWulXAmeKFg9MSOQIwzt2Dhz+RSt/PfGcZOJez3j4svu/2v6zGzCp8464NPdAkZ8w1KRt4CnSkQkPNj24G+Migvx7wasP86I2RsPwZ+EO0UDdP+8GdipPIWaOfl0+/Ps5lrCc4Y5/8MC67xQ8lDJJqtnM1em1fw2Bk2RuChCBR2AkESx3hxXj5UmZiCEeMcOU/iPWXOd23MSwI8b2MxwGG9PlZ5x27r94AhYJ7+Y96Mt+M/cKQECg==', 'is_continuation': True, 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='On Encoder Backbone. Here, we study popular image classification back- bones that show superior performance in traditional vision tasks to measure their performance in VDU tasks. The Figure 7(b) shows the comparison results. We use all the backbones pre-trained on ImageNet [7]. EfficientNetV2 [55] and Swin Transformer [40] outperform others on both datasets. We argue that this is due to the high expressiveness of the backbones, which were shown by the strik- ing scores on several downstream', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 12, 'orig_elements': 'eJxlU8Fu2zAM/RXCpw1oMttJ6nnHYR22S3dY1h6yIKAk2hYqS4YkJw2K/vso18WWFTBgg3qPfO9R3j1lZKgnGw9aZZ8gq+T1ioSsmmYlqyKvmqKo13nZiGpVi/w6z64g6ymiwoiMf8oabchiT4msnB3jYcCB/HJQTcKm43gepmMcBqMlRu3sh/nYoG1HbCnw+S4j22Z7rg5cOdixF+S5XpRTyf8VibKuC6VWslR1+VFhvRFVhY2oN2slrgvMnpkR6TEm8A8LN1Y6RR4+o3wQztISvpGnKzgRhDiqMwxuGA160D1PBmkwBN3MUkEwawGJFyB2GCF07gRhZJfaeeBX43yPVhJoC9Gj0omHBo46pAYRwwNTHfSEYfTEXUi/Id59+fWCXMK2I/iq2wSt3on308A0m6W5npPQgbt6CqOJjL4nGAMBGjNBxGwywOBpwXK0JQVM+J7M3VKEXbVfwk3DBjVnypW7EnabzR7QKvh5Yi1bjzYkcRzabp3vwY1xlguOh/iQGgr+hHQRAs060LcjvYQUOx2AH5UKblLW6bYDemRZHO+RWCK3aS5F81I6LTteDZtPvnnMeYKE6DXvQdsWgnTcI0kIdCTPSSsGMoCwn9PGwC2MmVTJzjkO6I2z/6UFiQaFNjqeX3X9A18I9qnYoux0JBnTdlJgiUqX23QsaqJfhvx7LPOiDst08V//iVv0nq/Zkbbpuj7v/wDc2D1z', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='tasks as well. We choose Swin Transformer due to the high scalability of the Transformer-based architecture and higher performance over the EfficientNetV2’s.', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 12, 'orig_elements': 'eJxlU8Fu2zAM/RXCpw1oMttJ6nnHYR22S3dY1h6yIKAk2hYqS4YkJw2K/vso18WWFTBgg3qPfO9R3j1lZKgnGw9aZZ8gq+T1ioSsmmYlqyKvmqKo13nZiGpVi/w6z64g6ymiwoiMf8oabchiT4msnB3jYcCB/HJQTcKm43gepmMcBqMlRu3sh/nYoG1HbCnw+S4j22Z7rg5cOdixF+S5XpRTyf8VibKuC6VWslR1+VFhvRFVhY2oN2slrgvMnpkR6TEm8A8LN1Y6RR4+o3wQztISvpGnKzgRhDiqMwxuGA160D1PBmkwBN3MUkEwawGJFyB2GCF07gRhZJfaeeBX43yPVhJoC9Gj0omHBo46pAYRwwNTHfSEYfTEXUi/Id59+fWCXMK2I/iq2wSt3on308A0m6W5npPQgbt6CqOJjL4nGAMBGjNBxGwywOBpwXK0JQVM+J7M3VKEXbVfwk3DBjVnypW7EnabzR7QKvh5Yi1bjzYkcRzabp3vwY1xlguOh/iQGgr+hHQRAs060LcjvYQUOx2AH5UKblLW6bYDemRZHO+RWCK3aS5F81I6LTteDZtPvnnMeYKE6DXvQdsWgnTcI0kIdCTPSSsGMoCwn9PGwC2MmVTJzjkO6I2z/6UFiQaFNjqeX3X9A18I9qnYoux0JBnTdlJgiUqX23QsaqJfhvx7LPOiDst08V//iVv0nq/Zkbbpuj7v/wDc2D1z', 'is_continuation': True, 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='On Input Resolution. The Figure 7(c) shows the performance of Donut grows rapidly as we set a larger input size. This gets clearer in the DocVQA where the images are larger with many tiny texts. But, increasing the size for a precise result incurs bigger computational costs. Using an efficient attention mecha- nism [60] may avoid the matter in architectural design, but we use the original Transformer [58] as we aim to present a simpler architecture in this work.', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 12, 'orig_elements': 'eJxNUktv00AQ/isjn0AKIXZIU3MDVUhcQKDAJYqi8e54PcL70D4aQtX/zqzbCi62PPPt9/IeHxqayZLLZ9bNe2ha2uGw7fZIG9RDT9223RDiTb/bd5ratllBYymjxoyCf2hGnsmhpXpYe1fyOWCguA56rNi6ztewrDGEmRVm9u7t83pGZwoaSrI/NuRMc5JpkMnZFTtQlHnbLaP4zySqvm+13qpO992txn437Pc4Dv3unR5uWmwe5USm37mCvzr47ELJ8J2Sn0tVX8NhIvjEpkSC/Sv1GtLkLwmyTMX76KNFpwj8CHc1EphY1xED6/kKmOBCkCgDwozRUAReFBL/ocrNCQzlBGomsV23C/WdVz+/fYDLRCJbB2xrdBDMC8+F8wQifoXM9SEZ0ho+lrwSEhUJEzuznK1aIE7FQ4ikOBFESmXOFVhigoFNZVTeirWldJzlK1XCHwsNOqBxZMXSLGDO8hIUWFITvgHHycLxZnMSP5L53rNehG1FLpkwqokzqVyiUGtKbNwKBilC6inpKaOPbLhKHyK6VKuVw8fd7em5RmQL2dcMabEhwWyYBfMfOz01KLVefPy1rhfn5U59wRgl3D0d6u9+PP0FPXr3ww==', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='On Text Localization. To see how the model behaves, we visualize the corss attention maps of the decoder given an unseen document image. As can be seen in Figure 8, the model shows meaningful results that can be used as an auxiliary indicator. The model attends to a desired location in the given image.', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 12, 'orig_elements': 'eJxNkbtuwzAMRX+F0BykdRI3drcunYp26VYUASXSjgBZMvRIX+i/l3L62gTee8kj8ulDseOJfT5YUtegti1r3Tdam03babq87BrCrtlt2z12vW7VCtTEGQkziv9DDdaxx4lrmIIv+TDjzHE901C9Vc5v8yLjPDtrMNvgL75lh34sOHIS/UmxH9WzVGepHHyZNEepN5ulFP8g0fR9Q7Q1G+o3HWHf6v0eB923O9JXDapPSWR+zdX84OFRnnAXDDr7voxfw2OAxAzH8AL5yDAFYgeaj3jitIIXhpNNpfp50U2IKQHmLBCShwnnBGFYNGIj6QijPbEH9FC8tPZAwZS6WbCT/GcNNwmMqJphka2HWzuWyNCt/jEkQUowMXrrx6E4iJyKy0ksmH8alMQEmOowLK/WWYxv0pDqdkOU3/22W5BJ0gFQSJONknThfIXKUCefyc+Y9So/B7vHGMV44rpA9fn8BVSqvDE=', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='On OCR System. We test four widely-used public OCR engines (See Fig- ure 9). The results show that the performances (i.e., speed and accuracy) of the conventional OCR-based methods heavily rely on the off-the-shelf OCR engine. More details of the OCR engines are available in Appendix A.1.\\n\\nOCR-free Document Understanding Transformer  13 ', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 12, 'orig_elements': 'eJzFUsFq3DAQ/ZXBpwaybrQb26veQkuhhzbQpPQQwjKWRmuBLAlJ3mQJ+feOloZu+wM9WIZ5bzRv3tPDS0OOZvJlZ3XzAZqxM0ooOapObfXYd+qahmEQciNlT/1aN5fQzFRQY0HmvzTGOvI4U23WwS9lFzFSaqM2lVvhcownGGN0VmGxwb//DTv0+wX3lBl/aMjvm0euRq7s/DKPlLgu1qdS+iMSlZRC641aa7neapTdOAxoRtlds2aBzSt3FHoulXzr4fbjd7g75kJzCz8JCuUCJiwJnqwmd1wtmTTEZWR1Jy7rsJ4yvLsjgs92v4IlEciLFu4ngkR5cSVDnsITlAkLHwS8swlpRq9qo22pvYQciS9Gz59SS0J1vIBgTnQV/IEXYivQ1ZmrEasItnYKOsNEeLDuyLP4CP7UEoxZ8X+VJ3LmTGcLXwPL05yKdfltwPke7B3ggVEcHYH1cBMjeW2f4aYVbc3hLaJvmBIHdKD76h7b+O/7ENJohSOaXumtuhJDL7tBbwSJfmuujPgf72PzV9zspUmc26egliocfnhNKReOwfo93Cf0uSZFCbgVzrf/MvPFzevjL0pQAmc=', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='Fig. 8. Visualization of cross-attention maps in the decoder and its applica- tion to text localization. Donut is trained without any supervision for the localiza- tion. The Donut decoder attends proper text regions to process the image\\n\\n= Easyock = Easy00 %0 = PoddleoCh = PaddieOCR g = Mshzre = CovaocR = MShzure = Covaock a0 /_/7 ———— 3 29 & - e Accuracy 2 5 s o Acguracy g time(sfimage) — Donur, 2560 Layoutmz ol i Do, 1260 BERT 40 Layouttivz BERT 18\" 05 Layoutlmvz 18\" u number of samples o', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 13, 'orig_elements': 'eJzFktuK2zAQhl9lMLS0kIMPsRsXetGmWyj0RBp6s7uEiTRyxNqWkeR0k2XfvSM7WZb2AXpjxt/M/BrNr+uHiGpqqPVbLaO3EBUqIyx3RS6KpUpimYs4UQXGalfmKi7SaAJRQx4leuT6h0jpmlpsKDRL0/Z+22FHdtZJFWpD2h+7IY1dV2uBXpt2fk7X2FY9VuQ4fx1RW0W3TDsm27ZvdmSZJ9kjM0/3Poh80tUMljP4pV2PtT4NcmAUCGucm6L3fJmAGuwc6Bb8nkCSMJIsYCtBewfnSaYwVHoDQR1qI54UZ/AxXAa0A29RtyTht/Z7wwjbI7ie73jQLrQrY4dDLu2j6gw2zEaRp+PDcNJBZw23j4daqrjahSEYC3JuENMN7yBs6LK8b2gtT3agTVgEb+Rv58qMFnnOjim1I7lUpMpCCJFlJRYk0vi/O/cOrtAdjbiDMYpjeBFz/MNIWZNZ7UOMUmr6vlpDxX9f3f5kiYOVOaAR64B+7k/9M3YHGMN8O38DN30aJ4t/v5BBWsJLmALBeyF6i+IIKeTgwDCoRlCxaQ29cmpY/Ouz2uCfnUCaFzF8wSPb35zA1KA5M4EkZfzhar2BxSXNDp1GlCxvIojzM68b5gPqYdxPeLMOm64mHuS5058H6x9v/wCG2Ctm', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='Fig. 9. Comparison of BERT, LayoutLMv2 and Donut on CORD. The perfor- mances (i.e., speed and accuracy) of the OCR-based models extremely varies depending on what OCR engine is used (left and center). Donut shows robust performances even in a low resourced situation showing the higher score only with 80 samples (right)', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 13, 'orig_elements': 'eJw1kUtrwzAQhP/KolMCqZs4D5Iem7anlkDoLZSwtla2QJaEHklD6X/vyqTXnZ3ZT6PTjyBDA9l01lI8gdjUq91crdVmXctGyZ2q58tmu9zU68V8sV1txAzEQAklJuT9H6G0IYsDFbN0NqezR0+h8lKV3SKnmx9l9N7oFpN29vEuG7Rdxo4i6ydBthNfPPU8Ods8NBR4vlj+8izRdyohb7qrYFfB3g0eg47OglPw/Hr8nME73lxO7x+XGtBKeCk4wAv7w/Glgs+egMmUCw8woG0pwkRXVM0geiI5WrBtc8D2Ni2hiQ2H/fGhwcjy4CSZCIwRuDBzgwuf5wxJnqzUtiuXrj2m4gF+irYEOkIu5okhlcYLLXdNYVrd6WLvrhGCa3JMd7o7G13IgraAYNwVAkWXQ8tRUac8djh6y93C2euupwCxdYEYhPGuOvWwnUPEwZvy1sAraVpa//8Q7jIH2qMvceL36w9F7LBN', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='On Low Resourced Situation. We evaluate the models by limiting the size of training set of CORD [45]. The performance curves are shown in the right Figure 9. Donut shows a robust performances. We also observe that a larger input resolution, 2560×1920, shows more robust scores on the extremely low- resourced situation, e.g., 80 samples. As can be seen, Donut outperformed the LayoutLMv2 accuracy only with 10% of the data, which is only 80 samples.\\n\\n4 Related Work\\n\\n4.1 Optical Character Recognition', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 13, 'orig_elements': 'eJzFUk1v2zAM/SuEgd0yT5Y/Eu82tNipW4GuQA9dEdA2bQuTJUOSk2XF/vsoJwWKofce9fj0yEe+x+eENE1kwl51yWdI+l1OdYm5LHZIHVXU5aIUtcxltW0KLJINJBMF7DAg85+TXmkyOFH83FmzhP2MM7l07vrIjeVwmtcyzrNWLQZlzadLWaMZFhzIc/0xITMkT4zOjOzNMjXkGM/yv4wF+h2iyK2BG3uEO/J2cS118EOFZdVM4YGADqj5SRBGgsl2pD00J9BqUkGZYYW9+kNgewgOlYmgpxDfV7d31/BYlE8p3DONXfTWTWhagnZxB/KAjn+P9mhAmVXKqWEM8FUNC1fqFK7jBlYKk8HZZvHhtZBfh0TtLdjGE4uyDAbmanQDOdadWcCxO71EUxuQZSV+LkJ026yWYnMRnyw3vOj7lh8e7Hkk3pPji2o2bY8fV6nzovzLojZA6ZBuYCfA4zTrONUXDy0aaNgfETPORuwSLsPz/yh+gyfGbr4dJGDLS8H2xH2511GFETLxYd0rE2M+NnAcVTuC8mfOq37x9i+x+I7O8VwHuo8n5lv/n8miLIu2rCraCtzirii227qspMC+k6WUu3fPZMFx1By6Dh6s+/Xa270Kmt7yJHuRUd60uSwzWWd1UWSybERLu0pkuejf31Oawe0cWEfD1Yh86cDxvKPWDkZF6TdsPv0DuPBqCA==', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='Recent trends of OCR study are to utilize deep learning models in its two sub- steps: 1) text areas are predicted by a detector; 2) a text recognizer then rec- ognizes all characters in the cropped image instances. Both are trained with a large-scale datasets including the synthetic images [26,13] and real images [28,47]. Early detection methods used CNNs to predict local segments and apply heuristics to merge them [19,69]. Later, region proposal and bounding box regres- sion based methods were', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 13, 'orig_elements': 'eJxlUstu2zAQ/JWFbgFkVw/bidpbg96KFAh6M4xgRa4kAhRJkFQSNci/d1dO0ENPgoazM7OP81tBlmZy+cno4isUdTc09e2RjtWtog4r7JWu+qFtUJHCpitKKGbKqDEj89+KwVhyOJMUa++W/BQwUNwHPQhXnvMatmcMwRqF2Xj35ePZohsXHCnx+7kgNxYXRgMjT26Ze4qM1+0GxX8hm6Gqqe1V2xzrpqu7w6Fujn2l6O5U1W01FO9ckek1C/mRFFdC5nqdwA/w6/4RUl70CqwJ2cOSjTV/CDRRAEsYnXEjzF6TTWAcmJwgv3hIS7/jSgqJQ92AGIgEpk0oRNJGZdLQszKLZVLZx2/Q3PDvRo6k/OjYKkKeyMn/Dq4Ia1gLasKIrBE3X+aAij4E1jQzz4TBlNEpSnv47vN0bSCiccx4MQKAxTjSLim03BAvKVEWMWUXLV2JZlodf7JRV9UE5+ZU1u0F0GnOhPYfflcebi97+IHRrh8t8faAL2DyPM0lsfH9w0OSMX4MAKxnc0g0ylmlTVQ2v8JESzSJfTf6TBxU8sxwrrvy1LHPT+TeS84wikvg3n1iLZHo/eK2Dnr/KoRIiZchtB4lxWekF9p2IZWMntsTy15PwK4lDF4tSVT8dbyTn/1IjkxeN5ctu/zwncjKUgnKz8E7rt9ZeiYrvUSPaqL/vI6n8nDZy1l/XvwDxsj3/ky/5RjfL38ButQtbA==', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='proposed [36]. Recently, focusing on the homogeneity and locality of texts, component-level approaches were proposed [56,4].', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 13, 'orig_elements': 'eJxlUstu2zAQ/JWFbgFkVw/bidpbg96KFAh6M4xgRa4kAhRJkFQSNci/d1dO0ENPgoazM7OP81tBlmZy+cno4isUdTc09e2RjtWtog4r7JWu+qFtUJHCpitKKGbKqDEj89+KwVhyOJMUa++W/BQwUNwHPQhXnvMatmcMwRqF2Xj35ePZohsXHCnx+7kgNxYXRgMjT26Ze4qM1+0GxX8hm6Gqqe1V2xzrpqu7w6Fujn2l6O5U1W01FO9ckek1C/mRFFdC5nqdwA/w6/4RUl70CqwJ2cOSjTV/CDRRAEsYnXEjzF6TTWAcmJwgv3hIS7/jSgqJQ92AGIgEpk0oRNJGZdLQszKLZVLZx2/Q3PDvRo6k/OjYKkKeyMn/Dq4Ia1gLasKIrBE3X+aAij4E1jQzz4TBlNEpSnv47vN0bSCiccx4MQKAxTjSLim03BAvKVEWMWUXLV2JZlodf7JRV9UE5+ZU1u0F0GnOhPYfflcebi97+IHRrh8t8faAL2DyPM0lsfH9w0OSMX4MAKxnc0g0ylmlTVQ2v8JESzSJfTf6TBxU8sxwrrvy1LHPT+TeS84wikvg3n1iLZHo/eK2Dnr/KoRIiZchtB4lxWekF9p2IZWMntsTy15PwK4lDF4tSVT8dbyTn/1IjkxeN5ctu/zwncjKUgnKz8E7rt9ZeiYrvUSPaqL/vI6n8nDZy1l/XvwDxsj3/ky/5RjfL38ButQtbA==', 'is_continuation': True, 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='Many modern text recognizer share a similar approach [37,53,52,59] that can be interpreted into a combination of several common deep modules [3]. Given the cropped text instance image, most recent text recognition models apply CNNs to encode the image into a feature space. A decoder is then applied to extract characters from the features.\\n\\n4.2 Visual Document Understanding', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 13, 'orig_elements': 'eJzFUk1v2zAM/SuEz0HmD1m2dxs2oKfm1O0SBAEl0YkAWxYkOWhW9L+PMlps7R/YyfIj+fgeyeNLQRPN5NLZmuIrFJVQgzDYKlSy6eTQ4tB1rVYoJaIUZbGDYqaEBhNy/ksx2okczpSLzeLWdPboKey9GXNuDqe738Lo/WQ1Jru4L2/hCd1lxQtFjh8LcpfixKhn5OzWWVFgvGo2KPwVWY9lRY3STd1W9VANQlR1q0pNvSyrphyLV65I9Jxy8iO6O8yLoeAgYxBILxdnf1OAeGVaQIh2thMGYIVhQX2FY9Pt2mbX1rt2OEG6YgKNDhSBdYmCD5TI5PfC1XqZlXWbL1hGiHSjgFOGZ0YMkc/914ki85728GBvxFquBDos3jPRpsu6mNBpbjGz/x3XxE0s+/4gfOuTDU0xC57u8P1wiMBSyGmGN+aN413gSJhWNho9atrDN9aUEwPYmJPdRmOzDuZ4TgE12+XR8JdChDEs80b6xhP3eXXvWz1gCOz9Rk954Dz5zyc1DmXdlW3Hh1RKZdpetvzXV7w1XVcj/Y+TEp9PStRSyw7HAbEXphr6clRKkBior6WpPp6U2Nfwy8aVl/xj0Wv2Cj8dDzQv0Fhu+c98nmyaqHg9/QFqdx5v', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='Classification of the document type is a core step towards automated document processing. Early methods treated the problem as a general image classification, so various CNNs were tested [27,1,15]. Recently, with BERT [8], the methods based on a combination of CV and NLP were widely proposed [65,34]. As a common approach, most methods rely on an OCR engine to extract texts; then the OCR-ed texts are serialized into a token sequence; finally they are fed into a language model (e.g., BERT) with', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 14, 'orig_elements': 'eJxVU11v2zAM/CuEnzbAy2LHjtP1aQv6NmRDMewlCAraph2hluRJcrKs6H/fyf1C32zyjjweqf1DIoNoMeFOtckXStqmXUtetpuVNKsyb+oiX2eborwq26paL7skpURL4JYDA/+QdGoQw1pmsjVTuBt5FLcY2xkb0+Eyzmkex0E1HJQ1n5/TA5t+4l488vtETJ8cEB0RuTOTrsUhnhVzyL2J7K6WebUsq6uqWq7rttysS/xtsqLImjzrJHkEI8jfEMHbgb1X3XNjsh2Fo1BrmymOTVEcKU9MjXVCPshIwZ7ZtYhNwWoO0r7BR2cbQT3TL+iG3XAhuHG0AAcnMzRWB6qGrcSxbi9GHA+kNMai5p2clLylEztlJ0/b3c7TWaAiiI+l9nmVZmlWHhZ0Kw3aD5eUzioc6dvN7S/ab2BWbPcioWYPFoaMw+hamdeRt7+JTUu77z+fGpxVK9AOnaONnP26TFcF+nx9ckLrWGVEnptjStr68NrFRWZMG/qxvSUsTRlItgTDHTewFB/+OiozszygPkVjYpg4mixO8aD+IagMiAz2PcBe/kxiGrmmDtoHtAH9MlO6N+zLzUAVhqAPsugX6ezIxydzvNVCJ+UnmN5hKZMTT6ojPrEaGIvBmANmmfrjrA9ecDwBr/Q4yHtL/dGe0dqJZncfuYTj7qzTDJ1YKRw6ze/HzwbX0uAtQKRmZeJJmCgb9Lg9ugg7D6+LdFWm+WERX8DL49ixc1jXSX7Fu308/Ae2TDlw', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='some visual features if available. Although the idea is simple, the methods showed remarkable performance improvements and became a main trend in recent years [64,35,2].', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 14, 'orig_elements': 'eJxVU11v2zAM/CuEnzbAy2LHjtP1aQv6NmRDMewlCAraph2hluRJcrKs6H/fyf1C32zyjjweqf1DIoNoMeFOtckXStqmXUtetpuVNKsyb+oiX2eborwq26paL7skpURL4JYDA/+QdGoQw1pmsjVTuBt5FLcY2xkb0+Eyzmkex0E1HJQ1n5/TA5t+4l488vtETJ8cEB0RuTOTrsUhnhVzyL2J7K6WebUsq6uqWq7rttysS/xtsqLImjzrJHkEI8jfEMHbgb1X3XNjsh2Fo1BrmymOTVEcKU9MjXVCPshIwZ7ZtYhNwWoO0r7BR2cbQT3TL+iG3XAhuHG0AAcnMzRWB6qGrcSxbi9GHA+kNMai5p2clLylEztlJ0/b3c7TWaAiiI+l9nmVZmlWHhZ0Kw3aD5eUzioc6dvN7S/ab2BWbPcioWYPFoaMw+hamdeRt7+JTUu77z+fGpxVK9AOnaONnP26TFcF+nx9ckLrWGVEnptjStr68NrFRWZMG/qxvSUsTRlItgTDHTewFB/+OiozszygPkVjYpg4mixO8aD+IagMiAz2PcBe/kxiGrmmDtoHtAH9MlO6N+zLzUAVhqAPsugX6ezIxydzvNVCJ+UnmN5hKZMTT6ojPrEaGIvBmANmmfrjrA9ecDwBr/Q4yHtL/dGe0dqJZncfuYTj7qzTDJ1YKRw6ze/HzwbX0uAtQKRmZeJJmCgb9Lg9ugg7D6+LdFWm+WERX8DL49ixc1jXSX7Fu308/Ae2TDlw', 'is_continuation': True, 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='Document IE covers a wide range of real applications [22,42], for example, given a bunch of raw receipt images, a document parser can automate a major part of receipt digitization, which has been required numerous human-labors in the traditional pipeline. Most recent models [25,23] take the output of OCR as their input. The OCR results are then converted to the final parse through several processes, which are often complex. Despite the needs in the industry, only a few works have been attempted', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 14, 'orig_elements': 'eJxNU0tv2zAM/iuEz66XuHEeO27dYYdtQNFbURSMRTla9ZpEN82K/fdRSrPulJjkx+9h+v61IUuOPD8a1XyEZhzWWqtdv1oPuNVbtVvvrgetNnvUaxwXq6aFxhGjQkaZf220seTRUQGr4Gd+jBgpdVHpMlvafIq1jTFaMyKb4D+8tS36acaJsvTvG/JT8yDVKJVHP7s9JakvV7WU3kXq3aLfLIbNbrNZrPdq2K4HedouV6vl2C81NX8EwfTCZfgmjHMxCF+/wBieKWVAOBpFkIScIGhIhBb+U5fhvu/bVS9adEhAL+iipRYm80xe0PvZj4cKxKOARzKRwbjio5W2ujCK6EwJRhTQzMEhk7Qd/pSl0uMz9xmuzGTY/K78LRwPRhgOmGFPQpno12wSKZBQKIU5w2F26K8s7oP4MR74QMAJlSl4cRNNJGs8dfAtZK4sIsgFRba4G9r++gEYn6giw8xxrnJ+fL4FYZWiSbJXqh3cyUSpJ8qzZYkvVZSXOL3kyaKLQ92jTeUutuVZhE4HyCQzpZrCSDmXiM7uypqgue4p+b50cEM5Gj5r8kTqnzXj1Zw5nVoI3p4kRE1HOIb0JEngM51TQmZyscgJHsirKw5X8lP1GD91cFtTsLIlB0dv+CJDtMWQzz6yETFGnyrvm7LLincTlww7+CTByegJMhtrJSTRF8qVZBJUeeUlO1ks95hwlGH5I4bkslx92135EC7fyHdMgpE7uyvn++fhLzJEQJw=', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='on end-to-end parsing. Recently, some works are proposed to simplify the complex parsing processes [25,23]. But they still rely on a separate OCR to extract text information.', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 14, 'orig_elements': 'eJxNU0tv2zAM/iuEz66XuHEeO27dYYdtQNFbURSMRTla9ZpEN82K/fdRSrPulJjkx+9h+v61IUuOPD8a1XyEZhzWWqtdv1oPuNVbtVvvrgetNnvUaxwXq6aFxhGjQkaZf220seTRUQGr4Gd+jBgpdVHpMlvafIq1jTFaMyKb4D+8tS36acaJsvTvG/JT8yDVKJVHP7s9JakvV7WU3kXq3aLfLIbNbrNZrPdq2K4HedouV6vl2C81NX8EwfTCZfgmjHMxCF+/wBieKWVAOBpFkIScIGhIhBb+U5fhvu/bVS9adEhAL+iipRYm80xe0PvZj4cKxKOARzKRwbjio5W2ujCK6EwJRhTQzMEhk7Qd/pSl0uMz9xmuzGTY/K78LRwPRhgOmGFPQpno12wSKZBQKIU5w2F26K8s7oP4MR74QMAJlSl4cRNNJGs8dfAtZK4sIsgFRba4G9r++gEYn6giw8xxrnJ+fL4FYZWiSbJXqh3cyUSpJ8qzZYkvVZSXOL3kyaKLQ92jTeUutuVZhE4HyCQzpZrCSDmXiM7uypqgue4p+b50cEM5Gj5r8kTqnzXj1Zw5nVoI3p4kRE1HOIb0JEngM51TQmZyscgJHsirKw5X8lP1GD91cFtTsLIlB0dv+CJDtMWQzz6yETFGnyrvm7LLincTlww7+CTByegJMhtrJSTRF8qVZBJUeeUlO1ks95hwlGH5I4bkslx92135EC7fyHdMgpE7uyvn++fhLzJEQJw=', 'is_continuation': True, 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='Visual QA on documents seeks to answer questions asked on document im- ages. This task requires reasoning over visual elements of the image and general knowledge to infer the correct answer [44]. Currently, most state-of-the-arts fol- low a simple pipeline consisting of applying OCR followed by BERT-like trans- formers [65,64]. However, the methods work in an extractive manner by their nature. Hence, there are some concerns for the question whose answer does not appear in the given image [57].', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 14, 'orig_elements': 'eJxNUsuO2zAM/BXC5zjNw3aS3tpFgZ5adBH0YgQBbdOOYFn0SnLSYLH/XjIPtAcDMjkczoxUvidkaSAXj6ZJPkNS7Ta7bbZbV6sqXyNW9aJar9o6KxZ5W1S0TmaQDBSxwYiCf09aY8nhQDrcsJviccSR/HxsWsVqO17HWxvH0Zoao2H36dG26LoJOwrSLxNyXXKQ6iiVo5uGirzUl9mt5P+JbHeL1WaRb3abzaKomnxb5PK3XWbZsl4tW0o+ZCLSn6jg3yZMaOHXF2AHDdeTug0QiPoAkQFduJCHt4mCKguAoafmfzCYIQUVOYf9yciQIMDT22Q8BTlgYGdcB3wWnvN93SPVANxCPJFQCIHsaqAjR14QveOLpUaqIsK4VmYVWLP3VMenrDLLDnN4mbzat9cZDBwihIiRUm5TmUjRy5qWbQqWL4AQzDBagtGMZI1TRheMeFOFLeglXPX88+VVp2RG3FZX+PrtdZ9a04seL8tTafqBfICyyGeFqvguULE4u+mUR3DiJsCFfS/yRS9I4B7raM7SRSculVawxoPDOHkSCnI13Qi8pCFf4OGmsCbv1MU9hOdlwOXEgZ5ZNCxxO47qgdDrVgV3stA9Ai7zjQjds9xR3UsI90Tv7LNH9EqcVhjE9tPECUUz2sBQkXCNnkfWfpltD3N9p88n/AO9Epxpr6/r4/AX/YQdpg==', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='To tackle the concerns, generation-based methods have also been proposed [48].', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 14, 'orig_elements': 'eJxNUsuO2zAM/BXC5zjNw3aS3tpFgZ5adBH0YgQBbdOOYFn0SnLSYLH/XjIPtAcDMjkczoxUvidkaSAXj6ZJPkNS7Ta7bbZbV6sqXyNW9aJar9o6KxZ5W1S0TmaQDBSxwYiCf09aY8nhQDrcsJviccSR/HxsWsVqO17HWxvH0Zoao2H36dG26LoJOwrSLxNyXXKQ6iiVo5uGirzUl9mt5P+JbHeL1WaRb3abzaKomnxb5PK3XWbZsl4tW0o+ZCLSn6jg3yZMaOHXF2AHDdeTug0QiPoAkQFduJCHt4mCKguAoafmfzCYIQUVOYf9yciQIMDT22Q8BTlgYGdcB3wWnvN93SPVANxCPJFQCIHsaqAjR14QveOLpUaqIsK4VmYVWLP3VMenrDLLDnN4mbzat9cZDBwihIiRUm5TmUjRy5qWbQqWL4AQzDBagtGMZI1TRheMeFOFLeglXPX88+VVp2RG3FZX+PrtdZ9a04seL8tTafqBfICyyGeFqvguULE4u+mUR3DiJsCFfS/yRS9I4B7raM7SRSculVawxoPDOHkSCnI13Qi8pCFf4OGmsCbv1MU9hOdlwOXEgZ5ZNCxxO47qgdDrVgV3stA9Ai7zjQjds9xR3UsI90Tv7LNH9EqcVhjE9tPECUUz2sBQkXCNnkfWfpltD3N9p88n/AO9Epxpr6/r4/AX/YQdpg==', 'is_continuation': True, 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='5 Conclusions', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 14, 'orig_elements': 'eJxFjsFqwzAQRH/F7LmksSPLVq/9hdxCMGvtyghkWdhrSAj590ghpdf3Zpi5PIADzxxl8AQ/FWg6GYMNWkuto9NR4ejaztWsLVFnFHxVMLMgoWDOP8D5wBFnLmVa4i5DwsTrIZEr2aLlnt4aUwreovglfn90wDjtOPGW/QU4TnDNNGUyxH0eec28Vm+0/p9Ujba6Q2cQe0W16Y9uHBUrw32jqXbwzA3hm5RwW/0u0YZ9y7Nb2fy7c/YSGJ7XFwXlUWc=', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='In this work, we propose a novel end-to-end framework for visual document un- derstanding. The proposed method, Donut, directly maps an input document image into a desired structured output. Unlike conventional methods, Donut does not depend on OCR and can easily be trained in an end-to-end fashion. We also propose a synthetic document image generator, SynthDoG, to alleviate the dependency on large-scale real document images and we show that Donut can be easily extended to a multi-lingual', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 14, 'orig_elements': 'eJxdVF2PEzkQ/CuteU4CIdkNe68HQrwc0h0nHhBadeyeGbMe2/JHlgjx36meyS4BKdJM7P6oqq6ez9878TJJqPfOdn9Rd9jtt+Zg9rvt7a7f7V7v99vD6/3d7uXh5tXLPUu3om6SypYrI/571zsvgSfRZBtDq/eJk+RNsr3G6nU9p/maU/LOcHUxvLhcew5D40EK7j93EobuC04TTu5Dm46Scb7dz0f5F8hbu7u741dsjL3p7Q64jv3Nod/KrbH2cLfvfiCjyreqwe8D1dEVeoz5YUWPQinHFIsQU4gn8STBrmtc40F9BhMNpD5mOrnS2JONpqlC1MKarORSOVgXhg19HJ+rWYIqY7QreqMirMi6LKb6M02cCnEgF1Krv4q5CSRxWCOAWCkIt1RqbqY2fY2tIn5D/wfvHoRMDCekQTogWlqVSy/UlAIueJGkLGKgD3//i56WDBoLFwccR6Ga2QXUdkEBXfPmMqL0hj5BFl/ilUblHOoo1Zk/oQ8SJHONeUX/acyb+G5FysZ7OTmuaAd5FkgSzFlhec6DrIthL5TlWtu5ZpkxY0RljI9I53phqDSA/8IEg9Wadu5GU/PVrb1TI3kqUus8GzAZMlucIeOJuCKaosXQ+xwnmrtERTLXuvxt4WnIiM+xDeOc9zzouRh6UHJJ0Fc29KHlBVVxJ8D8hgVwSmshxJjZucCCUOAY6ziHZh1kakdsBKgFM06cH5b4lN1J9YM5lrAi+eQMxMTSgWBZ9BndMEom9IJZJw4I0OwjFMCxiaWuSfoeLgSoIAX9+9+ZLD5SHwMcfpi2G4LrsaTzSBKbSlzmpKqjq3gURZnVJ6rbGdUQhQzVuZXFXS7YBi87KRt6G0ZgU72W3rJ+FjAe1/R1wQe8zSt4gOibrsC8sJc1evLmEXoLgiP0nq9/9/e1MaAzVLBxQq/yYoGdZeCsq3u909cbrZ+kp6/VP5xhb0D7qB+SH19+AjnT1zQ=', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='setting. We gradually trained the model from how to read to how to understand through the proposed training pipeline. Our extensive experiments and analysis on both external public benchmarks and private internal service datasets show higher performance and better cost- effectiveness of the proposed method. This is a significant impact as the target tasks are already practically used in industries. Enhancing the pre-training ob- jective could be a future work direction. We believe our work can', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 14, 'orig_elements': 'eJxdVF2PEzkQ/CuteU4CIdkNe68HQrwc0h0nHhBadeyeGbMe2/JHlgjx36meyS4BKdJM7P6oqq6ez9878TJJqPfOdn9Rd9jtt+Zg9rvt7a7f7V7v99vD6/3d7uXh5tXLPUu3om6SypYrI/571zsvgSfRZBtDq/eJk+RNsr3G6nU9p/maU/LOcHUxvLhcew5D40EK7j93EobuC04TTu5Dm46Scb7dz0f5F8hbu7u741dsjL3p7Q64jv3Nod/KrbH2cLfvfiCjyreqwe8D1dEVeoz5YUWPQinHFIsQU4gn8STBrmtc40F9BhMNpD5mOrnS2JONpqlC1MKarORSOVgXhg19HJ+rWYIqY7QreqMirMi6LKb6M02cCnEgF1Krv4q5CSRxWCOAWCkIt1RqbqY2fY2tIn5D/wfvHoRMDCekQTogWlqVSy/UlAIueJGkLGKgD3//i56WDBoLFwccR6Ga2QXUdkEBXfPmMqL0hj5BFl/ilUblHOoo1Zk/oQ8SJHONeUX/acyb+G5FysZ7OTmuaAd5FkgSzFlhec6DrIthL5TlWtu5ZpkxY0RljI9I53phqDSA/8IEg9Wadu5GU/PVrb1TI3kqUus8GzAZMlucIeOJuCKaosXQ+xwnmrtERTLXuvxt4WnIiM+xDeOc9zzouRh6UHJJ0Fc29KHlBVVxJ8D8hgVwSmshxJjZucCCUOAY6ziHZh1kakdsBKgFM06cH5b4lN1J9YM5lrAi+eQMxMTSgWBZ9BndMEom9IJZJw4I0OwjFMCxiaWuSfoeLgSoIAX9+9+ZLD5SHwMcfpi2G4LrsaTzSBKbSlzmpKqjq3gURZnVJ6rbGdUQhQzVuZXFXS7YBi87KRt6G0ZgU72W3rJ+FjAe1/R1wQe8zSt4gOibrsC8sJc1evLmEXoLgiP0nq9/9/e1MaAzVLBxQq/yYoGdZeCsq3u909cbrZ+kp6/VP5xhb0D7qB+SH19+AjnT1zQ=', 'is_continuation': True, 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='easily be extended to other domains/tasks regarding document understanding.', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 14, 'orig_elements': 'eJxdVF2PEzkQ/CuteU4CIdkNe68HQrwc0h0nHhBadeyeGbMe2/JHlgjx36meyS4BKdJM7P6oqq6ez9878TJJqPfOdn9Rd9jtt+Zg9rvt7a7f7V7v99vD6/3d7uXh5tXLPUu3om6SypYrI/571zsvgSfRZBtDq/eJk+RNsr3G6nU9p/maU/LOcHUxvLhcew5D40EK7j93EobuC04TTu5Dm46Scb7dz0f5F8hbu7u741dsjL3p7Q64jv3Nod/KrbH2cLfvfiCjyreqwe8D1dEVeoz5YUWPQinHFIsQU4gn8STBrmtc40F9BhMNpD5mOrnS2JONpqlC1MKarORSOVgXhg19HJ+rWYIqY7QreqMirMi6LKb6M02cCnEgF1Krv4q5CSRxWCOAWCkIt1RqbqY2fY2tIn5D/wfvHoRMDCekQTogWlqVSy/UlAIueJGkLGKgD3//i56WDBoLFwccR6Ga2QXUdkEBXfPmMqL0hj5BFl/ilUblHOoo1Zk/oQ8SJHONeUX/acyb+G5FysZ7OTmuaAd5FkgSzFlhec6DrIthL5TlWtu5ZpkxY0RljI9I53phqDSA/8IEg9Wadu5GU/PVrb1TI3kqUus8GzAZMlucIeOJuCKaosXQ+xwnmrtERTLXuvxt4WnIiM+xDeOc9zzouRh6UHJJ0Fc29KHlBVVxJ8D8hgVwSmshxJjZucCCUOAY6ziHZh1kakdsBKgFM06cH5b4lN1J9YM5lrAi+eQMxMTSgWBZ9BndMEom9IJZJw4I0OwjFMCxiaWuSfoeLgSoIAX9+9+ZLD5SHwMcfpi2G4LrsaTzSBKbSlzmpKqjq3gURZnVJ6rbGdUQhQzVuZXFXS7YBi87KRt6G0ZgU72W3rJ+FjAe1/R1wQe8zSt4gOibrsC8sJc1evLmEXoLgiP0nq9/9/e1MaAzVLBxQq/yYoGdZeCsq3u909cbrZ+kp6/VP5xhb0D7qB+SH19+AjnT1zQ=', 'is_continuation': True, 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='References\\n\\n23rd Nordic Conference on Computational Linguistics (NoDaLiDa). pp. 240–248. Link¨oping University Electronic Press, Sweden, Reykjavik, Iceland (Online) (May 31–2 Jun 2021), https://aclanthology.org/2021.nodalida-main.24 3, 11\\n\\n11. Friedl, edn. mastering-regular-expressions/0596528124/ 5 J.E.F.: Mastering Regular Expressions. O’Reilly, Beijing, 3 https://www.safaribooksonline.com/library/view/ (2006),', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 15, 'orig_elements': 'eJzVUk1P3DAU/CtPObFSSGwnmzgcW0AC8VFRegKEnuOXxWziRI53lxXiv9dZbQtFvVe92W9m9MYzvnuNqKWOrH80OjqCCJkuNdV1IyvKhWYCK10UitVcyqqQVRRD1JFHjR4D/zVqTEsWO5rEurcr/zjgQC4ZdDNxJ9hvhx2Mw9CaGr3pbbqHW7SLFS5oDPhdRHYRPYTpECaPdtUpcmHO57uRezdZN4wUF01WciwrJktOuspQ6bIqJUcVvQWFpxc/kW+ooaCtw45puvdya3xLE+9zAFo2oswrRZzJQjY4ryQrc8rLKi8l1vm/CKD4HEDTzJtGqUyUMqtFIUqmCxUSqDKlGqL6jwBE5jRc9U6bGr72dp8G9DbcumHld36whQsTrJjRm3qEg6v+GC/MMc4SGIYERM7uV4LxTOQymZjL+xVjKPshiOCHNWtyo/FbOGmp9q63Ydc3R+MYw/cNabIx3NB2+Yxrs4zhrKbwbg0H17Y1lmZwcIlbyPh+BZyvLAgm+CyGJ++H8ShNsQ4K/9S3/WKb9G6RTnhie42t0XjYobGJyCGLgfOPPV+hc+GBa7qd0vhL32qOPG9KXTQynHheqzyTRDJTeVlqpv6/vjlP4NQZ0m0MpG0CHY6eXCjq0NFi1aI7pJdhKif4GlM2r4q5kFzkKczhPDlJTpMjuPylCb3tNHDyrkngeldVdUOmbbcxfCHzHLgxZL8L22w2yYgNOqP6fjn2u6aTuu/S1iiHbpuuDW1SOBCMFbP4Y2cX4ROeeeqit4efWhx3Ow==', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='12. Guo, H., Qin, X., Liu, J., Han, J., Liu, J., Ding, E.: Eaten: Entity-aware at- tention for single shot visual text extraction. In: 2019 International Confer- ence on Document Analysis and Recognition (ICDAR). pp. 254–259 (2019). https://doi.org/10.1109/ICDAR.2019.00049 4, 8, 10, 22, 24, 25, 26', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 16, 'orig_elements': 'eJxFUcFu2zAM/RVCpw5wZFmOHTu3oinWDL2spwFdEcgW5QqwJcGWtwVF/32U120HQtR7j4+U+PzGcMQJXbxYzY7AhGlErera1IeDVk2DqtYVVqJH05VG9ywDNmFUWkVF+jdm7IhOTZiKtXdrvAQVcOZBm6RNdLyGjVYhjLZX0XqXf9CjcsOqBlyIf2boBvZCaCDk4tapw5nwot6g+f+QxlTGdF0pD03Zy1oehK67tlRt2XUGsWfvVBHxV0ziQnL4vPoMHngGX63L4Bslj3bN4AslD8r9Sf4hJ+uGDO75Ee5VREeHizZed+onzQAq7oDQ9AgwfoaF1CPC8uoj/LDLqkZInYFiVn2ScTiTiRRFS0nE2W0/QLo77wzOO0DXI5DdyfdrWgXcEntd7ALKaXjC3g/Obg1vznen26dPHELgIKv995VsS1m1cJP8iXiNMSzHPNfecj8PeSF4UYg23wp5EnEhxL6FfQZNBoXIQEoKusqKok5b+buwR7vEc8SJvb/8Bji8pzo=', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='13. Gupta, A., Vedaldi, A., Zisserman, A.: Synthetic data for text localisation in nat- ural images. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (June 2016) 6, 13\\n\\n14. Hammami, M., H´eroux, P., Adam, S., d’Andecy, V.P.: One-shot field spotting on colored forms using subgraph isomorphism. In: 2015 13th International Con- ference on Document Analysis and Recognition (ICDAR). pp. 586–590 (2015). https://doi.org/10.1109/ICDAR.2015.7333829 3', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 16, 'orig_elements': 'eJzVk9tq3DAQhl9l8FUCG69t7XrtvVs2odnS0iWBXDQJQZZGXoF1wJIhS+i7d+S09AB9gN55/n9mNJpPfnzLcECDNr5omW0hk7JGrtoNY5u6xYJJtqq6zYqzjVBN2zXZAjKDkUseOeW/ZUoPaLnBudjZKb547nHMvVQpN9nx7Gebez9owaN2dvnDHrjtJ95jIP8xQ9tnz6R6Ul7sZDocSS/rWRp/DanUWqmuY9WmYaKqq00h665lvGVdpxBF9o0qIr7GlFyyHD5MPvIF7PIFPKDkg9TvwVcdAo6G2xRu4f5s4wmjFpCuB8qNkLrA4AQfdJgnB23B8ngF08gH0CYNn8PBbuE4OoEote0DOAXUCQ43Nzewd1YhTS8QqHzvjJ8ijvCgQ2rHrYQjj6RYuEPheqvnYy72D8e7S7j4OFmEqijrS6A9lCxt7edCP+kQDxFNuu/fIEVTrVYV8kIQRsGEYLJt1k0tOwJZFvV/CHKVwy03hhui95no3T5NRdGtcHTT6wKOpOwkNwu4py/5NNHS2p2VKM5EPT8S3y8Wr8LJRVAaBwnBuxgJV8Ii3OBGlIm5CTCFJIep60fuT6CDM270Jx3MO2pqvSYW8URRIjevgp4Dob6C32BfOzElJrAj9xx0mHH/gfmwv97dXebgfQ6EZx6brdsCLtIhZJxi9GG7XEqnczf2y7LIy7Jol3NhnpJy+ltZU7Xwj8fx/B09CjlK', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='15. Harley, A.W., Ufkes, A., Derpanis, K.G.: Evaluation of deep convolutional nets for document image classification and retrieval. In: 2015 13th International Con- ference on Document Analysis and Recognition (ICDAR). pp. 991–995 (2015). https://doi.org/10.1109/ICDAR.2015.7333910 4, 14', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 16, 'orig_elements': 'eJxFUU1v3CAQ/SsjTqnkYLOE9Xpvq2zVrppTpKqHNFphGBxUGxDGUVZR/nvA2qjHeV/M8J7eCY44oUtnq8keiDANV43aMdG0W9N1rBHMNDuhRdvxjJAKyIRJaplk1r8TY0d0csJi1t4t6RxkwEiDXrWFTpew0jKE0SqZrHf1lR6lGxY54Jz5J4JuIM8ZDRk5u2XqMWacbVco/l/SGGFM3/NNu+Nqs920jd72HZcd73uDqMhHdiR8S0XMBIWfMo54qeBA/9AKfpt/OJehgiPGIJ3N0y/6g+7h+6scl3VD8AY0YgDl3asfl4LJERymGYyPoL1ayr+BnfK2oEY5z9ZczwPpNERM0WIOpHBye9g0TADj6SVPCaOT18R7727BYD5PIWTr8Sv4kNnLbOc17BGVH5xdw29O98fD4zcKIVDIFf1dcjbvOgE35ZFMvKQU5n1da2+pj0PNGspY09WrkRYRbTnnuV24q4DdlSq+WnqwczolnMjH8ydLmaiD', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='16. Harley, A.W., Ufkes, A., Derpanis, K.G.: Evaluation of deep convolutional nets for document image classification and retrieval. In: 2015 13th International Con- ference on Document Analysis and Recognition (ICDAR). pp. 991–995 (2015). https://doi.org/10.1109/ICDAR.2015.7333910 6', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 16, 'orig_elements': 'eJxFUU1r3DAQ/SuDTilsZcuS7fXelmxIl/YUKD2kYZGtkSNqS0KWQ5eQ/17JbOhx3pfeaJ7fCU44o40Xo8gBSFvvNReoRC+Y4Eqodt9WWghRdkIPNZIdkBmjVDLKpH8n2kxo5YzZrJxd48VLj4F6pbM20/HqN1p6P5lBRuNscaMnacdVjrgk/pmgHclLQn1CLnadewwJZ80Ghf8lta617ntetXs+VE3VlqrpOy473vcacSAfyRHxb8xi1lD4JsOE1x0c6S+6g5/6Dy552MEJg5fWpOk7faQHeHiT07o1BKdBIXoYnH1z05oxOYHFuIB2AZQb1vxvYObUFoZJLovRt/VAWgUBYzCYAimc7QGqktXAeHxNU8Rg5S3x3tmvoDGtNyAk6+kz+JjY62KWLewJBzdas4Xfne9Px6cvFLyn0HXs95qyedfVcJcfScRrjH45FIVyhrowFqykjJVdsRlpFtGWc96xEpp8hs8L/TBLPEecycfLP9ZKqGc=', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='17. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). pp. 770–778 (2016). https://doi.org/10.1109/CVPR.2016.90 4', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 16, 'orig_elements': 'eJxFUMlqwzAQ/ZVBpxSMbNmJFeeaBpq2h5BAKF0Isj1yBLYsbBlaQv69o9Dl9uYtM495uzBssUPrT6ZmK2BpleblMpMoMp0Wc5knC5RKVFIURSaWgkXAOvSqVl6R/8K0adGqDkO47u3kT045HLirdfAG2X+5m6yca02lvOlt/CO3yjaTanAk/Y2hbdgHsY6Yk526EgfiRX6jhv+SWi+0LssslcuM+qYyqfOyyFSRlaVGrNiVEh4/fTALyeEBI3jiEbye6V4ELwT3aCM4EDhMBB75Cu4RHQw4mnpSLbSoBmtsA7ofwHTUiLSqb6wJ/Tls7QrSROSw3Ww2sO6tRmpYIfSWps5NHgc4mpHMoGwNO+WJsXT3bwnM1sfd/o6DcxykTN4nWphJuYRZ2EzC2Xs3ruK47g3vhyYWCRciKeKQ48HDiwTm4ZG/P342o9967Nj14xvN6JCe', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='18. Hong, T., Kim, D., Ji, M., Hwang, W., Nam, D., Park, S.: Bros: A pre-trained language model focusing on text and layout for better key information extrac- tion from documents. Proceedings of the AAAI Conference on Artificial Intelli- gence 36(10), 10767–10775 (Jun 2022). https://doi.org/10.1609/aaai.v36i10.21322, https://ojs.aaai.org/index.php/AAAI/article/view/21322 2, 3, 4, 7, 9, 10, 22, 27', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 16, 'orig_elements': 'eJxFkltv3CAQhf/KyE+JxNoGNna8b9v2IZteFKmV+pBGKwyDl8YGhHEuivLfA1bSigePz3dmOAJuXwoccUIbj0YVOygkpdtuy+tGaiHTarqmQ9kqSftOI2cFgWLCKJSIIvlfCm1GtGLC3KycXeLRC4+h9Epnb8bx2a9YeD8aKaJxtnrHo7DDIgacE78t0A7FXVJ9Uo52mXoMSafNKoX/IbW+0LrvOWsvuWQNa2vV9B0XHe97jSiL19QR8SlmM70s4crZgcCvksBXMxH4koprQ+B7+l49isx+p/KHeGc3ItwT+Fnu4FNw8w724ANuYhDGooKPzDA5hSNoJ5fZ2AGchbwnCJs9z26JiQXoMUYMcI/PYGwSpvUAIDmDkBtYf3RwE6g0KN/EXMJNcBJRpakzOA3xhLDf7w/w2VmN6SAk5t32IRptpBEjHGzEcTQbGFbImzNanxOgddu0fxZWU57K9gLOrhcLrGbsvIRTjH7eVZVypnRhqGhd0qbuKiGEKR94Y5LAKGeM/LO6v3O54uw3VuFT6U++ytkqkdLIEasHg4/V2gepkxPYEmgJdDkNgTyNtfnqP17FNzPHQ8SpeL17A9x+yiQ=', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='19. Huang, W., Qiao, Y., Tang, X.: Robust scene text detection with convolution neural network induced mser trees. In: Fleet, D., Pajdla, T., Schiele, B., Tuytelaars, T. (eds.) Computer Vision – ECCV 2014. pp. 497–511. Springer International Publishing, Cham (2014) 13', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 16, 'orig_elements': 'eJxFUV1v1DAQ/CsrP7VSZOLkmpB77BXESTwUikpRqU7+WN8ZHMfyB6Wq+t+xUxBPXs3seGZ3758JWpzRpYNRZAuEbWQrNy3Tg+iYZmKULcpRt8MghJz6njRAZkxc8cRL/zPRxqLjM1axWlxOB889BuqVrr2VTk9+pbn31kiezOLe/KUtd8fMjxgLf0/QHclDQX1BDi7PAkPB2bBC4X9IrS+0FqLvxre97IZubNUgpp5PvRAaUZKXokj4O60TTRQ+5OLTwFfawCfDlwa+lerLit3RLXxeRI4JokSHUHWgMKGsQeHRpBPIxf1abF4BhzlwW570uISfYJzKEhXMEQOkgBgp7N0W3lvE1MBVMbrmP5TlxbDUN/JkysYbuKwJ8lNCy3mIlYQzVJGew26ZfU7lt1sTq+H33LWsh3e73S2UakPBewqbaXwlLhijcOODccei2buidOuOS8jrLKyJJ1MH3Z34DGf1g3Ng6x3/3eWjiWmfcCYvD38AClqmFw==', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='20. Huang, Z., Chen, K., He, J., Bai, X., Karatzas, D., Lu, S., Jawahar, C.V.: Ic- dar2019 competition on scanned receipt ocr and information extraction. In: 2019 International Conference on Document Analysis and Recognition (ICDAR). pp. 1516–1520 (2019). https://doi.org/10.1109/ICDAR.2019.00244 3', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 16, 'orig_elements': 'eJxFUd1r3DAM/1eEnzrInO/kcm/d3UPT9qmDMdaVQ7HlnCFxjOOwdaX/++xsY2CQrN+HJPT8xmiimYy/aMmOwGpVtq2iwwGHtqyyGvEgMlWVoqoa2YqOJcBm8ijRY+C/MaUnMjhTFMvFbP5i0ZLjVqrIjbB/tTuM1k5aoNeLSf/CE5pxw5HWgD8zMiN7CVUbKhezzQO5UM+bveT+D6lUrdQwlEV7KEXRFG0mm6ErsSuHQREJ9h4Unn76SC4yDndb6JPAN57A6UomgYeQ3VEC9yF+Qp3A15A8oEP/C9cEzuH3uCXwOcR7/IFXdEHJv/Aj9OIjSHRFlncgltmS13EhCG8VaAxJcCRIWw+LcIBGgjZqcfO+N4ShHIqYcujNEXaf3nhyZifgBKfFKArbCoqm50Vs8T5wG7DXVa+75ROJZTR/Ot/0p/Pt0wcO1nLI67z5vgXXMq+LDG6if4Cu3tv1mKZy0XxxY5pnPM+zLt2lPJJ4lhVVBWW8yr+DPerV955m9v7yG+vAqV8=', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='21. Hwang, A., Frey, W.R., McKeown, K.: Towards augmenting lexical resources for slang and African American English. In: Proceedings of the 7th Workshop on NLP for Similar Languages, Varieties and Dialects. pp. 160–172. International Committee on Computational Linguistics (ICCL), Barcelona, Spain (Online) (Dec 2020), https://aclanthology.org/2020.vardial-1.15 10', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 16, 'orig_elements': 'eJxFUV1v2zAM/CuEn1LAU/xR20ne0rTDgmZbsQ7rQ1cEsk3ZwmRJkOSlQdH/PtpbsDfqjuIdj89vESocUIejbKMNRJmoVlWRNKUQeZ2VdZGk2bquy6QoVtfXiYhiiAYMvOWBU/9bJKRCzQecPrdGj+FouUXHbDv3TnQ425nm1irZ8CCNXv6jFdfdyDv0xD9HqLvohVBLyFGPQ42O8LScIfffpBCFEHWdZ9Uqb7Iyq5K2rNc5X+d1LRCb6J1+BHwN80Ypg08n0olhy2L46PAcwxP7RvXn5h7NScdwzzbw3Zy4az3wsZvykLoDha/kV4FDb0bXoAdhHPjJNHDdwlY44jVsB/xb3OlOSd8z2OsNPDjTILY0yIMREHqEKvTwZNwv3xsLRsOXw8M88lEOUnEHh0scMfzgTmKQpDkp3UqusAmegbWMEkl+jlmS5mmVTVoBnZ5jJa87MwwyBMRpPj3sGC7UgayM0gfZeFjsd7vDVQw3nPZSRMfwaLnUsPiqldR4BYtbbCBLsoS6+hCs3yyXvKHdQ2+U6c7MuG458ew35Ub+PqQsLSBNprteTn4guX3AIXp/+QMQOsfy', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='22. Hwang, W., Kim, S., Yim, J., Seo, M., Park, S., Park, S., Lee, J., Lee, B., Lee, H.: Post-ocr parsing: building simple and robust parser via bio tagging. In: Workshop on Document Intelligence at NeurIPS 2019 (2019) 1, 2, 4, 7, 9, 10, 14, 28, 29', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 16, 'orig_elements': 'eJxFkctOwzAQRX9l5BVIJjQJbZIuEQvKS5W6QKhClR/jYJHYluPwUNV/Z9KCWIx9PPeOPRpv9ww77NGlndVsCayalWWlalHO61znShqDFe2qRqXnKAXjwHpMQoskyL9nxnboRI9TsfZuTLsgAsYsaDN5Jzl9h6MsQuisEsl6d/krd8K1o2hxIH3L0LXslbKBMjs39hIj5fPFMRX/mzRmboyUZVHVpSoWRTXTC9mUoimlNIiKHagi4VeazEWRwe0nvcPhOeNwb3sOG4KXCe4INug5PBKsRXw/af/0gHhyHeH6D26zJaz9kC68ikC9Dda1S5Cj7TQRDLYPHYJwGqKX45COHozwYQVI6yGJtiVjBiu3hGcf34c3H8A7uPFqnL6DhIRdZ1t0ii5K8IRjXK03UMzyBs6m9RxyDgWHKw4Vh4ZDPqOgU1FTNNN4/yb/YIe0Stizw+sP4NqWfg==', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='23. Hwang, W., Lee, H., Yim, J., Kim, G., Seo, M.: Cost-effective end-to-end infor- mation extraction for semi-structured document images. In: Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. pp. 3375– 3383. Association for Computational Linguistics, Online and Punta Cana, Do- minican Republic (Nov 2021). https://doi.org/10.18653/v1/2021.emnlp-main.271, https://aclanthology.org/2021.emnlp-main.271 2, 4, 7, 9, 10, 11, 14, 27', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 17, 'orig_elements': 'eJxtUk1v1DAQ/SujnEBynDjZ3ezuDS2IFrYfggNCpVo59iRrKf5Q7BSqqv+dSSjlwiHK85uZN/PGvnvKcECLLp2MzvaQiXqjV6Ktds0WRaPbdiWwUVo0WK93td5lDDKLSWqZJOU/ZZ0Z0EmLc7H2bkqnIAOOPOhuzp3D6TEsYRnCYJRMxrviJTxI10+yx0jxuwxdn90TG4g5ucm2OBIvmoUa/w2pNisaZiNFuW31psFKlHKLsty1ldo1qLJnqkj4K83JVc3h4if1YfCNMzgiMrgg8N1YBp8IfJ7BRwJf0TO44ns4+Jhy7DpUyTwgoNN58jn9wLjOjznYxQRQh1GqBRINEa3JYxonlaYRNWivpnm1YOxskcOl28Pt6BWiNq6P4DtIZ4SqrAT1dB2SR4VAch9sMCPtaoArTGevI3WGa0m6RB1flvZHK0bS4hACh7pu1j+mqhQ1wS35fhejV0a+TnjwNkxpOc86VDiZmIyKDG7cYByCJJO3k0sSDtJJBu89uTWORnHwBcPU0g3Cm2v/sEz9lsM5pRD3RaG94X7sC1Fysd2s6+JBFHMKR+uGkFtpHK8awV4LpKLLJ2+D7x+Xyv9kQ8VgxYAewI6BKOkjAUFM1cyv5+/DOpKJy4Q2e77/DahX4xw=', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='J., Park, S., Yang, S., Seo, M.: Spatial depen- 24. Hwang, W., Yim, In: semi-structured document dency parsing for Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. pp. 330–343. Association for Computational Linguistics, Online (Aug 2021). https://doi.org/10.18653/v1/2021.findings-acl.28, https://aclanthology. org/2021.findings-acl.28 2, 4, 9, 10', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 17, 'orig_elements': 'eJyNUkuP0zAQ/isjn0BynYfb5nGrKiG6KrBSDyu0rCrXnqQWiW3FNlCt9r/jhAUuHLjNfI+Zz4/HZ4IDjmjCWSvSAkEpeNXwjVK4VQ1vVM1VJRXWKq+wxJxQICMGoUQQSf9MOj2gESPOZmVNDGcnHE7MqW7WznS4uYUWzg1aiqCtyV7pQZg+ih594h8Jmp48JdQl5GzieMEp4UW1QNPfkHK73jR8K4q8vqhtylXkokaRN5dSNhVK8pIcAX+EWXzHKNyL6SuFU6o+p4W/qhNaCh9YCyeXIokBFDo0KyjXDN5/X2QPs0GPFA6mBY+jXvkwRRnihAqUlXG+uOQz8gYpoNemh85O8E4blWoPtoNwRdh5b6VeDr7wezu6GJY+7T0madQ+aOlb2O2Pq8Pd/uPxHsq8LBg4x4Dz/Ess84LzNWf/P43CJzNog/BmF/tl3FsG1xCcb7NMWc3s1GdFzop6u+HZtyJbNnav4VdCDqys6R9H6oUJVzvY/sZg9v5LDyWFNYWGQrF8lt+Pf0yZDgFH8vL0EyjOxiY=', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='J., Park, S., Yang, S., Seo, M.: Spatial depen- 25. Hwang, W., Yim, In: semi-structured document dency parsing for Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. pp. 330–343. Association for Computational Linguistics, Online (Aug 2021). https://doi.org/10.18653/v1/2021.findings-acl.28, https://aclanthology. org/2021.findings-acl.28 3, 10, 14, 27', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 17, 'orig_elements': 'eJyNUUuL2zAQ/iuDTy04siw5ft1CoDRL2i7ksJTtEhRp7IjakrDktmHZ/17Z3baXHnoQzHyPmU/S43OCA45owlmrpIWk7BommSgQa1HmsmENrRTb0qK8FHxLeZJCMmIQSgQR9c9Jpwc0YsTFrKyZw9kJhxNxqlu0Cx1ubqWFc4OWImhrsld6EKafRY8+8o8Jmj55iqiLyNnM4wWniOfVCk1/Q8qy2Da8FDmtL6qskOVU1Choc2GyqVAmL9ER8EdYxHckhXsxfU3hFKvPceGv6oQ2hQ+khZOLkcQACh2aDbAtgfffV9nDYtBjCgfTgsdRb3yYZhnmCRUoK+fl4aLPyBvEgF6bHjo7wTttVKw92A7CFWHnvZV6vfjK7+3o5rD2ce8xSmftg5a+hd3+uDnc7T8e74FRlhNwjgDn9MvMaM55wcn/T0vhkxm0QXizm/t13FsC1xCcb7NMWU3s1Gc5JXldbnn2Lc/Wjd1r+I2QA2F1+scRe2HC1Q62vxFYvP/SA08hp/EUKbBq+ePf33+MqQ4Bx+Tl6SfjlcVT', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='26. Jaderberg, M., Simonyan, K., Vedaldi, A., Zisserman, A.: Synthetic data and ar- tificial neural networks for natural scene text recognition. In: Workshop on Deep Learning, NIPS (2014) 13\\n\\n27. Kang, L., Kumar, J., Ye, P., Li, Y., Doermann, D.S.: Convolutional neural networks for document image classification. 2014 22nd International Conference on Pattern Recognition pp. 3168–3172 (2014) 1, 4, 14', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 17, 'orig_elements': 'eJzVkd2O1DAMhV/F6hVIpbRppz9zt2JuZmdAI0YCLctq5CZuN6JNqjQFRqt9d5xhYSUED8CVEx/bOvZ3+xDRQCMZf9IqWkOU5k2qZCVVgXXbpE0n8qxOV3XW5J0UWRrFEI3kUaFHrn+IOj2QwZFCs7Jm8acJJ3LJpLpQG2R/ni4yTtOgJXptzesneUDTL9jTzPptRKaP7jg7ceZklrElx/msuqTcs0lZFqsmLzFL61aVFbEvrAnTphWyqUhGj9zh6bsPxaJM4BoVOZ7Wx/A2ieGoR2vOaGLY8e8DKRyUjuGKP5/0PJMbg3aVrOF4Nv6evJYQFgY0CtC9Aq87LTUOYGhxl+C/Wfdlhs46MOgvyVmSIQg2wJG0vdFh8wS2Zg0fQ/W9ncAa2BBNsCd0Rhs2+G57OMILkWbFS8jycKRf99vr2W89jWG9P7mVnVBlUWFLalVXdVGKQq6EwrYuW9l2xX/IrUpgh+Eie+ayW0Z0MVzz84ZiOHDcM7Ibjht7AcbENsmRmb2x5qsdlmD3H4SUlUu4HeiRHYMccJ4DUfwJKNwehGDWW+PJGXwaxYM74nUkBWwH9EGE989sYZoSyLOy/rzwjDzPKvGbZAxFDFnxd553PwDVUxsY', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='28. Karatzas, D., Gomez-Bigorda, L., Nicolaou, A., Ghosh, S., Bagdanov, A., Iwamura, M., Matas, J., Neumann, L., Chandrasekhar, V.R., Lu, S., Shafait, F., Uchida, S., Valveny, E.: Icdar 2015 competition on robust reading. In: 2015 13th Interna- tional Conference on Document Analysis and Recognition (ICDAR). pp. 1156–1160 (2015). https://doi.org/10.1109/ICDAR.2015.7333942 13', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 17, 'orig_elements': 'eJxFUV9vmzAQ/yonnjqJGowJhLylyTZlS/eQan3pqugwF7AGNjKmW1r1u89ONk3yg+/37053T28R9TSQdkfVRCuI8lxUQvAsL2uZL8pCLKtc5EUqqFzUdSmjGKKBHDbo0OvfopPqSeNAwdwYPbvjiCNZNjanoA20O48XGsexVxKdMjr5S/eo2xlbmjz/FJFuo2ePjh456nmoyXqclxfI/h9SFvmiEgXydFk3RUkZT3FJmFZ1JquSZPTuHY5+uyDOlgy+okX3ilMMWxbDZzPQ6+2dao1tMIa9h74paXo0cwzrIOjM1MXw4L932DaozcuV2P3CYbbec++Le78Cn/gl2GkeUOtr1qZD3Vic6GeHNoZHdvDgfr7mPXR4QuVi+OSL77JTYYJAPGL/Qvocw0e2gp1s0EKW8gVIM4zkVFga+GdNPU8OLGGjdMtgp1dXHReu85Ujq/EWghx72Bh9Ir84ScG7NXIOp4a1586TmsAPCgeSptXXBje7zXZ9+MBgHBlwvih+zD5ccF6kcBPaeKpzbpxWSdIYxYxtE54yztMquVhZELFSCFHlmZ8p3Pjf+fdqcjtHQ/T+/Af9jr3G', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='29. Kim, W., Son, B., Kim, I.: Vilt: Vision-and-language transformer without con- volution or region supervision. In: Meila, M., Zhang, T. (eds.) Proceedings of the 38th International Conference on Machine Learning. Proceedings of Ma- chine Learning Research, vol. 139, pp. 5583–5594. PMLR (18–24 Jul 2021), http://proceedings.mlr.press/v139/kim21k.html 3', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 17, 'orig_elements': 'eJxdUV1r3DAQ/CuLnhLwybZ8/rrH9unaOyhpaaBpOGR5bYvIkpDka0vIf6/sNvTjRSwzs7Oz2odnggpn1OEie3IAItq2afKsHKqhb5lAwbq6baqBsSpjRd2RBMiMgfc88Kh/JoNUqPmMa3Nv9BIullt01PbDql3p8MNuNLdWScGDNDr9TSuux4WP6CP/QFCP5DGiNiIXvcwduojn9Qa5v0JW+7ItKp5nTddXNbI84w3yrO2YaGsU5CV2BPweVjFrKbyXcwL3NIGPRifwJhYbcqQH+CxVWF8fU+247nevkSA4rv1g3IwOvskwmSWAiCK4GrWsS4Bx4HBcK7/Ena+bCYWjPsAZpeIJnOOoL1N0TOAThRvsPb2FD84IxF7q0YMZIEwIRROm2BfQ6e1/uIK3Rg8YlxYIccCZi0lqhBNyp2Mn/d/lzHfwrwTu0MdSTMmamEJetAlYS6Esm+LrwrK8KMt2H53Opzu4yZtfGNvDu0UBy1h+m8AUgj2kqf0zjM4qXteh9+k1WqZPcmb5E53CrKBYb/p67pP04RhwJi+PPwEq88LT', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='30. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. In: Bengio, Y., LeCun, Y. (eds.) 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings (2015), http://arxiv.org/abs/1412.6980 8, 25', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 17, 'orig_elements': 'eJxNUUtP3DAQ/isjn0BKs3kQJ9kbLJdttxKC9oAQWk3sSdZqYluOt4Ii/nsnEVW5WKPv5W/spzdBI01k49FosQVRSeop76UqZN3KVjayI1nKsqrypq6aTCQgJoqoMSLr30RvRrI40WLWzp7j0aOnkHrdL9qFjq9+pdH70SiMxtnNBz2iHc440Mz8kyA7iGdGPSNHe546Cozn9QqF/yWVvKraUmKeNZ2WNRV5hg1h1naFamtS4p0dkV7iIi6zFL4ZO0yYwG16lyZww9PXdAvXGic+gfc5OQ29CzBHp044R6PA+Wgm82etm8LebuGG+xmXwCNnHGh3tssIF6Tn9BLKoFkUKdjVgSPsnO2JWysCZ9mAwXINuCcfaOZlVt2cwH53uIciy6sEHtDCraGBL9ldJ/DzgY/v+Ar1lzb5kHxK/RFQ/YK74BSR5ugZLhbNZQKnGP12s8HwYn6nLgwb7OZNfpUXqWybDBoOq5bn//czBzPHfaRJvD//BepXopM=', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='31. Klaiman, S., Lehne, M.: Docreader: Bounding-box free training of a document information extraction model. In: Document Analysis and Recognition – IC- DAR 2021: 16th International Conference, Lausanne, Switzerland, September 5–10, 2021, Proceedings, Part I. p. 451–465. Springer-Verlag, Berlin, Heidel- berg (2021). https://doi.org/10.1007/978-3-030-86549-8 29, https://doi.org/10. 1007/978-3-030-86549-8_29 4', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 17, 'orig_elements': 'eJxtkk1r3DAQhv/K4FMLtlbyt/eWj0OWbKFkoZc0LGNp7BXYkpFlmjTkv1f2tvSSk613nnk176Dn94gGGsn4s1bRHqIu71RXU5mpWrVFJ2rCWgqSHa9kW/A2iiEayaNCj4F/jzo9kMGR1mZlzeLPE07k2KS6lV3L/m3ayjhNg5botTW7v+UBTb9gT3OoP0dk+uglqFNQzmYZW3JBF9Umuf9DyjIvmqxEwetWlRWlgmMYlDdtKpuKZPQROjy9+hXOBIPHAfWIJoYTi+FIF0MxfGN7uLfSESpye7i1i1Ha9ElrX6FzROAdahMUsB0gKCuXdU+gTWfduKWAcIVDuf2OVtHA4GA21yt6Y3B4m/UMaBQ8kbS90Rv8c0m5yOBwl8D9zROkPBUhZ+kvod+TM5s7DnBnTUchuAzzHnGZ0ayTn35p/5tc2J0KB5o8rZuC4uoqeLwZxvDdWUm0hprDAZ2HA4OJQV6IK5qXBYPT5AJBLvmxWvYx3IavDrt6IB0iJRC8e/iyWn5lcPF+mve7nbKaWdfvBGeC82rXVHWSJTzjSV0WeZPUkDbxZzR8jp/TBvL1Rfx7LEc9+0NIFn28/AEi0dHN', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='32. Lewis, D., Agam, G., Argamon, S., Frieder, O., Grossman, D., Heard, J.: Building a test collection for complex document information processing. In: Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Develop- ment in Information Retrieval. p. 665–666. SIGIR ’06, Association for Computing Machinery, New York, NY, USA (2006). https://doi.org/10.1145/1148170.1148307, https://doi.org/10.1145/1148170.1148307 4, 5', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 17, 'orig_elements': 'eJyNUl2PmzAQ/CsrnlqJEiDEhLylOTVNdddWifpwup6ijVmIVbAt29yHTvffu9C099oXtDM7s/YsvnuJqKOedDiqOlpBVOGpKEoSaVoJeSqLRZNVRZlhI7CZyyVGMUQ9BawxIOtfokZ1pLGn0VwbPYSjRUsusXUzasd2eLZTG63tlMSgjJ5d2h3qdsCWPPfvItJtdM+sZeaoh/5EjvmsnCj3dkkpikU1F5ily1MtSsqzFJeEaXXKZVWSjF7ZEegpjOJ5nsA1PSofw1USw7rFPobtWDkujY7hwOCTU1STi+Ebg60z3veo/zg+E7o6hi/JCj4OqquVbgEhkA8gTdeRHANBYxzD3nb0BLWRw7hTUJrpfkoM1hlJ3rM7gZ1ewfcR0zjNg2kgnAnyKpxhrfWAHUsCOT1ZGa03N3DYbXd72BjdEO9CEvDQPXm+nTwD6hqu6IE6Yz/A5Wye8Xb8ngJHfMAuAZuAEIufQ55mcyFEcpk8EVUqeDPeG6nwX64N5xrCmPsG5Vlpcs8xfKVHuDXuF1e3Mfw4rOFdnqbifQLnEKxfzWa1UYlx7SxLkywrFjP+LLNyAst5yn/1P4VQxLAYX8vfh3StfNgF6qPX+9+kJ9sl', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='33. Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., Stoy- anov, V., Zettlemoyer, L.: BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In: Proceed- ings of the 58th Annual Meeting of the Association for Computational Lin- guistics. pp. 7871–7880. Association for Computational Linguistics, Online (Jul 2020). https://doi.org/10.18653/v1/2020.acl-main.703, https://aclanthology. org/2020.acl-main.703 5', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 18, 'orig_elements': 'eJyFUl2PlEAQ/CsdnjSB4WtZYN9WTcyZPTVqTPS8bGaZBsZADzLDKl7uv9vgrj744Ft3VXV1T2XuHjzssEdyR628HXh1lJRJWkSZkvmmzuoykpms6w3KMj/FtfJ88Hp0UkknWf/g1bpDkj0uw8rQ5I6DHHAUg6oX7UK7eVhpOQydrqTThsIL3UlqJtmgZf7OQ2q8e0YHRo409SccGY+LFRr/HlmWqizjvE62uVJpulFJWaUyq7I8P+Xbeus98oTDH24Rp6mAA37X1odb4cNBTz584uKlmWXnw+ulbOXPsyZN+FWq37Jb0/KjuNkvM3iefXjD1Xtn5gAkmbMPH7n/jM5xfmbGkWViB8/27z7s4AWS0VZTAxa/TUgVBs4E1xqGkftRLhsbqM0IJN00yg6ucUCDhOOalA+sJNtdGkkKKtOzQ4tkGRJwQzt4O5oKUQXAjhZMDa5FyArXwp5oYudbRLdsu1B7a02lV8/1gOdsObm1Z/FBUwDNpK3TlRUwDALyIo+/TEkUp3lRROL/Btdxzo06ThaevJo6SKIkeiqgdW6wuzBURgszNmEcibjYZml4jsNFImTVBT0HJPIo9f/IGZXkWtOZZhawDP4jhmz5Vtcfd+Abbhz23uP9L1H48GQ=', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='34. Li, C., Bi, B., Yan, M., Wang, W., Huang, S., Huang, F., Si, L.: Struc- turalLM: Structural pre-training for form understanding. In: Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Vol- ume 1: Long Papers). pp. 6309–6318. Association for Computational Linguis- tics, Online (Aug 2021). https://doi.org/10.18653/v1/2021.acl-long.493, https: //aclanthology.org/2021.acl-long.493 14', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 18, 'orig_elements': 'eJyNUlFr2zAQ/iuHnzpwZMuO7ThvaWEsw9kKgY7RlaBYZ0dgS0KWxkrpf9/ZTdlgL3uwdPfd9313nPX4EuGAI2p/UjLaQlRXVVGkuahRbM6YZqKWVZfLKquKDWa4jmKIRvRCCi+I/xJ1akAtRpzF0ujgT1ZYdMzKbubOZf9sl7KwdlCt8Mro5FoehO6D6HGi+mOEuo+eCLWEnHQYz+gI55sFcn8NWcu65lWXlZWUeb6WWd3momiLqjpXZVdGr6Tw+MvP5HzNoFEx3LEYbum+pfu70DEcKPhG/emk6FNYwuOf8COFRxI0bAtH70K7Ah+cGJrDNV8ysA5X3gmlle6hM27+Rghaopu80JJgBnu9hXtnWsQ5n8B04C8IRe0vsNM6kM8B0c8W19JumkyrlmUtrndmtMEvOZEbYgY1edVOQD0WBedkttcenX6nfTZKe5LqDml9LQKZfRFvczfX1b/NNU1z75sHM6wgjGS2hcYQcj//zOkDA2sZlHla/whZyvMy5xv2fzPS1mjKGL7qQWmEm13oIUszTp4X7+20TRJpFDOuT3jK+KYs8uQnT2YKE+2wGmgMtq7z+EqHJCFYaH8xg+mfF+E/ZODLS31/eQ2tau9xjF6ffgPSfPCK', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='35. Li, P., Gu, J., Kuen, J., Morariu, V.I., Zhao, H., Jain, R., Manjunatha, V., Liu, H.: Selfdoc: Self-supervised document representation learning. In: 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 5648–5656 (2021). https://doi.org/10.1109/CVPR46437.2021.00560 14', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 18, 'orig_elements': 'eJxFkd2K2zAQhV9l8NUuuLLkH9nObUjbbLcQUshFd5cwscaOii0LWyoty757pWxLrzQ65zuDRvP0mtBIExl31irZQEKybgqsCyU7wfuLkKpDagvOm74Ul4YnKSQTOVToMPCvSa9HMjhRDKvZeHe2aGlhVvWRjbb7bW82WjvqDp2eTfbXHtEMHgdag/+UkBmSl6DaoJyNny60BF00N2n5/8i2VW0r6j6XtVJFUaq87QqsuqquL7XsZfIWEo5+uQgXFYNHncKBpfDJp/AQzi+ezHv1dV5w0UE+sX24fr/inMLnUD2gDsgxImh+eIPuipFKQzMfkQ18o7FXc/defFh9GPunXklBEH38U1jILrSG6jY0jISL0WZgsDcbyHkuYL/b7bLt6SNsZ9NTmLEjCOR2nqx3tMBJrzGJRsEBXVAMHKmbB6NvHe+2p8PxnoG1DCpZNs8+56KoZCXhLvYP1tU5u26yTM2azcuQCc6E4G0Wk6Usi5pFkHFeSQ6ijFv5t7BHvbq9oyl5e/kD1Y+nBw==', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='36. Liao, M., Shi, B., Bai, X., Wang, X., Liu, W.: Textboxes: A fast text detector with a single deep neural network. Proceedings of the AAAI Conference on Artificial Intelligence 31(1) (Feb 2017). https://doi.org/10.1609/aaai.v31i1.11196, https: //ojs.aaai.org/index.php/AAAI/article/view/11196 13', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 18, 'orig_elements': 'eJxFUdtq3DAQ/ZVBTwkYyVpl7XjfnEBhYQuFFloIYRlbI1upLQlbzm4J+ffKTkqfdHQuoyPm6Y3RQCO5eLaaHYAZtSv1vbmTJleVLAwp3aCSRVPmhO2+YRmwkSJqjJj8b8zYgRyOtIa1d0s8Bww08aDN6l3l+CdsMoYw2Baj9U58ygO6bsGO5qQ/MXIde05sSMzZLWNDU+Ll/UZN/0tWla4qWZpdUWqt1J3eVa3Cfbsvy6YsTMHeUyLSNa5mVXA4WfQZfOUZfO9tBg8JPGACvxL4mSp8oJNd0pUf4EeKNv5K8wFqMDhHWIeBpkht9BNcbOwBYbauGyjRFMDRMuGQjnjx028O3ybfEunkmMEbiD1BXddHePTOUPpKS+Ad1FO0xrY2JY8u0jDYbpOUvJG3cPOFGtjlsrzl0McY5oMQ2lvup07InMsirwQiWv6qpJVcSlkV2acThPAvM9/k1W+dpisPfRBrDYHp4XYg8WrpIrYgSLUu5N+uTnaOx0gje3/+C3Saqpg=', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='37. Liu, W., Chen, C., Wong, K.Y.K., Su, Z., Han, J.: Star-net: A spatial attention residue network for scene text recognition. In: Richard C. Wilson, E.R.H., Smith, W.A.P. (eds.) Proceedings of the British Machine Vision Conference (BMVC). pp. 43.1–43.13. BMVA Press (September 2016). https://doi.org/10.5244/C.30.43, https://dx.doi.org/10.5244/C.30.43 13', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 18, 'orig_elements': 'eJx1UdFq4zAQ/JXFTynkFNuy49pvaThory2UBhp6vRIUaW2LcyQhyVyP0n/vqtzRp75Iq5nZnUH79JrhhCc08aBV1kEmlZC1KvO+Pxd50bTHeo11Icum4rzEPs+WkJ0wCiWiIP1r1usJjThhalbWzPHghEPPnOqTNtHxr/ughXOTliJqa1b/6EmYYRYDBuKfMjRD9kyoI+Rg5tMRPeHF+QfkP0O2rWrbounLdaMU55UqW8lFLeumOTbrfp29UUfEl5jEvGFwo+cl7NkStiMaOqnaWzMs4Zo9smt67Yj/SfelIPoH62AXhf9mMHawgeAos5hAxEgZKD14DFrNCCT4Y/1v6K2HINEgJFeipR2MTlIGV6aDey1H4RUZw15PwZLJd3bPLpPzSccxhduwOwYLVIGdwZ23ElFpMwSwPcQR4cLTvDDCrZCjJqMHHVKSrTU90tdIhMXF7cP2jIFzDCrOil9zmRc8VZwBcRsaiyHAYocuYvpcIMGaOsYYXehWK2U1s35YFTmry6pabRnPWcWXn4IX9oUGCp72+X/VNzrEKzLJ3p7fAfBWucY=', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='38. Liu, Y., Gu, J., Goyal, N., Li, X., Edunov, S., Ghazvininejad, M., Lewis, M., Zettlemoyer, L.: Multilingual denoising pre-training for neural machine translation. Transactions of the Association for Computational Linguistics 8, 726–742 (2020), https://aclanthology.org/2020.tacl-1.47 5', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 18, 'orig_elements': 'eJxFUU1v2zAM/SuEThvgOv5Ioji3YRiKDuku22FbWwSyRdsaZEmw6G5Z0f8+ymvRE8n3Hp8o8u5JoMUJHZ2NFkcQpSxUiyXWRdHospB1f5Bt3dWyxa3c6kpkICYkpRUp1j+J3lh0asLUrL1b6BxUwDkPuk/aRNMlrLQKwZpOkfFu80Jb5YZFDRiZvxPoBvHAaGDk7JapxZnx8rBC89uQTaObppR9tZda1zWP1XS12nU7KVu57/fimTsI/1AS14ccTmbJ4EeewTXHzyn6i7IZfOH0ZDL4zvGTXpx/zOBrokf199E44/CX0hncJhn+NvF/+hOJeGn+gjPj+RFuF0vGmvQVCxqdN5ELCDNe0aySzwC9n8HhMrNiUt3I1sCci3bdRw7fUqG6VETwPdCI8CFG35lVsPZ/9FNYaK3Z5pQeNJFMF4FXJKv9/VIVZS23Fbyriqp4n8FIFOJxs1Edb5pGb/1wyf08bBKfE8NXZb6VsEvHeL3TiU1vCCfx/PAPWDysTw==', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='39. Liu, Y., Chen, H., Shen, C., He, T., Jin, L., Wang, L.: Abcnet: Real-time scene text spotting with adaptive bezier-curve network. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (June 2020) 2 40. Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin trans- former: Hierarchical vision transformer using shifted windows. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). pp. 10012– 10022', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 18, 'orig_elements': 'eJyFUstu2zAQ/JWFTjYgK3rYkuVbK7i1AwM14sJBkwYGRa4sohJJSFTcNsi/dymn7a09cTg7S+wM9/HFwwZbVPYkhbcCr2JRyTFN06paZmmZLxO+ELmI0iyeizgrPR+8Fi0TzDLSv3iVbFCxFl2z0GqwJ8MMdoERldO6sv1hxjIzppGcWanVzVu5Yeo8sDP2VH/0UJ29J2INMSc1tCV2xEfLker+DpnnIs+jrIrTTIgkocFynrAFX2RZmaVV6r1Sh8Xv1omTPICdHHz4EvhQ1Kh82BA6jKggtEEfPtN5K4nYEbinoRxawbuSK7QruEPWzKxsEXqOCsG9Db3R1kp1hou0NVAixspnhBJ/SuxmfOjoQt0X3X0LYKtWsO80RxTU0oOuwNYI2/V6fVMcP0ChVYVkkSNoRbfWDBY7OMqe0gKmBOyZJUbRLFyflXQpwqQ47u+mMLkdaKg4jMMpxDAP3ww/kJedMzU6Z/oKNsM1gHuUV+KhHv3+UR8IfBxI/Z4SOFykAtsx1c+g0l2L3Qo25I91vKa/bOD5OuEouQpg6F0qfS0ri4LSUUJf+v9HsFXO37ge9O6/A5lsi+I4DcCYAKIwjOKvQxxGicNxDJNP3GpaHhdJNIW5D7RCUewW7vcu7mRvtxZb7/XpF7UX8Gc=', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='(October 2021) 4, 8, 12', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 18, 'orig_elements': 'eJyFUstu2zAQ/JWFTjYgK3rYkuVbK7i1AwM14sJBkwYGRa4sohJJSFTcNsi/dymn7a09cTg7S+wM9/HFwwZbVPYkhbcCr2JRyTFN06paZmmZLxO+ELmI0iyeizgrPR+8Fi0TzDLSv3iVbFCxFl2z0GqwJ8MMdoERldO6sv1hxjIzppGcWanVzVu5Yeo8sDP2VH/0UJ29J2INMSc1tCV2xEfLker+DpnnIs+jrIrTTIgkocFynrAFX2RZmaVV6r1Sh8Xv1omTPICdHHz4EvhQ1Kh82BA6jKggtEEfPtN5K4nYEbinoRxawbuSK7QruEPWzKxsEXqOCsG9Db3R1kp1hou0NVAixspnhBJ/SuxmfOjoQt0X3X0LYKtWsO80RxTU0oOuwNYI2/V6fVMcP0ChVYVkkSNoRbfWDBY7OMqe0gKmBOyZJUbRLFyflXQpwqQ47u+mMLkdaKg4jMMpxDAP3ww/kJedMzU6Z/oKNsM1gHuUV+KhHv3+UR8IfBxI/Z4SOFykAtsx1c+g0l2L3Qo25I91vKa/bOD5OuEouQpg6F0qfS0ri4LSUUJf+v9HsFXO37ge9O6/A5lsi+I4DcCYAKIwjOKvQxxGicNxDJNP3GpaHhdJNIW5D7RCUewW7vcu7mRvtxZb7/XpF7UX8Gc=', 'is_continuation': True, 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='41. Long, S., Yao, C.: Unrealtext: Synthesizing realistic scene text images from the unreal world. arXiv preprint arXiv:2003.10608 (2020) 6', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 18, 'orig_elements': 'eJxFkE1LxDAURf/KIyuFUpt2+rl1JcxuEJRhGF6T1xpo0pCm6jjMfzepisucey65vOOV0USajD8ryTpgTdGjyGXeCt7XJS/bQfQ7KrjgZS1KrFgCTJNHiR6Df2WDmsigpliWs1n92aIll1o5RDfG/mK3GK2dlECvZvPwG09oxhVHWkJ+ZGRGdgrUBnI2q+7JBc6bDbn/kW0r25bXQ17VUhbFLs4tsBRlXfd1NVTsFhqePn2UdzyF/WzGBA5pAq84J/CYdvBsHOEUpQ4OF+PfaFFfyowQsVq8ErAIMgRRAaXjSBjcrCGosG5t+JjdJFNA96LewTqyThn/8+zyLCtSnlVZA3d5lmf3sB3v7xj78MeTJ81up2/UTno+', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='42. Majumder, B.P., Potti, N., Tata, S., Wendt, J.B., Zhao, Q., Najork, M.: Rep- resentation learning for information extraction from form-like documents. In: Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 6495–6504. Association for Computational Linguistics, Online (Jul 2020). https://doi.org/10.18653/v1/2020.acl-main.580, https://www.aclweb. org/anthology/2020.acl-main.580 1, 14', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 18, 'orig_elements': 'eJyNUktv1DAQ/iujnEDyOnE2z721nFp1y0KRkCjVyoknWbfxQ47DUlX979i7BQ5cuFjj7zEPe+5fEpxQofZ7KZINJB3jeSeGBoshbxjnWdN1bS16ltcZW9dFQiBR6Lngngf9SzLICTVXGM3C6MXvLbfoqBVD1EbaP9sTza2dZM+9NDp9oyeux4WPOAf+PkE9Jg8BtQHZ60V16ALOmhPk/jbZtqJtWT3kVS3Eel2IvO3XvOzLuu7qaqiS1+Dw+NNHcZFT2PLHRQl0BC7pjhLYGe8lgdsQfgmDELgL0VfUwhO4ppfh8u3ADYFPIbrlj8Y9EdjSDXxGuwKHc2jlNAZMyJ2WeoTBOJA6nOpMhOKO96dwcEZFXq0m+YQgTL/EB58pXOkN7JzpEUXIMYMZwB8QysYf4ELrhU+wRfQx/xt1Mc+ml+cSseQHo+xy7iWIb4JykbOXfUhuLYWqaMvvSx5+riqzgv6/ncBHPUmN8O56mSDP8uw9hYP3dt6kqTCSGjemLKOsqcp1+oOlUUJ5P60Ul5qWTUb+yI/HY2SO2FGINq79wUxmfP7XBIwAO+3Y7525Cf1ceVTJ68MvT9Havg==', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='43. Majumder, B.P., Potti, N., Tata, S., Wendt, J.B., Zhao, Q., Najork, M.: Repre- sentation learning for information extraction from form-like documents. In: Pro- ceedings of the 58th Annual Meeting of the Association for Computational Lin- guistics. pp. 6495–6504. Association for Computational Linguistics, Online (Jul\\n\\n2020). https://doi.org/10.18653/v1/2020.acl-main.580, https://aclanthology. org/2020.acl-main.580 3', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 18, 'orig_elements': 'eJzFUktP3DAQ/iujnFop603iOI+9QU8goLRFqlSK0Kw9zhoSO3IcVIT47zgL9EEPPfZgazzfN/7mdfmQUE8D2XBtVLKBRFatqlSeFaiaLRclxpMVIte52tZ5pZMUkoECKgwY+Q+JNj1ZHGgJVs7O4XrEkTwb1Z67wOF+3MM4jr2RGIyz6xe4R9vN2NEU8cuEbJdcRe8YPdd2Hrbkoz9v9i7/K8m2VW2b17qoaqU4L1XRSo5Cirre1pWukscYEehHWMglZ3CKN/OgyKdwyM5ZCucuBJPCWTQvYiEpfInWV7IqpHDMDuPj2w5dCp+idYY3zt+mcMo28JlGTyuYYir7MqAn9NbYDrTzYGy8h2cginuUe1N7Nyz4sOrNLYFycl4aPjE4shs4924FkkjFXyZwGsKOQDRhBwfWztjDKVFYFF6gg2ly0jyLLKIf3DDOz9lE8omxK+hmMwUjo8A4MqjKVnyfiyznlchK9u8PXsNT+Gh7YwneHc/9MqzXOZ5E/CjQsLT57f7wbcu5VlIIbDRvZUY6jomXsuSNLCv8H/vTvt2fZttoUrUsiTd5JeusFljInBpRtVq3+o/9KbIie89gF8I4bdZr5QxzvlvnGcubSvD1Xb5eKAxlvxrQWCaaLP1Jj160Yed6190zWAL/IgP/vbln6H0s8Y4uFv3Hqycj2iYj', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='44. Mathew, M., Karatzas, D., Jawahar, C.: Docvqa: A dataset for vqa on document images. In: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. pp. 2200–2209 (2021) 1, 8, 12, 14\\n\\n45. Park, S., Shin, S., Lee, B., Lee, J., Surh, J., Seo, M., Lee, H.: Cord: A consolidated receipt dataset for post-ocr parsing. In: Workshop on Document Intelligence at NeurIPS 2019 (2019) 2, 7, 10, 12, 13, 22, 24, 26', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 19, 'orig_elements': 'eJzVkttu2zAMhl+F8NUKeK5P8SF3XZJh6dohQID2oisCRaISobakyfK6rti7j8oBxYa9wC4I0P9P2iQ/P7xG2GGP2m+UiKYQ1ViXpZiISSErIZq8alqOvGmEKMsiK9sohqhHzwTzjOpfI6k61KzH0CyMHv3GMosusUKG2mD7F3uwmbWd4swroy9Pdsf0bmQ7HMh/iFDvokdSLSkbPfZbdKRn7UFyb0M220aiqHmJRZNVvE7rCct5hs2kaqVsZfSLOjz+8KG4LBO4ZX6PzzHcJjF8Zo75n2yIYU5P1+yZ7ZmLYZZMYW74929sClcQ1hvQgzQOSAKjQRg+hkOB6sPACSz1FFbOcESh9G4AI4G+AsvFYnE5u/sI90p7dDAzWiINzzG85ertBoeOmentGMru1EBaAtYmkOdp+nXM06ygrIV3eZpnF5DF0MSQ5RRluN35rDdq8EuPfdj6b5xYTbKs2LZCthXfbkVb1VIUtWhEKkUt/keckwRWzD3FsCZ8673Sx+wGMYYP5+Q6eKPbnzI0R/YH7xORnhknAmdOGEynaH0U4JCjsv4P+NYM/r3hlDA3EOYj9nvjnoa9sYHo/PxfLAl316ndATXz8AVHt1ytgTgeGGbtBRC8mvilJ45FTKwpSorq30wffwMulBvZ', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='46. Peng, D., Wang, X., Liu, Y., Zhang, J., Huang, M., Lai, S., Zhu, S., Li, J., Lin, D., Shen, C., Jin, L.: SPTS: Single-Point Text Spotting. CoRR abs/2112.07917 (2021), https://arxiv.org/abs/2112.07917 2\\n\\n47. Phan, T.Q., Shivakumara, P., Tian, S., Tan, C.L.: Recognizing text with per- spective distortion in natural scenes. In: Proceedings of the IEEE International Conference on Computer Vision (ICCV) (December 2013) 13', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 19, 'orig_elements': 'eJzVUV2L2zAQ/CuLnxLwOf5ILDuvvkBzpJAm4fpxHEGyV46oLRlbTq89+t+7cloKR/9AX8QwsztoZ55ePWywRW3PqvLW4MlKcJlKxlZMsEzGIguTJMM0qiImWVV5PngtWl5xy2n+1ZOqQc1bdMuV0aM9d7zDPugq6WadbL93k8y7rlElt8roxW+54boeeY0D6U8e6tp7JrYj5qzHVmBPfJRPVP/3k5nIJFasXGKSRWnJQrbicRlhtkpzKXPp/aQNiy/WDS/TAPbk7MN94MNH7tAnQjs1+vCZwJfLxD0QfDdO8L2TufLhOMnjDezUbWin9M3reEFCBaEHR+2CNRz3pyO9StcN3u2N0hZO9A84dsZaYgMozOEAXAyLOIriIGR5xGAWh3E09+FibTesFwvev6hrYPp68WYwdpn9iXOnBru12Lpr39a4Enm5yhIZx2KZiRgFspingoeCs4yx/D+skVGN1JQPp+DDFL668q9jy3vuw56Ik3KiK+rEp1pcHwcsTa3VD4oenBN8U/YCdNgdDB2WVl0RKsrR9O4cUBo0t2PPGxhK1DgEsNVr2PemRKzIZAAjwV4QtpvNhjSLvZ6SoI3CaIl0XolAVoVpu5FkeFSDs55ti+JxDrN7LNElAnEYJXOIkn9X+vwLPHsZ9g==', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='48 Powalski, R., Borchmann, L., Jurkiewicz, D., Dwojak, T., Pietruszka, M., Palka, G.: Going full-tilt boogie on document understanding with text-image-layout trans- former. In: Lladés, J., Lopresti, D., Uchida, S. (eds.) Document Analysis and Recognition — ICDAR 2021. pp. 732-747. Springer International Publishing, Cham (2021) 14', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 19, 'orig_elements': 'eJxFUV1v2zAM/CuEn1rA8WzHie28dQ1QZMiAIN2e2iKgLcrRIkuGPpClRf/7pKxF38i7I+5IPr0lJGkk5Q6CJStI+q6sO75s57whtuA5r6tmQUXbNNWyKbsiSSEZySFDh0H/lnAhSeFIcZhp5d1hwolMNjEetZF2l+lK4zRJ0aMTWn37oCWqweNANvBPCakheQnoFJCD8mNHJuBFe4XMV8imazixuq9o3hTLvs7rBZZ9Qc1i2XLe8uQ9TDj666K4amCnzyjtSaSwz1L4rk1/HFGpFLah/eHNSdBZ9K8prEO/Pus/eErhV6h3gpzx9vWEKfyMPcpYPmQreNBCDcC9lDMnpINO60EQaAVM9z5eFLxiZKxDxaL0LNwRYqiZGMN+M4kX7R04g8rOgGszhqvBRq1gK5E9+zyn1oZ4wXarJ0PWif8Bf/dHwUKKxwxuiNnsFtafjncK5cUKC8ET9tTrQYl4bnj2ZV5UsLlf3+2hzMsig2nKoJ6Xs7qqM3icTMhIJvg7Mur6I5Sw850U9hioFO6POMJNnL2FoorP+/zrVli3cTQm7y//AJ3Mu6c=', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='49. Riba, P., Dutta, A., Goldmann, L., Forn´es, A., Ramos, O., Llad´os, J.: Table de- tection in invoice documents by graph neural networks. In: 2019 International Conference on Document Analysis and Recognition (ICDAR). pp. 122–127 (2019). https://doi.org/10.1109/ICDAR.2019.00028 3', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 19, 'orig_elements': 'eJxFUV1r3DAQ/CuLnlK4yp852/d25Ei5ctBy9C0Jx1paOyK2JGy57RHy37tyEgoCjWZmRyvtw6uggUay4WK02IFI87qjYouVxlKrskJVpCrvmqIudbvNW7EBMVJAjQHZ/yo6M5DFkWKxdnYJF4+eJul1F71RDle/yuj9YBQG42zyIQ9o+wV7mll/EGR78cSsZ+Zil7GlifmsWanpf5N1y03qSpVU1NlWVWl1i7nKqL7dNl3XdOKNKwL9DdFcNhLOpsUN/JQbOCwhMNwz/OYGPaK1Gzjx6d5N9nFJ07ak+V0/4+gY/mB4GlC/i5H5LnfwC9uBQNNXCKTii8DE9dsZxbRTS/zTGdor9BP6Z7C0TDjwFv646WWWcLQ7yNOsYRBosuuvsOHO2Y74qZzCmYePINizdp3NDGg1nEm53pr11pvj3WF//iLBewlZnj8uHFpkeQU3MZ2F5xD8vEsS7Yx0U59kqcyytEnWQhlNMk157FDEgXzO6mTmcAw0irenfx1QqRk=', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='50. Rijhwani, S., Anastasopoulos, A., Neubig, G.: OCR Post Correction for 2020 Conference of Endangered Language Texts. on Empirical Methods in Natural Language Processing (EMNLP). pp. 5931–5942. Association for Computational Linguistics, Online (Nov 2020). https://doi.org/10.18653/v1/2020.emnlp-main.478, https://aclanthology.org/ 2020.emnlp-main.478 3, 11', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 19, 'orig_elements': 'eJxtUV1r2zAU/SsXP3XgyZK/nbcSwhikaej21pagyNeOhi0JS+5WSv/7rk2Xvuzxng/pHM7jW4QDjmjCSbfRBqIMucq5aJucnzNRlwWWSvGqKrEqS5GVUQzRiEG2MkjSv0WdHtDIERdza80cTk46nJhru0W70OHVrbR0btBKBm1N8kEP0vSz7NET/xih6aNnQh0hJzOPZ5wIF80KTZ8h63PdYVupHLNalKriVSFTJbAuyqbrmi56J0fAP2ERF5zBg/51+S2NjuEHi+HWSB+kt87Og/V0E3bA+az7GL6xDdxvH+BofYCtnSZUS17o7AQpTzlhpkPKohBsBzvTUgO6W9h/VIGf9LFnQKbd6PREjQe4w3CxrQdt4CDDPBF01R8nq9B7bXq42d0d9scvDJxjUDSZeJpTLrKiyVMGt95bpeU1ztaObg7rvTxH/ln7oBU1ujeDNgg3B/uypqYXLyE4v0mS1mpmpz4RnC3zZsmLSBYJw9EM7usotWF5VcdXg1Q0EqUfbP+6OuE/cshiEGJZ9N/Ye8ryPeAYvT//BRt0wbY=', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='51. Schaefer, R., Neudecker, C.: A two-step approach for automatic OCR post- correction. In: Proceedings of the The 4th Joint SIGHUM Workshop on Com- putational Linguistics for Cultural Heritage, Social Sciences, Humanities and Lit- erature. pp. 52–57. International Committee on Computational Linguistics, Online (Dec 2020), https://aclanthology.org/2020.latechclfl-1.6 3, 11', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 19, 'orig_elements': 'eJxtUcFu2zAM/RXCpw1wHNuJ4zi3IQOWDN06NBt26IpAoWlbqCwJMoWtKPrvo4MWu+wgCHqPj++JvH9OyNBIls+6TXaQrMty1eSXSq5VmV8u2DbNuq5pWzRrbBGTFJKRWLWKldQ/J502ZNVIs7h1NvLZK08h82031840P/krrbw3GhVrZ5evtFG2j6qnSfj7hGyfPAjqBTnbOF4oCF40Vyj8C7m9bDtqa1zTaltssM7rSpVY0LbaNF3XdMmLKJj+8FxcFRmccFDUUUjhLkvhK8WW8HF+7rMdfAD+7RYTkwdJGJzCAToXQEV2o6RFuN3fgXcTLwBdCITzDzI42h18Cw6JWm37CVwHPBB8l7PmAT47bRlOx0+HH1/gpwuP0+A8OAt7Ny7AR74OQhm4EXXUkxhNV999NByDEAcKmmUUKZwcagFOqMkiTSkc4qisZk0TKNtKC8lGQYmOMvA+g6r8Fcu8WFX1HJQp2Dc7sR81M9Frlv8nSeHWGm0J3n0khDIv8/cpDMx+2i2XCmVvPDjj+qfMhX4585lRTDig6cyiyDawSqEo5h2/rf9GOh+ZxuTl4S8PAc4I', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='52. Shi, B., Bai, X., Yao, C.: An end-to-end trainable neural network for image-based sequence recognition and its application to scene text recognition. IEEE Transac- tions on Pattern Analysis and Machine Intelligence 39, 2298–2304 (2017) 13 53. Shi, B., Wang, X., Lyu, P., Yao, C., Bai, X.: Robust scene text recog- In: 2016 IEEE Conference on Com- nition with automatic rectification. puter Vision and Pattern Recognition (CVPR). pp. 4168–4176 (2016). https://doi.org/10.1109/CVPR.2016.452 13', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 19, 'orig_elements': 'eJxlUl1r20AQ/CuLnhKwZH1L9ltr/GBIwbglbUmDWUkr+ah0p+pWpCHkv2el2I2hD8cts7N7M8M9vDjUUkeaj6py1uAUEfpplkVpFCd+WKervCgT9JOU4qSI0tBZgNMRY4WMwn9xatWSxo6m4crokY899jR4fVVP3KnNz/3cxr5vVYmsjF6e2y3qZsSGrPQfHNKN8yhoL8hRj11Bg+DBaoaGD5F5kddUZWVMUR6kZeZnCYZlQHmSrup6VTuvMsH0lydyEnrw9aQW8NmTg1L8kOInmgVsvDV80kC6ctm4cgEPqDQWLYGmccBWLn4yw2+ozQCqE11ugZYqsPRnJF0SDFSaRqvJFKBsUGzhyiiwAVuSJpj0XLM92G23W/g2oLZYujBhFmRij8w0aFGG7bNVdl77BcuTki07zdS2qpnfjiSZMFzlv8bQD6Iw8mO4kSq7hSCCJLry/V1yfjd+9zwuYP+RwL9M1nAwxWj5P7muPLoG2Zu+K94YXdMwCxC1G9O5cPb/pPgEOLLpxHs5TbOqz0F40I9iC+6VvUR1MXq4ivBmc78/3Aq59yAO0rO1OMjS2VoqrRNzb9fLZWWUZ4ZmGfheEPir5TTpTRwvTkJJYPpfl693pyzvmDrn9fENOh3rlg==', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='54. Taghva, K., Beckley, R., Coombs, J.: The effects of ocr error on the extraction of private information. In: Bunke, H., Spitz, A.L. (eds.) Document Analysis Systems VII. pp. 348–357. Springer Berlin Heidelberg, Berlin, Heidelberg (2006) 2', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 19, 'orig_elements': 'eJxNkMtOxDAMRX/FygqkKPQ9ndnxWDDAChAbQCM3dUpEm0RJihgQ/046AsEy59iOfR8/GY00kYk73bMNsJoq1TQ19U1V5XVXksoxl+W67RCLrGwZBzZRxB4jpvpPpvRIBidamntr5rhz6MgL16uldtFx7w4anRu1xKitOfnRI5phxoFC8o+MzMCeE3WJ7Mw8deQTz9cH5P+WbLtWUb+SFZVt3shVtqqxkDm1dbNWaq3YV+qI9B4PF1UC7nF4eUMO14LDGcnXkfYcbtPj3NqpCxyuxAbuXwhIKZIxgFVgpQfy3nqwBuLi3qNHuWy/aOf1G0YCbZT10+EoAVuzgbPZvBKHyzT9zun4weFU3Ag4oj6IY7iwcl7ihlOD4z7oAHf7EGkK8LDdCnBOQFm1T3OR5WVZr0Sa4bUZyKe9/agNXJLuaUzJDPwH8X8Mjoosa46hWML9zf1Gh7hNf7Cv52/ECJwQ', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='55. Tan, M., Le, Q.: Efficientnetv2: Smaller models and faster training. In: Meila, M., Zhang, T. (eds.) Proceedings of the 38th International Conference on Machine Learning. Proceedings of Machine Learning Research, vol. 139, pp. 10096–10106. PMLR (18–24 Jul 2021), https://proceedings.mlr.press/v139/tan21a.html 12 56. Tian, Z., Huang, W., He, T., He, P., Qiao, Y.: Detecting text in natural image with connectionist text proposal network. In: Leibe, B., Matas, J., Sebe, N., Welling,', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 19, 'orig_elements': 'eJxdUttq3DAQ/ZXBTwkYr++XfewFmrBbclkITRoWrTxai8qSsORNS8i/d7RuCeRJw5mjMzNn5uk1QoUjar+XfbSGqMaizxnHQ8ebqmmwxLIsiqJumgMXZVlHMUQjetYzz4j/GgmpULMRw+fe6NnvLbM4JbYXgRvS/o89p5m1SnLmpdGrf2nF9HFmR3SUf4pQH6NnQi0hez2PB5wIz7ozNL032R5agX3DSyzarOZN2lQs5xm2Vd0J0YnojX54/O0DuaoS2DEdwzaJYYMx3CZr+CqE5JIUNfpTvob7kSmFE4ymR+WA6R4Ec54QPzGppT4mcKXXsEWp2CL1OFDvMewSuMDeJZdwMxmO2BPXgRHgB4Si9QP9Ix19Hpsp+Gy0QJqFIxgNW8YHqZH6YtNS5YPKRwLcoaOQDzGcjEogK8gdaylI067+OedpVmRpltaktN3cwUXWLmBewvWsIE/z7DKGwXvr1quVfa+WjIq2NqFzqxOprjzTecaSwY8KshwqUtzJ4OMjDf9tPg//EEIMJizvDb23kpkYfpDJX9Aj96HpsAuQGsiFeSIT5EgbhhdJ7nCjdWAZLZ1fiNSUNY5otJwXM/1arN+gPFCJT1RiS8fnYrim8B4D+J2iB1SKasXhrP5f3IY0rzyO0dvzX7qG6qg=', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='M. (eds.) Computer Vision – ECCV 2016. pp. 56–72. Springer International Pub- lishing, Cham (2016) 13\\n\\n57. Tito, R., Mathew, M., Jawahar, C.V., Valveny, E., Karatzas, D.: Icdar 2021 compe- tition on document visual question answering. In: Llad´os, J., Lopresti, D., Uchida, S. (eds.) Document Analysis and Recognition – ICDAR 2021. pp. 635–649. Springer International Publishing, Cham (2021) 1, 14', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 20, 'orig_elements': 'eJzVUk1v2zAM/SuETy3gerHkr+RWJD2kS4ch7XJpi4CWmESALXu2nCwr+t9HJe02bMDuAwRI4uPHI/keXwKqqCbr1kYHEwgUFaUej8c5jotciyJFyrFUmZSJ3oxTDEIIanKo0SH7vwQbU5HFmnywbuzg1i221EWt3nhfD7tje4KxbSuj0JnGfniDK7TbAbfUM/4YkN0Gz2xt2bK2Q11Sx3YxOpm6XyR1no2EFCSl3kglqMwIy0zHGQpBWsrglSMcfXPe+S6CC9J9dAnTpm4HRx2sTM8c4GkQo1jCzXS64iJxFkHbRpBmZ3suIrhvO2O3HDG3HGdP1LGCz0N5BZXpdwyGMN1hDRc+wSXE0nf13vAn7DqO2dOD58Kk/px2USRKFLGkMs9SJl7KTFCSoipVnG+K5P+bdppH8GBcE8IyCuEO3Y4OfPP7Fg+4w47nFa34u8JqT/YYwg1/PiLP6Tv2IcyiCcyVxo6JiBgUr4yuwBnfBvDRjRr8AGFv+oFX8XWg/oSh7Q/ktxXxsiawqFA/DaNRmTSc9ZZrLJq2876+Rghf1M5oDOH+pzpm75mvecXH3vScUsOSVLO15/JveplPZ9fLE72zYDKZnpEsGf9LMn8JRsQsmBDi5HfNLEzv5o7q4PX5B2v6HZw=', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='58. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L.u., Polosukhin, I.: Attention is all you need. In: Guyon, I., Luxburg, U.V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., Garnett, R. (eds.) Advances in Neural Information Processing Systems. vol. 30. Curran Inc. (2017), https://proceedings.neurips.cc/paper/2017/file/ Associates, 3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf 4, 5, 12, 24, 27', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 20, 'orig_elements': 'eJxFUsFu2zAM/RXCpxXwZMey4yS3bMCydEERrFh2KIqAsehEqC0Zktw1Kfrvo7IWu5F875GPEh9eE+qoJxP2WiULSJqyquf1pFAyn5albGZqXst2hi3SXM3LPEkh6SmgwoDMf01a3ZHBnqJYWTOG/YADOTGoNnIjHM7DFcZh6HSDQVuTvcMdmuOIR/KMPyRkjskjVweu7M3YH8hxvcivJfffpKqneSELklK1sinoMCU8TNVkikVBSsrkjRWBXkIkVzMBO/R/0OgUliKF+xNeiFwKd5xs0fX4Hv/ylyfrSIcUbjm9tYZ8ChsOV7anS1RH2g/UPso3YowNbGf9+HTSJoW1WMAyBDbKK4L2gF0HZzuCIVIC1mYBq/Fsr0zWjy+H0R15rthx+oW315btcfybhdicUvjOyTdyx5GN/OR4p/2JN8FwQvOPukJnKIQIwydSXtzAUj2jaciDNnBHo8OOR7eWF7362jrLoNfmCPdnH6j3Ap5tJ0DmAr6OzqFhfsPtinxS36RwCmHwiywbopAUC70w3FcPXjRNdv3vLHKz+NsZLL23jcYQX0+2FVFRyqqsFdF80h5UXslm0pQ4KyvEz9uPa4EyhSqFSZFCwWFRx/v4OJ2N9mHNVpO3x79buNV/', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='59. Wang, J., Hu, X.: Gated recurrent convolution neural network for ocr. In: Guyon, I., Luxburg, U.V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., Gar- nett, R. (eds.) Advances in Neural Information Processing Systems. vol. 30. Curran Associates, Inc. (2017), https://proceedings.neurips.cc/paper/2017/ file/c24cd76e1ce41366a4bbe8a49b02a028-Paper.pdf 13', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 20, 'orig_elements': 'eJxFUV1r3DAQ/CuLnxpwZFt2fOd7SwtNr4QSGpoUQjhW0vrO1JaNPpIcIf+9q0tDnyTtzM7Oah5eMxppIht2g8k2kCnVdWatL5oeTYd91a2kwr7rFa2alup1lkM2UUCDAZn/mvXDSBYnSs1mtjHsFlzIicX0iZvgcFxOMC7LOGgMw2yLf/CIdh9xT57xh4zsPnvk6sKVnY2TIsd1WZ5K7r9Js2pLWUuqa9PXWpJqCVVrqhalJFPX2Rt3BHoJiXzRCbjnMTl8Fzl8izn8Fhu4wkAGHOnokjDo2T7NY0zewFJ0OPIRnmf3B/rZwaydgK3lvnicbQ5blrqOLyo61v0l7vj5md0Pcw63fL/HcUR94HH8+EpuH30OP/l+N/jDM1oMB7Tv1Ct052lUSAT4RMaLM7g0T2g1eRgs/Hh3s7XsYzp9Hty4mUE/2D3cHn2gyQtg9wLqUsAX3ggtXHo/64G35Mlbq1laltXqLIdDCIvfFMWSRMiwiBdp42HxQuvilF6RuAWk8AotG80fTpWmpqrbFhulaI1Np0qJpVyf33zkDVWdMv2I+3rwYcvmsrfHv2ybwuA=', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='60. Wang, S., Li, B., Khabsa, M., Fang, H., Ma, H.: Linformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768 (2020) 12, 27\\n\\n61. Wightman, R.: Pytorch image models. https://github.com/rwightman/\\n\\npytorch-image-models (2019). https://doi.org/10.5281/zenodo.4414861 25 62. Williams, R.J., Zipser, D.: A learning algorithm for continually running fully re-\\n\\ncurrent neural networks. Neural computation 1(2), 270–280 (1989) 5', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 20, 'orig_elements': 'eJzVVE1r3DAQ/SuDT1nwevVha+29tZTSjySUptDSNATJGntFbdlo5SSb0P/e8W5KSsmh15wkzZthRu896fIhwQ579PHa2WQDScW4rA2WWGpuZI2FqlAxuebGaM65TlJIeoza6qgp/yFpXIde9zgX28FP8XrUI4ZstM2cO8NxPx5gPY6dq3V0g189wp327aRb3BF+maBvkyuKjhS59lNvMFBcsEMoPA1p14oJKVBK28haoFGojbJcaSHQSpn8ooqId3FOViyDr9QmhYsshVOXwmtaP2612ekUzmj/9oC+o92ZntcNZflmCD2GDVxg1yx1jNSd5oZbF7fQOY86QD30Y4d3Lu4z0OGbu4Ex4Bicj8fjRjCmMpavVQknggm2AC5SEOv55n9IOXW7+D5iP8/8rxhMMrTr0qjSGqtyrlguNC95zUzVVNy8QDE4ieHabey1T+EzUf1pH4dQb8H11Af6wWK3y2Ab47jbrFYt0T2ZjJhehdvHutX/0leUqCRvGK9yabUpLBrRrJuyMVIqAl8efeORrOWBrOWRrNlavFo8cWYHlw2hXXGWFaLkq3v0gx2yPOd5qTiIApSYVeg6p/vdrMIHsv53N+4wpPCGNHkFHfnbO9+C7tohkAg90IMgx9Mr8JPuuj2EyR8ymulwwuXfspzrEIicG/wyT/6ctTkvyMSNyZmu6fplXrGCPM1FvZa5ZC9Pm3oKcyl4nILuaIm3Q/hJXj4/BubfYoqHqYCfiMX8EbAfE4knRcnghFdltYDieXNf/QYkcsGD', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='63. Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., Davison, J., Shleifer, S., von Platen, P., Ma, C., Jernite, Y., Plu, J., Xu, C., Le Scao, T., Gugger, S., Drame, M., Lhoest, Q., Rush, A.: Transformers: State-of-the-art natural language processing. In: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. pp. 38–45. Association for Computational Linguistics, Online (Oct', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 20, 'orig_elements': 'eJxlUttO20AQ/ZWRn1rJcRybOJA3BG0FCiUlqBdRhCbesb3Sene1F1qK+PeObUIf+jaXM2fOXO6eE1LUkw4PUiRrSE72SNUCl/nxanlEVIgip5NmWeKRKMSqWSUpJD0FFBiQ8c9JIxVp7GkoFkbH8GDRksusaAbskA5PdkyjtUrWGKTR89e0Qt1GbMlz/i4h3Sb3HLUcedCx35PjeJGPIfdPpFhVeVEWVJaiKeuC9hXhvhKLCouCRFkmL1wR6HcYwFWZwTejmhRusxTOaR9DChs2d6i7FL6yddZh7I0WKVyOkFEVcZy9KyNTOB1A0gesU9iyfYNRhYlwYyJT37D1Mepgfsn6DxcNNPgovdET565TJBtybLH3aDRsFQbSE90VTr0uyWkZuPEPdrYqTrXf45TdEOxqNFPbT7FtD3Tnjvc/Nd10hjwr+zKIjL4bpK/h1qH2jXE9Ob+GXeDOM9PMQkczdAE0huhQweEYYJ2pyXup2wwu9Bq2g0+CfQ+mAa7joxQ5nBnNM5GuCXiiD72Vju+r4IpCZ4QHqeHzK/fmwL1942YlTz5Qzwvn5fvgxs/wGVibQXn8Mxb5ojxaZnDqvanlmAWegtv2NobRH5iZKvJtZO1TuNZKaoJ313UYJb7PoAvB+vV8LozMjGvnizxbHFfLcv64mA+QjHqt7EywCJ9V6Rsea94Hz6FM+zQW/g+GYjk88eG/N6ziggdKXu7/ArHwDOc=', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='2020). https://doi.org/10.18653/v1/2020.emnlp-demos.6, https://aclanthology.org/2020.emnlp-demos.6 25', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 20, 'orig_elements': 'eJxlUttO20AQ/ZWRn1rJcRybOJA3BG0FCiUlqBdRhCbesb3Sene1F1qK+PeObUIf+jaXM2fOXO6eE1LUkw4PUiRrSE72SNUCl/nxanlEVIgip5NmWeKRKMSqWSUpJD0FFBiQ8c9JIxVp7GkoFkbH8GDRksusaAbskA5PdkyjtUrWGKTR89e0Qt1GbMlz/i4h3Sb3HLUcedCx35PjeJGPIfdPpFhVeVEWVJaiKeuC9hXhvhKLCouCRFkmL1wR6HcYwFWZwTejmhRusxTOaR9DChs2d6i7FL6yddZh7I0WKVyOkFEVcZy9KyNTOB1A0gesU9iyfYNRhYlwYyJT37D1Mepgfsn6DxcNNPgovdET565TJBtybLH3aDRsFQbSE90VTr0uyWkZuPEPdrYqTrXf45TdEOxqNFPbT7FtD3Tnjvc/Nd10hjwr+zKIjL4bpK/h1qH2jXE9Ob+GXeDOM9PMQkczdAE0huhQweEYYJ2pyXup2wwu9Bq2g0+CfQ+mAa7joxQ5nBnNM5GuCXiiD72Vju+r4IpCZ4QHqeHzK/fmwL1942YlTz5Qzwvn5fvgxs/wGVibQXn8Mxb5ojxaZnDqvanlmAWegtv2NobRH5iZKvJtZO1TuNZKaoJ313UYJb7PoAvB+vV8LozMjGvnizxbHFfLcv64mA+QjHqt7EywCJ9V6Rsea94Hz6FM+zQW/g+GYjk88eG/N6ziggdKXu7/ArHwDOc=', 'is_continuation': True, 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='64. Xu, Y., Xu, Y., Lv, T., Cui, L., Wei, F., Wang, G., Lu, Y., Florencio, D., Zhang, C., Che, W., Zhang, M., Zhou, L.: LayoutLMv2: Multi-modal pre-training for visually-rich document understanding. In: Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Vol- ume 1: Long Papers). pp. 2579–2591. Association for Computational Linguis- tics, Online (Aug 2021).', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 20, 'orig_elements': 'eJyNUk1v2zAM/SuETx3g2JGc2EluQYYOHZKth2FfXREoEm0LkCXDloIFRf97KS9dD7vsYEh8fO+RpvjwlKDBDq0/apVsIBGyXlSnuWTrGquFXJSsQrbirF5VS1bLdZJC0qEXSnhB/Kek1gat6DCKlbPBH3vR45D1qo7cmPaXfkqLvjdaCq+dza9pI2wTRIMj5R8StE3ySGhPyNGG7oQD4Xw+QcNbk6oq57zgWBSqLiTHU4niVCpWCs5RFUXyTAqPv30kl4sMvocUfmTp33N/TuELnbugKaDLN6TLbbxQQyl8iJwr99Y4Ki21S+E9hT/bibGL6hZJ8IYdpqsL0XIDe3Fxwe8PZ76BQzBezzqnhIF+wJkfhLbaNlC7Ac56DMKYy2zQsgXlZIjvAcEqHEYvrCJiBnd2A/eDk4gxHsHV4FuE5dq3sLWWHOCA6KPpNbUdRyf1NO6pzs51ffBTTOQ9MYMevZYjUI1JwRiZ3VmPg32lfXSaetk5W2OcAgKZfRI+DNHi+nh/+hrHWPvmqzMzoD8ARiNwhNzHdRjfZdD3GfBltf4V+JwVfLlm2f/1OIPYZQqfrdEW4WYbGloKzsiz9b4fN3munM7c0ORsnrFVuSzyM8sjJRPSzAy1kVHN9EqHPCdYWN8645rLJPyHDDyFRQrrFBjtHyMxo5ATyiv6VnF7Xxd7T3O889glz48vHlQJ7w==', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='https://doi.org/10.18653/v1/2021.acl-long.201, https: //aclanthology.org/2021.acl-long.201 2, 4, 9, 10, 11, 14, 22, 27, 28', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 20, 'orig_elements': 'eJyNUk1v2zAM/SuETx3g2JGc2EluQYYOHZKth2FfXREoEm0LkCXDloIFRf97KS9dD7vsYEh8fO+RpvjwlKDBDq0/apVsIBGyXlSnuWTrGquFXJSsQrbirF5VS1bLdZJC0qEXSnhB/Kek1gat6DCKlbPBH3vR45D1qo7cmPaXfkqLvjdaCq+dza9pI2wTRIMj5R8StE3ySGhPyNGG7oQD4Xw+QcNbk6oq57zgWBSqLiTHU4niVCpWCs5RFUXyTAqPv30kl4sMvocUfmTp33N/TuELnbugKaDLN6TLbbxQQyl8iJwr99Y4Ki21S+E9hT/bibGL6hZJ8IYdpqsL0XIDe3Fxwe8PZ76BQzBezzqnhIF+wJkfhLbaNlC7Ac56DMKYy2zQsgXlZIjvAcEqHEYvrCJiBnd2A/eDk4gxHsHV4FuE5dq3sLWWHOCA6KPpNbUdRyf1NO6pzs51ffBTTOQ9MYMevZYjUI1JwRiZ3VmPg32lfXSaetk5W2OcAgKZfRI+DNHi+nh/+hrHWPvmqzMzoD8ARiNwhNzHdRjfZdD3GfBltf4V+JwVfLlm2f/1OIPYZQqfrdEW4WYbGloKzsiz9b4fN3munM7c0ORsnrFVuSzyM8sjJRPSzAy1kVHN9EqHPCdYWN8645rLJPyHDDyFRQrrFBjtHyMxo5ATyiv6VnF7Xxd7T3O889glz48vHlQJ7w==', 'is_continuation': True, 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='65. Xu, Y., Li, M., Cui, L., Huang, S., Wei, F., Zhou, M.: Layoutlm: Pre-training of text and layout for document image understanding. In: Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Min- ing. p. 1192–1200. KDD ’20, Association for Computing Machinery, New York, NY, USA (2020). https://doi.org/10.1145/3394486.3403172, https://doi.org/ 10.1145/3394486.3403172 2, 3, 7, 9, 10, 11, 14, 22, 28', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 20, 'orig_elements': 'eJx1ksGO0zAQhl9llAMCyZvGdpo0vVWtgGpbhFQhKMuqcuNJa5HYUWKzVKt9d8YRiAPiYOX3zDe/xxM/PCfYYofWn4xOlpDwqlnMs7Oq57JGXpelVFVeFPMcRSmqSicMkg690sor4p+TxrRoVYexWDsb/KlXPQ5pr5vIxrS/9VNa9X1rauWNs7Pf6VbZS1AXHCn/kKC9JI8U7SlysqE740BxkU2h4W+TuiwyIQVKqRtZCzwXqM6F5oUSArWUyQtVePzpI1zMU/gSGBxTBjvDYE/fdSCxI/E+UAMMDiQ/I8Xekvh6dSFiS9ipmwu+7ZbwccA7Pyhjjb2AayCag7Ia2gmBxg2gXR3iJMF01D8Eq3EYPUFUk8LWRhdXI8b9OJlcEUThr7Ba7+GwfXe/2RDmcbDTjFQLa2cbpIvXCM7CvXVPLWry3pixdj9wuMEr2NCfgL2xdzCd06fAeSW+BZFxyUWWpRB9p30VR7kaR1eb6YSp7bXr+uDjvfaqvhpLrgw+4BMc3fCd1JHBp8MKXotMZG9SuHrfj8vZTDuTuuEy41nKeT6fSVnl+aJIZZ5JXgr2Dwj/IYFYyaBkUDFiaHFaOQNBCbGIr+TPA9qZ0W89dsnL4y9iBtCj', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='66. Xu, Y., Lv, T., Cui, L., Wang, G., Lu, Y., Florencio, D., Zhang, C., Wei, F.: Layoutxlm: Multimodal pre-training for multilingual visually-rich document un- derstanding. arXiv preprint arXiv:2104.08836 (2021) 10, 27\\n\\n67. Yim, M., Kim, Y., Cho, H.C., Park, S.: Synthtiger: Synthetic text image generator towards better text recognition models. In: Llad´os, J., Lopresti, D., Uchida, S.', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 20, 'orig_elements': 'eJzVU1Fv1DAM/itWn0DqlTTtpeVeDw0GNwlpIDbGdEoTtxeRJlWajp0m/jvO7RAS4g/wFNvf58T259w9ZWhxRBf3RmcbyJDVvWCikbpRrezXol6XrGXVulWMdR1mOWQjRqlllMR/ynpj0ckRU7L2bon7SU4Yikn3iZvgeJxOsJwma5SMxrtXZ9hKNyxywJnwuwzdkN1TdKLI3i1jh4HinJ1C4U+RuhGMVxyrSveV4tgJlJ3QpZCco66q7CdlRHyMiSxEATdLDrdFDruHHD7RuV0MOWR8ofdzeJugM+XCenpJGZ/DG3K/Hk6MbeIiJV0UG9jJo1/iox03cLXYaEavpYUp4CoGaZxxA/Q+wJgwa1KDFh7MTIc9roJRB9BeLWnosLgVaAxzlE4TswAZbsxDumsKhvCTu+ElqwvWtpWAF5zx8iWUNBPepAn+Hu7OzPEy4ph6/1tU/npdMVZyxTViz0XJW96oGnm11shr8R+K2hRwa8YcrkiYD8lI2m0PpNq7Ion1UYbvOVyTWtdHFw/RDBjONkajIF0EZqSaYECHQUZSLPofMugZOowRwzMnoPKDM6k/IKHRzgVcOloCK/W3hf5E7ecc3qcN8iTbHM3z4nxWB6NlKuHfKt3/Aok2FP4=', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='(eds.) Document Analysis and Recognition – ICDAR 2021. pp. 109–124. Springer International Publishing, Cham (2021) 5, 6, 23\\n\\n68. Zhang, K., Shasha, D.: Simple fast algorithms for the editing distance be- tween trees and related problems. SIAM J. Comput. 18, 1245–1262 (12 1989). https://doi.org/10.1137/0218082 7', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 21, 'orig_elements': 'eJzVkc1u2zAQhF9loZMNqLRIkaLkm2Ff3D8UcU5NA4MSVxIBiSIoqm0Q5N1LOQ1a9A16ndldzM738JzggCPacDU62UMikZWVkpTVuSg4LSoppVAy401bN3mVJSkkIwalVVBx/jlpzYBWjbgu68ku4eqUQ0+cbtfZ1Q5P7mYr5wbTqGAmu/ttD8p2i+pwjv5DgrZLHqPqonK1y1ijjzqjN8n/CVmjrBva5jRnXPOGY1OJlpZUSNHynNfJS9wI+DOswxvUM9nCaWqW9U84WDU8zWYGZTXcYTN11qyR4NvCMprD+Xg63AHLGCXgHAGaVa8OZZzAxXljO/RwtgG9vT2jBviy1IOZ+2ilcOzVCJv1wBZECkUKLF9/favhs/I+7n3H+zVhjPovg7pts1JwhsgFlaWgecFQ143gPJNlKf4/BkVJ4Guv1nY+kBQuvZp7lcKJ7OFiRjcgtGoOoIZu8ib04wzt5CH0CKgjHNuBNnNQtkGo8R2EH4gWgkd8pehxUAE1OD/Vsck5YjofPsF7AsdpdEuIEMsUIj/xRrJgsKEMaFVWWwJ9CG7e73Z6MmTy3Y5mhNJc7iLCMisZyL/xfYxJzgHH5OXxF7Qe/Lg=', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='69. Zhang, Z., Zhang, C., Shen, W., Yao, C., Liu, W., Bai, X.: Multi-oriented text detection with fully convolutional networks. In: 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). pp. 4159–4167 (2016). https://doi.org/10.1109/CVPR.2016.451 13', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 21, 'orig_elements': 'eJxFkd1uGyEQhV9lxFUqbdnFy9peX9byhaVUihIpzU8jC8OwRmUB7ULSKMq7B5xG5Wr0nTlnGHh8I2hxRBcPRpENEKElW7frjqtlLzi2LJ9GCq47bGS3aEgFZMQolIgi978RbSw6MWIxK+9SPAQRcKJB6dJb5PgazrIIwRopovGu/idb4YYkBpyz/kjQDeQp05DJwaXxiFPmC3ZG0/9LHnF1lEy3rF1wxSVH2XearVm36jRv+ZG8Z0fEv7E0L3sKD6c8p4IHWn2V21zenNBV8CtX98J/okuTPskPYSq4oxv4mWw03/1k8nhUUFJBYURZ1oAXE0+gk7WvIL179jYVLCw4jC9++jNT2Lu8QsOWsN/tdrD1TmNeRSJk+9aPIUWc4NbMJU44BVciZuLgGqUfnDmPudjeXl1/oxACBc66/nfKiS1nyxVclOwsnWIM86aulTfUT0PNGpp/rq+Lk5YeyjsGrC2v/vUhl2aO+4gjeX/6AM0gn2Q=', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='70. Zhong, X., ShafieiBavani, E., Jimeno Yepes, A.: Image-based table recognition: Data, model, and evaluation. In: Vedaldi, A., Bischof, H., Brox, T., Frahm, J.M. (eds.) Computer Vision – ECCV 2020. pp. 564–580. Springer International Pub- lishing, Cham (2020) 7\\n\\nA Appendix\\n\\nA.1 Details of OCR Engines (MS, CLOVA, Easy, Paddle)', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 21, 'orig_elements': 'eJzVU11P3DAQ/CurPIEUUjuJ88HbcVD1KhCooFNbipA/1jlLjmMlDgIh/nuda6u2iPeqb/HM7np2PLl9TtBijy7cG5UcQ9K0bV40OZNStUQ2ZVtjqQiTEeWMlDJJIekxcMUDj/XPiTYWHe9xaVaDm8O95x7HzCu91C50ePJ7mntvjeTBDO7dT9py1828wynytwm6LrmLqI/IvZt7gWPEc7qHxt8iBdZCUl3QIi9VKUuULdO0oaxmuixKkbzEjoCPYSmuSQZfd4PrUvicpXC949qgOeEP3JkUziL00UQDBviCHqcUVtkxbPqo4EjwCRUELizCiHLonFm0H8NpXD6FflBoU+BOAT5wO+8Xy2ATC7aouFVmGZbCiZnkbtApfFgO4/CYwk38ej/yXR8vzy4yOEA1ZYewHno/Bxxha6Y4C77NOaEFnK3XW8hJHhfxPgNWlT8I1kTk2o/GdbFn42Kn24vgFq5mcQTWTDuzLL7e8R4OlhGHUC++/3qSczOFTcB+cex1FFBI0oq6apsKsVWyEC3ButWk1aWuBfkXUchfR4GyptVSatR1zphWVApdywpJ3uRENPSvKKxg5T06ZR7/9ODGBItvGkBR0opho0vFKlbwIqcNaUilGZHI+X9oQEbhNCo2doJBw+X6E5y5zjic4ODiOubk/HK7ij8Fn55SuOJKWTx8w6q774VVSx8=', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='Current state-of-the-art visual document understanding (VDU) backbones, such as BROS [18], LayoutLM [65] and LayoutLMv2 [64], are dependent on off-the- shelf OCR engines. These backbones take the output of OCR as their (one of) input features. For the OCR-dependent methods, in our experiments, we use state-of-the-art OCR engines that are publicly available, including 2 OCR API products (i.e., MS OCR7 and CLOVA OCR8) and 2 open-source OCR models (i.e., Easy OCR9 and Paddle OCR10). In the main', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 22, 'orig_elements': 'eJxlU11vGkEM/CvWPYEEV7jkCOlbSlMpUtK0SRpVQijy3fpgxbJ73Q8SFOW/17vQI1We4MZjezz2zl8zUrQh7Z+kyD5DNi3Pz6qz6TlVZSPqUlSIDYmayqLEST0W2QCyDXkU6JH5r1kjFWncUEwWRgf/1GJLNm9FE7kx7HdtCmPbKlmjl0Z/OoQV6mXAJTmOzzPSy2zBaMvIkw6biizjRZEgexRJY6rHk5KmzakoJ+UJnhTj6Wg6mjTlqCbE7I0zPL34SJ4FG1PBefQ0NM3Qr2iI1sNWuoAKhKlDNACCFmSZpYXUS+g9fv3VhwrrdWU0uQG4UK8AHXy5u72H+XjKQq9xZ4K/voH5pFwAJ3bItmDslCksGwS1xLW5hdFgmr0CcCtSDdzO7oDHltwih4cVOTr2BI9rAiYD12wDp+/5LIJBaaHHLAb7IHUMN4Q+2Fjom7Epj9nDY3fe28oIHkWyjGCBXnhRMs7O2DNB4OYfXHonkEuiTxO1oeJNqh3gFqXCSlEsWquQrCtS0sWPK2itEaH2Dnoyp3wAN/cxdJasml3fPl7Ez2k/fRdgWOjQsbI6KYeNEaS63Et0uwifJ/YPFEIl2njUz+FKp3k3yKOl+xu8Y4B0cTYBzcGW2SqOw9bayPeyXpOHeNGOf+fjYvG/wA/57HHHd9HNCHZ3JDXzNunM2WLuUae/vavLPm/UrXk/ex+6ut7wbtDx7vblg9bRR8+7iTuPWHdqDW4kG89Nu4a1Qudkc3haSfvhtv8EVrrH3DPvOt31z4uDjgEPpJR5Tr24R2tpK01w8GzsOjb+HYANQZWnY855BfwEjgcR9fMNKTaRhzjY0JXM4/v+9/S/o7WsbksP8VW+Lf4CzmeBOA==', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='paper, Paddle OCR is used for the Chinese train ticket dataset [12] and CLOVA OCR is used for the rest datasets in the document information extraction (IE) tasks. MS OCR is used to measure the running time of the LayoutLM family in document classification and visual question answering (VQA) tasks, following the previous work of Xu et al. [64]. Each OCR engine is explained in the following.', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 22, 'orig_elements': 'eJxlU11vGkEM/CvWPYEEV7jkCOlbSlMpUtK0SRpVQijy3fpgxbJ73Q8SFOW/17vQI1We4MZjezz2zl8zUrQh7Z+kyD5DNi3Pz6qz6TlVZSPqUlSIDYmayqLEST0W2QCyDXkU6JH5r1kjFWncUEwWRgf/1GJLNm9FE7kx7HdtCmPbKlmjl0Z/OoQV6mXAJTmOzzPSy2zBaMvIkw6biizjRZEgexRJY6rHk5KmzakoJ+UJnhTj6Wg6mjTlqCbE7I0zPL34SJ4FG1PBefQ0NM3Qr2iI1sNWuoAKhKlDNACCFmSZpYXUS+g9fv3VhwrrdWU0uQG4UK8AHXy5u72H+XjKQq9xZ4K/voH5pFwAJ3bItmDslCksGwS1xLW5hdFgmr0CcCtSDdzO7oDHltwih4cVOTr2BI9rAiYD12wDp+/5LIJBaaHHLAb7IHUMN4Q+2Fjom7Epj9nDY3fe28oIHkWyjGCBXnhRMs7O2DNB4OYfXHonkEuiTxO1oeJNqh3gFqXCSlEsWquQrCtS0sWPK2itEaH2Dnoyp3wAN/cxdJasml3fPl7Ez2k/fRdgWOjQsbI6KYeNEaS63Et0uwifJ/YPFEIl2njUz+FKp3k3yKOl+xu8Y4B0cTYBzcGW2SqOw9bayPeyXpOHeNGOf+fjYvG/wA/57HHHd9HNCHZ3JDXzNunM2WLuUae/vavLPm/UrXk/ex+6ut7wbtDx7vblg9bRR8+7iTuPWHdqDW4kG89Nu4a1Qudkc3haSfvhtv8EVrrH3DPvOt31z4uDjgEPpJR5Tr24R2tpK01w8GzsOjb+HYANQZWnY855BfwEjgcR9fMNKTaRhzjY0JXM4/v+9/S/o7WsbksP8VW+Lf4CzmeBOA==', 'is_continuation': True, 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='MS OCR MS OCR7 is the latest OCR API product from Microsoft and used in several recent VDU methods, e.g., LayoutLMv2 [64]. This engine supports 164 languages for printed text and 9 languages for handwritten text (until 2022/03).', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 22, 'orig_elements': 'eJxdUNtKw0AQ/ZVhnxRKmrupb6IvQqui1ZdSypidtAvJ7rI7qZbSf3eTegGfBs45w7msjoJa6kjzRklxDaK6KnOkqsKSkhiLUuZxmsdJXlBS1PUsExMQHTFKZAz6o2hUSxo7Gp6l0T1vLFpykZXNoB1oPtiRRmtbVSMro6ffdIt62+OWfOBXgvRWrANqA7LRffdOLuBpOkLuLyQlVCdlQVWTy6IsMszSpIqruGyKuCZEcQofTJ88iBcv8Hj7DOdzBcoD7whaZPI8MjdP92CdkX3N0DjTwULVznjTMKCW0HuSoDR42pPDFhzVIQm83b1CGGJnpJ8ARdtoAnM8mJ7ni30KqzJfR7DcBbdQSmkC31trHHtIyhx+a0NjXDBXmoPJkHi0nP0T7AL44RQz6bPootesWkjjNJ3G2WU0TPmz8gM6Fzbe03IY4LT+Ajvvm0w=', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='CLOVA OCR CLOVA OCR8 is an API product from NAVER CLOVA and is specialized in document IE tasks. This engine supports English, Japanese and Korean (until 2022/03). In the ablation experiments on the CORD dataset [45] (Figure 9 in the main paper), the CLOVA OCR achieved the best accuracy.', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 22, 'orig_elements': 'eJxFUctu4zAM/BVCpxYIUj9TZ29BNgtkt2gWRdFLEAS0TDvC2rKgR5C26L8v5b5u1MxQHA73r4J6Gkj7o2rEDxCSUqS0zPM6l7d5cVuUdZUWS5lmy4SrTMxADOSxQY+sfxWt6knjQLG5GXXwR4OG7Nw0bdRG2j+biUZjeiXRq1HffNA96i5gR475vSDdiQOjhpGjDkNNlvEsmyD7bZJSkumipKotmnJR5phnaZVUyaItE0mI4o07PF18FK/vdk8r2K0f4KuqQDlADau/WzB2bIL00NpxgPvV0+ZTh7qJMmdIKuzVC/FTQzPKEOOC7QY8un9uDo8nlrF1pQlcMGa03sFGd71ypxn8RoOaHE3//Rkt8dyroL3qIUuy7CbJr+ew1eBPLKn7KR2gC0eo4hwH4zu33j38hJi6Iw/7ojzA1S/VBUuwjL6iZEAupvSvZ+89X6ujPCk68woRrsl5RmSwKJ/n8QyfF7pHa9nBmR5jeG+H/2/mras=', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='Easy OCR Easy OCR9 is a ready-to-use OCR engine that is publicly available at GitHub. This engine supports more than 80 languages (until 2022/03). Un- like the aforementioned two OCR products (i.e., MS OCR and CLOVA OCR), this engine is publicly opened and downloadable.9 The entire model architec- ture is based on the modern deep-learning-based OCR modules [4,3] with some modifications to make the model lighter and faster. The total number of model parameters is 27M which is small compared to', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 22, 'orig_elements': 'eJxNksFu2zAMhl+F8KkFYtdxatfZbSiG7bCuwNbtUhQBbdGxMFkSJLpZUPTdRznpupvF/yf50eTjS0aGJrK80yr7AFl93VZ12W231aDqcmiaYV1WPbXYdG1zozBbQTYRo0JG8b9kgzZkcaKUrJydeefRUyi8GpI3yXz0i4zeG90ja2evzrJBu59xT1H0x4zsPnuSqJfIzs5TR0HiVbWEwjskralfNzW1w7Wqm3qDm2rdlm3ZDHXZE2L2KhlMfziZP2E8wv3td3j72IKOgBAI1TFnl8+RFl26a0vAI3Jy+LkTWnMEfEZtsDMEInzW/GXuCngYxXLOiLP3LnCEyYUl30Jbwr/R4GK2rA1UZVVdlZvLAn7aHIz+nbxSdJCstAH5LaSAD26h8cGpuZeiF7qgYgV3P5YwWgW3X+9/fUyvy5VUeOf4H9p5StWSXbmDNU42JiMUWyEnSN0EdXKKDGDoR83U58BzWKp0GCXX2YUvmYIFReRzQxistvv85EhAIs9Ghny8Xm2e4KB5hOimJU0P521HYAcTngc+dTV6PzKFhXDAKJ/FgsaO0cBp+eCGs1vWLzcmppj4qps7OIy6H9MjTmgM9G5KJ6JSp9QkMjLlbsjlkWPgU50zZpFO7+0qv2EIQvlMD+lgXp/+AgGgBnA=', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='the state-of-the-art models [4,3].', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 22, 'orig_elements': 'eJxNksFu2zAMhl+F8KkFYtdxatfZbSiG7bCuwNbtUhQBbdGxMFkSJLpZUPTdRznpupvF/yf50eTjS0aGJrK80yr7AFl93VZ12W231aDqcmiaYV1WPbXYdG1zozBbQTYRo0JG8b9kgzZkcaKUrJydeefRUyi8GpI3yXz0i4zeG90ja2evzrJBu59xT1H0x4zsPnuSqJfIzs5TR0HiVbWEwjskralfNzW1w7Wqm3qDm2rdlm3ZDHXZE2L2KhlMfziZP2E8wv3td3j72IKOgBAI1TFnl8+RFl26a0vAI3Jy+LkTWnMEfEZtsDMEInzW/GXuCngYxXLOiLP3LnCEyYUl30Jbwr/R4GK2rA1UZVVdlZvLAn7aHIz+nbxSdJCstAH5LaSAD26h8cGpuZeiF7qgYgV3P5YwWgW3X+9/fUyvy5VUeOf4H9p5StWSXbmDNU42JiMUWyEnSN0EdXKKDGDoR83U58BzWKp0GCXX2YUvmYIFReRzQxistvv85EhAIs9Ghny8Xm2e4KB5hOimJU0P521HYAcTngc+dTV6PzKFhXDAKJ/FgsaO0cBp+eCGs1vWLzcmppj4qps7OIy6H9MjTmgM9G5KJ6JSp9QkMjLlbsjlkWPgU50zZpFO7+0qv2EIQvlMD+lgXp/+AgGgBnA=', 'is_continuation': True, 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='Paddle OCR Paddle OCR10 is an open-source OCR engine available at GitHub. We used a lightweight (i.e., mobile) version of the model which is specially de- signed for a fast and light OCR of English and Chinese texts. The model is served on a CPU environment and the size of the model is extremely small, which is approximately 10M.\\n\\n7https://docs.microsoft.com/en-us/azure/cognitive-services/', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 22, 'orig_elements': 'eJzFUstu2zAQ/JWFTi3gSKJsqXKuRh+XtkGRoocgMFbkUiIgkQJJOS/k37tU2jrtD/QiCDOzw9nHzVNGI01k49Go7BIy1clGY6V0o/etqGqtStV2aofNTrTYqWwD2UQRFUZk/VOmzUgWJ1qLnV3iccaZfD4rnbSJjg/zSuM8j0ZiNM4Wv+gRbb9gT4H5m4xsn90yOjNytMvUkWe8qlbIn0OSICmamlq9U3VTb3FbibZsy0bXpSTE7JkrIt3HJL5CpUaCr4dvcP4VJZgAaMHNZC+CW7x8kXAEYwnwhGbEjsUY4aOJn5Yuhx8ESyAFCKPph3hH6QtvTE75BibXcatv4UQ+cIPgNMSBGFY0wt1g5JBeDDNJg+P4AIouIJjesp92nj01hsiJ1Iv5GoY93tt+NGFYicPA0QJB6izkcP3HPhmTP7EVP4xwuPrOfZyMdzZtdq1NWYJ5pL9zcSF7eT4AThQmDrY5Z+V1eXdvJoyJFeXnPG3s9zK/oPe8yhNdpznzwP+9JNFWoqN2X27FttnX+53aCRK6VdS1Uuz1f7mk14fxbohxDpdFoZwM+WSkd8HpmEs3FXwUSyjwcfFUSNdbkzq9SEM2kkLxehAfnIvs/nz7E5R3Fls=', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='computer-vision/overview-ocr. 8https://clova.ai/ocr/en. 9https://github.com/JaidedAI/EasyOCR. 10https://github.com/PaddlePaddle/PaddleOCR.\\n\\nOCR-free Document Understanding Transformer  23 \\n\\nFig. A. Examples of SynthDoG. English, Chinese, Japanese and Korean samples are shown (from top to bottom). Although the idea is simple, these synthetic samples play an important role in the pre-training of Donut. Please, see Figure 7 in the main paper for details', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 22, 'orig_elements': 'eJzFUl1P3DAQ/CurPLXSXb65S3hDHK2gUota+oQQ2sSbxJJjW7ZzcEL8967pUaGKdx4SxbMz493Z3D4lpGgmHe6lSE4hydumr7tKiH6otpu2FI0o6pO62NQtdTVRsoJkpoACAzL/KRmkIo0zRbEwegn3Fi251IohcmM5HOxLGa1Vsscgjc6OZYV6XHAkz/XbhPSY3DFqGbnXy9yRY7wsnxkL9BiiSW9muwRy67300cjsye0lPaxN71JophCsP82yXpk9pigzhjPSKbSvlVGGaelS9smuUAoSZ5fZBfrDj/OfKRT5O7RrFELR3/fxEMmx/9fRbmRQlHCf/+e5aSkf6qrum42oeiyHvOxOaqrbrqlO2k37EXlWb/PkSdaDI4Kd6ZfYOPzWgpwPqIXUI9w41H4wbiYHLIW3U1/ObPze1EXT9KLsiKo2LygvtlRui6rLh6LZdh19/NRf5JjCWQoXjzhbRR7MAL8OOkw785VRPSrppxWcT1KTpxVcocX4BZwKfDOOUIM/StER+Mk8aPg0ODNDMJYf6EwIZv7M16gwmWWcIEwE/MMhSA9eRvEqYuzq49UUZP/P1Co88GXANON4FwGcUSzXLy7W0To4lDpuiFvfxchSuFaEsVvP6+QJF25s+yqZmQ0voQJvEwSnL5V/u83v6BzHuqebGNLz3R8jAlql', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='A.2 Details of Synthetic Document Generator (SynthDoG)\\n\\nIn this section, we explain the components of the proposed Synthetic Document Generator (SynthDoG) in detail. The entire pipeline basically follows Yim et al. [67]. Our source code is available at https://github.com/clovaai/donut. More samples are shown in Figure A.\\n\\nBackground Background images are sampled from ImageNet [7]. Gaussian blur is randomly applied to the background image to represent out-of-focus effects.', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 23, 'orig_elements': 'eJzVUkuL2zAQ/iuDTy3sOqkdx/HetoSGPXR7aC4lhDCyR46oLAlJTjYs/e8deVsISyntqfRmf6N5fI/dc0aaBjLxoLrsDrJmSU3bFvW7YrGqq7lsiCRiXa+WRSFqqrIbyAaK2GFEfv+cSaXJ4ECpubNmjAeHjnzuOpnepnK8uKmMzmnVYlTWzH6UNZp+xJ4C13cZmT7bM+oYOZhxEOQZL8pvjEV6imnIfV7Amg9QOoCV8Pli4pGiamFt2zERgQ0Z8hithzdTdW03b9Oun2dsVdSU8czX3OuFXM3rZUVFVVWI80ZgWYkWF1QsFkz+n3CfIP8XBl2L9WAgHlWAQG3afANnAnpyGlUqELR2cNbw8EnLhDhvnQ3U/amwwJO6yY4cttzP75TnMcqRVoZAYGDaWl9AWq3tOcAXNQBFQG7YLet9Dp9GD8GOvk33dAR8L554IApNgBGOMbpwN5v1Kh5HkfPNs1bbE6KaTaLn8NHyyoCD08S96ftozyad9kH1I//f59cBeETPNNSJtkmmXwShWhQrEmXVVPO6nQtqUJREFS1LUTZNXf5/QXiP7dfe29F0cPWphrT1RbFJvQ6ktwM8JPyRTdolfzY4hqDQgNDsFLvj0XR2YEsnLtwU7ZQd8Wpywj05TyGlx47x1spbyWkKQFJyJMPvbNl/B0OShDU=', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='Document Paper textures are sampled from the photos that we collected. The texture is applied to an white background. In order to make the texture realistic, random elastic distortion and Gaussian noise are applied. To represent various view angles in photographs, a random perspective transformation is applied to the image.', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 23, 'orig_elements': 'eJxdUctuAjEM/BUrZ0Tp8gjbc6Wql6oHbgghk3iXiLyUZKEV4t/rbEGVekvGHs+Mvb0KsuTIl73R4gWEljRDPUdqFnqupWpbKSXOFzO9nD13SykmIBwV1FiQ+6+iM5Y8OhrJwQ9lHzFSmkbd1d5aLt9xLGOM1igsJvine9mi7wfsKXN9K8j3YsdoZGTvB3egxHgzH6H0Z7JdUatUI5+bxVouZ11L1CFKuV41zUHSUtyYUeir1ObXoIYaED6rL6jwkCgDD4SMLlrS0KXgoBwJ4jGUkPmJBS4EKlhLqpCewoardy4YZtcszCwB0MPlaArBAdWpT2Hw3P7uISRd9QI4PNE4/cFPhNbkYtQEEnrN2mSx/kEzHFLdEI/V8IZDzoYFfDCZRst3YTYUeE7kJDXbGZMJQ4azoQsze8sBjf+N0yeMxzwBfIjxGnLkWObMlhjLXUhuPMu/ZNWzcXyMab3V44wfmBJW8qZu+Lb7AWhDxvA=', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='Text Layout and Pattern To mimic the layouts in real-world documents, a heuristic rule-based pattern generator is applied to the document image region to generate text regions. The main idea is to set multiple squared regions to rep- resent text paragraphs. Each squared text region is then interpreted as multiple lines of text. The size of texts and text region margins are chosen randomly.', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 24, 'orig_elements': 'eJxNUstuGzEM/BVC59j1ax/pvbeg6MG3IDC4EndXgF7Ro60b5N9LqXaTiwTMcIZDSs9vggxZcvmilfgKYt4NkzwdurEfcJRHPD7uu9Nx3w04T7RTg3gAYSmjwoxc/yZmbcihpSpW3pV8CRgoboOaa22l8zU0GkMwWmLW3n250QbdUnChxPyzILeIF0YDIxdX7ESR8cOpQfEjJBIddv08DH0/jKrfSRz3w7yfjlKNErtOvLMi0+9ci898wxNefcmATsEPzJmig7MHq62WkFcC0/gE2kEkNJtfPhoFystSd5MeAGGlEnXKLIjF0GbCRArCzWwhRxGzj6ATtDmZzL55311AW56L/RdeQCVvIoIa9YanLZxZY5GTaEVY/bg0UQZbTNbBEKTXwttQd0XlI4UNH6m2aW68L1wihpUNv6Fc/4s+9WreK/HteIgQKTOP6aOR0Y4S+LmJ/gVL+g/dkdT2+dnQYlw0J+JOIFfPcSByjbfmuq3Pff8J3zHy4Pon1ccR7y9/AYHq3vU=', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='Text Content and Style We prepare the multi-lingual text corpora from Wikipedia.11 We use Noto fonts12 since it supports various languages. SynthDoG samples texts and fonts from these resources and the sampled texts are rendered in the regions that are generated by the layout pattern generator. The text colors are randomly assigned.', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 24, 'orig_elements': 'eJxFUU1P4zAQ/SsjnyHbZNM0cF2kvXEBiQNC1SSepNY6tmWPERHivzNO2+Vk6X3Nm/HrpyJLCzk+Gq3uQfV37dR3dbMf2rbth55qJN031O2au063Wt2AWohRI6PoP9VkLDlcqJi1d5mPAQPFKuipaAvNa9hoDMGaEdl49+tCW3RzxpmS8K+K3KzeBA2CHF1eBoqCN+0GxZ+SSNTsuulw6LpDr7vdiH19mOrh96j7Efd79SUOpg8u4md54Y93LH5Ap+GJV0vwQhAilVjgE8GSLZtba0odC8ULo4/BR4Qp+gVezD8TSBus6rp4cyJ49OxhkuRUN5CMGwkMQ8pBbJzgHaPxOcH/HSt4Wh2fHvxfSLgES2kblLZWW855lvSR9EjJ5zjSmS4dzyZ9NcWicZqiQMZtikizHFdiT8ibYCZHEVkUw7opLK4+MwRkpuiuvI8VPAt72dv6eMmX0X6xK2BKZnakq/Jp1/98xChe807lxOrr7Rt4OMl9', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='Post-processing Finally, some post-processing techniques are applied to the output image. In this process, the color, brightness, and contrast of the image are adjusted. In addition, shadow effect, motion blur, Gaussian blur, and JPEG compression are applied to the image.\\n\\nA.3 Details of Document Information Extraction', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 24, 'orig_elements': 'eJzVUstuHCEQ/BXEebPZ2XnnFsmOlRwiH3yzrFUDzSwRA4RHYsvyv6cZ24pl5Qdyg6rq7qKa20eOFld0+WQU/8T4QUsxj/MghNLDNDdq6psOxrmdD3LqhyPfMb5iBgUZSP/ItbHoYMVarLwr+RQgYNwHpau20vkhbDSEYI2EbLz7+EJbcEuBBRPxtxzdwu8IDYScXFkFRsKP3QbFvyYB8XgY9DgOwzip4SBhakbdiFaqSULf8yeqyHifq/jap/whRC8xJeMW9sU4sPZhx5JfkYV3bEZ5duZnwcRoItsso2LZs3xG5ksOJTOzksE9++oINIm9lO82ifTWxx0T0Szn7DYYnCLY5QgpM6832dbieYT6UVJGtfUDpUzNh9ydQfnfDLVGmXds9RVmwhZqfgWF3MLrtfb/dn15RUPWEOtDSPkP98+2a+yvG/kOMdI+fuFNDYtSe/8dBBxHOTYo2m4CMbUd9jOMHQw4CTm38P99h8/7ll2QY2NT3cWFl6W+l8LXPq6bG3Z5T7uS9fg2rBuTLfKnuz/Gfw21', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='Information Extraction (IE) on documents is an arduous task since it requires (a) reading texts, (b) understanding the meaning of the texts, and (c) predicting the relations and structures among the extracted information. Some previous works have only focused on extracting several pre-defined key information [12]. In that case, only (a) and (b) are required for IE models. We go beyond the previous works by considering (c) also. Although the task is complex, its interface (i.e., the format of', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 24, 'orig_elements': 'eJxdU8mO2zAM/RXCpwRI3WbP9FagOeTSSwv0MBgMaImOhbElVUsWDObfS8oeTNGTFj6S7z1Kj68V9TSQTc9GV1+h2q2brV6tHmiz2+n9TtG23a+WpLHFRn/ZHqoFVAMl1JiQ8a9Va3qyOJAka2dzevboKdRet4KVcLr7Ekbve6MwGWc/T+Ee7TnjmSLHHyuy5+qJbz3fPNs8NBT4frUpV+GDZIOrvdovqVlvDtgc1hvaPuB+gzs6NOphjdUbZyS6JQGfbOvCULrC8ZYCqrKdnY5z4FU7lUV/BBMBLWDQ2eUICeMLRGMVgUkQ6E82gSLMcM4H1MaeQTrEBcyaOWSrKcSEdgx0BAOhlb1ry3HCMgBmag4+kDZMZAIH6gvBWAAxhaxSlnY4uAlCI3XSYD4E1fDTDSTVLkZIX114idDhhVhaf4eWxUVOYZ1TvnSMdKGAvaR90tQay4gXuv9bGB6Xq6caTpZ7YwKFkRZjSTGgqGDVPJN3ZzT3CnA6wuA09bGG3wRnBw3dHYNFwH8kmzsoFmzYN+EkpmAfXQ3f+tS5fO5G22QKPBjlBt/TbcGz4EHZRKFFnszM1FQvCnKkLnYb63MqHF1OvJ1LgWikwKRIzlSewaLQppvnhzgK9zl4F4lndSWInbtCFIsjSj6YQR4rzK6dUV3RX8aH16nrNG32wRtPPVs7h6tJo5iRjmC+y0ep5QO8/40fGAITuNAvebZvT38B5m491g==', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='input and output) is simple. In this section, for explanation purposes, we show some sample images (which are the raw input of the IE pipeline) with the output of Donut.', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 24, 'orig_elements': 'eJxdU8mO2zAM/RXCpwRI3WbP9FagOeTSSwv0MBgMaImOhbElVUsWDObfS8oeTNGTFj6S7z1Kj68V9TSQTc9GV1+h2q2brV6tHmiz2+n9TtG23a+WpLHFRn/ZHqoFVAMl1JiQ8a9Va3qyOJAka2dzevboKdRet4KVcLr7Ekbve6MwGWc/T+Ee7TnjmSLHHyuy5+qJbz3fPNs8NBT4frUpV+GDZIOrvdovqVlvDtgc1hvaPuB+gzs6NOphjdUbZyS6JQGfbOvCULrC8ZYCqrKdnY5z4FU7lUV/BBMBLWDQ2eUICeMLRGMVgUkQ6E82gSLMcM4H1MaeQTrEBcyaOWSrKcSEdgx0BAOhlb1ry3HCMgBmag4+kDZMZAIH6gvBWAAxhaxSlnY4uAlCI3XSYD4E1fDTDSTVLkZIX114idDhhVhaf4eWxUVOYZ1TvnSMdKGAvaR90tQay4gXuv9bGB6Xq6caTpZ7YwKFkRZjSTGgqGDVPJN3ZzT3CnA6wuA09bGG3wRnBw3dHYNFwH8kmzsoFmzYN+EkpmAfXQ3f+tS5fO5G22QKPBjlBt/TbcGz4EHZRKFFnszM1FQvCnKkLnYb63MqHF1OvJ1LgWikwKRIzlSewaLQppvnhzgK9zl4F4lndSWInbtCFIsjSj6YQR4rzK6dUV3RX8aH16nrNG32wRtPPVs7h6tJo5iRjmC+y0ep5QO8/40fGAITuNAvebZvT38B5m491g==', 'is_continuation': True, 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='In the main paper, we test four datasets including two public benchmarks (i.e., CORD [45] and Ticket [12]) and two private industrial datasets (i.e., Busi- ness Card and Receipt). Figure B shows examples of Ticket with the outputs of Donut. Figure C shows examples of CORD with the outputs of Donut. Due to strict industrial policies on the private industrial datasets, we instead show some real-like high-quality samples of Business Card and Receipt in Figure D.', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 24, 'orig_elements': 'eJx9UkFu2zAQ/MpCpwaw1ViWLbnH2AjQSwsEuRmGsRJXEmGJZMhlnCDI30uycd1DkOvMznJmuPu3jEaaSPFRiuwHZFQscL28rbrV+paKVbepi2JdNHVZFZ3oNm02g2wiRoGMYf4t6+RICieKYqGV56NBQzY3oouzkeZXk2g0ZpQtstTq+wc9ouo99uQCv89I9dkhoCYgR+WnhmzAizJB9mqywaJqqwU1y7LGpl6WtNpgVeKa6qbdLDF7DwqmF47DPxXwQDChVJCszeBMwOQYOu0txCSO2IFU7eiFVD3wWYPxTTALDal2mNCeHHyTOeUz2P5+2MG+XB0AlYBH2Z6IYb8oDjcJSForn5EpbBTesZU4Xl/52HLnnZyDIudgi1Yk6QO1JA3f5HAve28J7sAN+uyAXnAyIznQ3eXBs+Qh5dKejedE7WL9/8TbT8TJ+xfSnQ/NaIieW/7fvtGhDBmX/G3zi4SpXqkcE4pkAZyeCCzhOB/liWCQ/TB/8jhKfgV3NRcr+bSQsO0SapfHq7kc1C+0NpzTMz3Gv34//AFLjPLW', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='A.4 Details of Model Training Scheme and Output Format\\n\\nIn the model architecture and training objective, we basically followed the orig- inal Transformer [58], which uses a Transformer encoder-decoder architecture and a teacher-forcing training scheme. The teacher-forcing scheme is a model training strategy that uses the ground truth as input instead of model output from a previous time step. Figure E shows a details of the model training scheme and decoder output format.', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 24, 'orig_elements': 'eJzNkk2P0zAQhv/KKOdtSfMtbkiwEgfgQG/VqprY48TIsSPH2VKt9r8zTijtlgNXTrbm85l35vCSkKGBbDhqmbyHBOtMpTKviirfVU1a7xDTDJtUpdiqokiTB0gGCigxIMe/JEobsjhQTJbOzuE44kh+O0oVY6M7nMfFjeNotMCgnX33223QdjN2NLH/kJDtkie2jmw52nloybM9KxaTv4EkytJK1XVV1Y2sUoHNrla7NheyEViWyStnBPoZYvCHbQEfmVibCZyCL06Sgb1HbbXt4LvoeXxAK+HbHMY5wKPzA4YId+He62Ao1rwXS6aZqrJi1yrMS2KxZJuWtahL0VStyOn/EOtfG70V67OF0BMMi0joRa8DiTD7VaFwUc21P9isn+kBTgQtTkxqzBmUM8adSC5FnNfdBrTFRW47KRaWPBzKhrFPvRY9zBNNgG/cZAU39xtJy/s3BEIg5K35DWeISPMHa1qWuYU9d78PWn2gY791vGsa/wJ1Z6bGsDJF/s67eRl6Dj3gxJPE89B24tIyntJaxq1no7wbuPTo6Vm7mSto7sah4xYedRfhP8HUu1PsL6/neFX7bopl1osIlx7LaW5vb/MreobnTezjBl+ffgEw8Ea2', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='11https://dumps.wikimedia.org. 12https://fonts.google.com/noto.\\n\\nOCR-free Document Understanding Transformer  25 ', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 24, 'orig_elements': 'eJzFkM1OwzAQhF/F8hmSOGnspFe4cEJC5VRV1cZeB4v4R44jQFXfHaeiouIFuM43s5qd/YnihBZdOhpFt4SiavqBQd9x6LnQ2A6bbgN1o/qm1xUKekeoxQQKEmT/iWozoQOLa1h5t6RjgICxCEqv3hWnr3DBEMJkJCTjXfmDJ3DjAiPOme8pupEeshqycnSLHTBmvd5cpPhbEhDrimshOBed4pWEjgnNhkaqTkLb0nNOJPxMq5mxt5TCvC1LtdgwFx/m3VhUBgofx4Kw+oq1d2kuRu/HCQvpbel88sXa8vrAzqQJ1+N/VxNYtZyJSvWtHoBLzCX6iino8oZMy/9Yrb0d4fnh5V5HRPLo5bIWJ69OYZwTOGXcSHYR3Kx9tBhJjpLbr59sPkzPh2/zwLDa', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='e 100', metadata={'text_as_html': '<table><tr><td>G79?§Eﬂ Futiaz 074 WeD”</td></tr></table>', 'filetype': 'application/pdf', 'page_number': 25, 'filename': 'donut_paper.pdf', 'orig_elements': 'eJw1js1qwzAQhF9F6FwSWfFPHUx6SdsXKPQQB7GOVq5AUoSzgrYh714ppIedw3yzwxyuHB16DKSs5lvGm6aVGymNMbWuK1nr5xYm0Yuuk/2mnU78iXGPBBoIcv7KjXUYwGN51ueQSEWIuKyiNiVbMP3EO4YYnT0B2XNYP7CDMCeY8ZL5gWOY+TG7MTsqJD/hkn3ZZIvwmxRc1Bd5V7oGgsnhbqAln969d/3LmISA7nVMZhKSvSWy8MtEV7NP3I9JikoP6xzNshS5//Pbo7t0IquEKKv+B3+UDL8d/wC18Vqq', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='Fig. B. Examples of Ticket [12] with Donut predictions. There is no hierarchy in the structure of information (i.e., depth = 1) and the location of each key information is almost fixed. Failed predictions are marked and bolded (red)\\n\\nA.5 Implementation and Training Hyperparameters\\n\\nThe codebase and settings are available at GitHub.13 We implement the en- tire model pipeline with Huggingface’s transformers14 [63] and an open-source library TIMM (PyTorch image models)15 [61].', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 25, 'orig_elements': 'eJzFU01v1DAQ/SujnFqphHxudpE4gKBsD0UcInFYVtXEHmetTRzLdkpXFf+dcZZKBSEkTr1Z8/HezJvn3WNCA41kwp2WyRtIum5VbfJCrqgsqxJVRqohIrHKMtURNskVJCMFlBiQ6x8TpQcyOFJslpOZw51FSy61UsXamA4nu6TR2kELDHoyr3+lBzT9jD15zu8SMn2y56jlyJ2Zx44cx4v6B8cCPYQIcq37FN6n8PEBRzuQh0lBq8WRAuzyYg/fdTjAhzgIWEdSi0jnU2gP5Ai0BzPBQZNDJw4n0AbCgcAHN4swcwGjaaMmNy5jwoVOKb0CSZZR30J+CWjk0jJM501iB6E4wJFOv7UyFQ7j5AMo/UAyhWtkLeTzqQCZcUR35HDE7aZB8vOCKy6jOk/CfUbnGPSe2igCq/Hn1SqsMpE1cq2qkpqNRFGs1grLteDLNZJe/Grv0hpu4r3i0Gd94sKtQ2206WHLZM6i45ECOf98+VaHgf62dF0VVG8aoVSB+bqmWmVrgUrlaiWbRtUvsvQScv9xmecqsUdBTJI69LTo4ykEludsFLxnA2E38CvAJx22c5fmJXxlVz8puziTzCsIOjqLoQaw2tKgDZ1/xnbue0ZUKOjbXGT5xkNwaHz0LQufV7BblfuFHNnblsH8NDvBhtcdf5oTtDe3t3Dx5dRO/IOYmgU4M/nLvObufJ/+w7v7n9Knd8Y=', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='For all model training, we use a half-precision (fp16) training. We train Donut using Adam optimizer [30] by decreasing the learning rate as the training pro- gresses. The initial learning rate of pre-training is set to 1e-4 and that of fine- tuning is selected from 1e-5 to 1e-4. We pre-train the model for 200K steps with 64 NVIDIA A100 GPUs and a mini-batch size of 196, which takes about 2-3 GPU days. We also apply a gradient clipping technique where a maximum gra- dient norm is selected from', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 25, 'orig_elements': 'eJxlU01v2zAM/SuETxsQZ3LiOMluBboNwYBih247dEVBW3QsVF+T5LVZ0f8+ylmafRxFPj4+PlI3TwVpMmTTnZLFWyjaBVGD7bbZVFW97Ku6ktiJupYtUrtpV8UMCkMJJSZk/FPRK00WDeVi6eyY7jx6CnMv+4zN6XTwUxq916rDpJx98zut0e5H3FPk/E1Bdl/cctRz5M6OpqXA8cVqCoWzyBpr0Ym13PT1ktZbVrhoNj0uNx11zVpS8cwViR5TBr93AVBrME6ShhRQWWX3M3ggGCMBwoC6L32gTkVWBq96XzWvX4Bz+ErHB1zm8biIo3Ah0YDzSRn1kwLcLMUttAeQ1AXCCZEGAk0YMgkETNwqTsETM/jgStgHipHiHK45xfGkUP9T53qGUvlSpyJESpAcVFTWgFYyL6aM65WlEtJ4xmnqEknogzMZvjqVTXO90E7Cjg717NdCiI8QE/kIDyoN0NRw9WV3ubuAi0oI+PDpc5zaIhiWVLaYugEiO5E1VNuG3R0UhxLeEyNbx74tymUuBImHODVHHR3kmzgwzz6gVLxh6LTyfvKPusGq7yMxF4W8KIOPyowmY0s4oq0L5v85xVwc55yLk6+eFbDTTo/5/LLM4zbPXi5Wjfg2CiHX1XYhgP3MnvxlvB8w0hx2FqR7sDHxqg3PGO/jbAKf25RwbMTDs/TO2RSc1iSn4ugM/ckgXTfmLwi7d0CP/HlUfjFnHNlDjLP8U06f6AoDH4X6Qdf5vp9vfwFZdkLc', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='0.05 to 1.0. The input resolution of Donut is set to 2560×1920 at the pre-training phase. In downstream tasks, the input reso- lutions are controlled. In some downstream document IE experiments, such as,', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 25, 'orig_elements': 'eJxlU01v2zAM/SuETxsQZ3LiOMluBboNwYBih247dEVBW3QsVF+T5LVZ0f8+ylmafRxFPj4+PlI3TwVpMmTTnZLFWyjaBVGD7bbZVFW97Ku6ktiJupYtUrtpV8UMCkMJJSZk/FPRK00WDeVi6eyY7jx6CnMv+4zN6XTwUxq916rDpJx98zut0e5H3FPk/E1Bdl/cctRz5M6OpqXA8cVqCoWzyBpr0Ym13PT1ktZbVrhoNj0uNx11zVpS8cwViR5TBr93AVBrME6ShhRQWWX3M3ggGCMBwoC6L32gTkVWBq96XzWvX4Bz+ErHB1zm8biIo3Ah0YDzSRn1kwLcLMUttAeQ1AXCCZEGAk0YMgkETNwqTsETM/jgStgHipHiHK45xfGkUP9T53qGUvlSpyJESpAcVFTWgFYyL6aM65WlEtJ4xmnqEknogzMZvjqVTXO90E7Cjg717NdCiI8QE/kIDyoN0NRw9WV3ubuAi0oI+PDpc5zaIhiWVLaYugEiO5E1VNuG3R0UhxLeEyNbx74tymUuBImHODVHHR3kmzgwzz6gVLxh6LTyfvKPusGq7yMxF4W8KIOPyowmY0s4oq0L5v85xVwc55yLk6+eFbDTTo/5/LLM4zbPXi5Wjfg2CiHX1XYhgP3MnvxlvB8w0hx2FqR7sDHxqg3PGO/jbAKf25RwbMTDs/TO2RSc1iSn4ugM/ckgXTfmLwi7d0CP/HlUfjFnHNlDjLP8U06f6AoDH4X6Qdf5vp9vfwFZdkLc', 'is_continuation': True, 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='13https://github.com/clovaai/donut. 14https://github.com/huggingface/transformers. 15https://github.com/rwightman/pytorch-image-models.\\n\\n26  G. Kim et al. \\n\\nFig. C. Examples of CORD [45] with Donut predictions. There is a hierarchy in the structure of information (i.e., depth = 2). Donut not only reads some important key information from the image, but also predicts the relationship among the extracted information (e.g., the name, price, and quantity of each menu item are grouped)', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 25, 'orig_elements': 'eJzFUl1r3DAQ/CuLnxq4+Gzf6c4X6NOlLaUPhZK3cIS1tLZFrY9KcpMj5L93ZRJISt7zZJgZj2Z29/axoIkM2XSnVXEFRYuVoKrZ1qKv66ptqKeuFrTf7sVmLyUVKygMJVSYkPWPRa8nsmgo/6ycndOdR0+h9KrP2kyns19o9H7SEpN2dv1MT2iHGQeKzN8WZIfixKhn5M7OpqPAeCOeGEv0kLJJvRlT8vFqvR50GueulM6s5eT+Iur1EqCEevuOZpyHQduhR0nrFNDG3gVDIbJcvCMP93oYk0GOek4uyPFSG451aZyiKZY5/EuvG50mKjjk/8Psuk5Sf2hwv232bSOopq5tleyF3FFF1UcMc/d6mM0O4FsJP7QBSoBTCa97fc+F3+tVKyn6tmoaPKDAw77p2l0vNt1mc+iqvhYf3uurHko4lvDlAY2fKILr4fjz1zXcbsUJ7nnLcJ1jgA+ktMyP8R3cjBQIdASEUVNAXvoZtIU0EsQUZplm5tlK23w7S0b4pEsqV6DIs+lnaC7KZ2vrEjg7nSEQqgjRGfY23oWENsFvOr+x6YMzy0PLla2gm/M+ontJGBcy0LTI46g9oHF2WGBuHVAmUm+TUTlwsizIs1+xlZb8Qavgz8whdDrnNoRyBN7uDDqRAeSOQ3CzJ3Xx+hp4plz/iD6bF0+nf7MbZlE=', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='CORD [45], Ticket [12] and Business Card, smaller size of input resolution, e.g., 1280×960, is tested. With the 1280×960 setting, the model training cost of Donut was small. For example, the model fine-tuning on CORD or Ticket took approximately 0.5 hours with one A100 GPU. However, when we set the\\n\\nOCR-free Document Understanding Transformer  27 ', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 26, 'orig_elements': 'eJzFUstu20AM/BVCZ0WVFEuWe2sd9HFpisBBDq5hUFpKXmQfwi43dhrk37vrpoDbH+h1ZkgOh9y+ZKRIk+G9FNl7yJq2olog1f3QtKtFP4i6K+teUD9e92IlshwyTYwCGaP+JRulIoOaUrGwJvB+xplcMYsxaRPNz/OZxnlWckCW1rx7oxWaKeBEPvLbjMyU7SI6R2Rvgu7JRbxuXyPGdOLUZH17dwPbRRN1Gzk8EsO2qneARsDH4KUh72GNTuTgNSpFDrz8SWBHkGYODI68VSF5yIGKqcihihv+CGUplqu2zEF6YPJMooAHyQfgA/0tAU/M0kz5mdJWkAJ2KE3EYLCe07CbFAUc0f+2UcAn64BOqGdFl4VjdHzF4VxrDZy3i8q31djaR4ixOXuSGpnUM5RFAwcbnIdjcmcNwYeqLOHz9/sCvtgjPZHL4XggA0dKVtOwFPWfK3xD5+INnmiTEo3R/vsC1C6rshq6bqzqul50FQ7NqscljXVD3fX4P15gefkCt+u7q9ERxZCHkIzDvRHkPMcnSDluHBo/Wqfj8WMpXG7/VcfG2evuF8Nu914=', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='2560×1920 setting for larger datasets, e.g., RVL-CDIP or DocVQA, the cost increased rapidly. With 64 A100 GPUs, DocVQA requires one GPU day and RVL-CDIP requires two GPU days approximately. This is not surprising in that increasing the input size for a precise result incurs higher computational costs in general. Using an efficient attention mechanism [60] may avoid the prob- lem in architectural design, but we use the original Transformer [58] as we aim to present a simpler architecture in this', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 27, 'orig_elements': 'eJxNUttq21AQ/JVFz44q27Ed9c00UAqlTYuTPrjGrKWVtPTcei6+NOTfs0dp0oJAsDs7O7Nnto8FKdJk4p7b4j0UbUu4oukMpzjvVnW9oGucTyu6qev5rF40xQQKTRFbjCj4x6JjRQY1jcPWpLh36MiXru0yNrfjxY1tdE5xg5Gtefe3rdD0CXsK0t8WZPpiJ1Unlb1J+kBe6rPVk9QinWMmmS2W1c9UVe1qWs8qCBQjmx4660Gh78lDliblMAEq+3IC3x8+X324/XQHArm1zcO39QTiQNDYEIFN40ngLXh03KpLCT84DrC8hvW0quDj3b0QvYyBp9+JPQWwhnJHVl0ATftvxRsinuwrIoAY9/bMGiPlBZuBA8hnbISQvPMcsgU2IgvfJOVSlsnGJcHxHxpNIjhPDQeSXSGpEZ58gIH7Qcw3Vgt8vDGq0WLIxD0Z8qhKuB950QB1HTcsDw8Yo/xkADQ1AxoOGrbLagc6uztabkcdYuFwBRKWzIe+GThSE5OwQkuBezOBgwg9ESQRlyes556zjI1HE0S8FoHbxc0OMGQcsoZos58w6hCT2inB/MdOL2eRa52s/1XC1+TzgGItzP4CdJawcQ7w6DNoVJlBKG3yjTwECgUekRUe1Ei2do5My2dYl8syZ/A1nl/QeznckTY5ak+7Z0q6EXg=', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='work. Our preliminary experiments in smaller resources are available in Appendix A.6.', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 27, 'orig_elements': 'eJxNUttq21AQ/JVFz44q27Ed9c00UAqlTYuTPrjGrKWVtPTcei6+NOTfs0dp0oJAsDs7O7Nnto8FKdJk4p7b4j0UbUu4oukMpzjvVnW9oGucTyu6qev5rF40xQQKTRFbjCj4x6JjRQY1jcPWpLh36MiXru0yNrfjxY1tdE5xg5Gtefe3rdD0CXsK0t8WZPpiJ1Unlb1J+kBe6rPVk9QinWMmmS2W1c9UVe1qWs8qCBQjmx4660Gh78lDliblMAEq+3IC3x8+X324/XQHArm1zcO39QTiQNDYEIFN40ngLXh03KpLCT84DrC8hvW0quDj3b0QvYyBp9+JPQWwhnJHVl0ATftvxRsinuwrIoAY9/bMGiPlBZuBA8hnbISQvPMcsgU2IgvfJOVSlsnGJcHxHxpNIjhPDQeSXSGpEZ58gIH7Qcw3Vgt8vDGq0WLIxD0Z8qhKuB950QB1HTcsDw8Yo/xkADQ1AxoOGrbLagc6uztabkcdYuFwBRKWzIe+GThSE5OwQkuBezOBgwg9ESQRlyes556zjI1HE0S8FoHbxc0OMGQcsoZos58w6hCT2inB/MdOL2eRa52s/1XC1+TzgGItzP4CdJawcQ7w6DNoVJlBKG3yjTwECgUekRUe1Ei2do5My2dYl8syZ/A1nl/QeznckTY5ak+7Z0q6EXg=', 'is_continuation': True, 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='For the implementation of document IE baselines, we use the transformers library for BERT [8], BROS [18], LayoutLMv2 [64,66] and WYVERN [23]. For the SPADE [25] baseline, the official implementation16 is used. The models are trained using NVIDIA P40, V100, or A100 GPUs. The major hyperparameters, such as initial learning rate and number of epochs, are adjusted by monitoring the scores on the validation set. The architectural details of the OCR-dependent VDU backbone baselines (e.g., LayoutLM and', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 27, 'orig_elements': 'eJxdUmFv2jAQ/SunfNoklkFKoNs3OtiE1FFEKdMUIXSxL+AusS3boUVV//vOoR3Svlj2O7+7d++ueEmopoZ02CmZfIVkOO4PM8ry/PoKq7wilCVimYtM5l9ELoZJD5KGAkoMyP9fkkrVpLGhSJZGt2Fn0ZJLrazi3xgOJ9uF0dpaCQzK6M9v4Rr1vsU9eY4XCel9smXUMrLTbVOSYzwbvzIW6DnEJN+Ng3AgUI09C+/ygalAGtFGAOYzKNFTrTT5HjwRtJ46TnCofWVcQ85DrUqH7gT8hpvZag3FNde+Wd3dQzGI11s8mTbc/jxmUIyGvdFoC6gl/Pq9ma0WUGRX2xTe1dwvJ9MZY/n2X+leFzBVpYTC+j+9gxEoH3XJFNb8rTGSag/oOpHMlhxUeg+LzXw6n8By2O/BZtDnkytO+AI/lg/+jYyPDB7YZWfR8SwC99cD34oDoAelVYgKakKnY06HgbpWzhZH78gacWBOFIDysfWBFZQn1sVk4yIrduOFceSB/Y6vI9ZKnu33FM5a0ImDCiRC67ik5E1R3BdXiIS7b6tPkixpGce0mT6wWeJPaTRdBgYfKN2nF/c7oZdRfDxLPHJaLGteAw0TGzOqZ5ik4zQu1fu+LdBxq+pI67g7r9u/aw/7Yg==', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='LayoutLMv2) are available in Appendix A.7.', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 27, 'orig_elements': 'eJxdUmFv2jAQ/SunfNoklkFKoNs3OtiE1FFEKdMUIXSxL+AusS3boUVV//vOoR3Svlj2O7+7d++ueEmopoZ02CmZfIVkOO4PM8ry/PoKq7wilCVimYtM5l9ELoZJD5KGAkoMyP9fkkrVpLGhSJZGt2Fn0ZJLrazi3xgOJ9uF0dpaCQzK6M9v4Rr1vsU9eY4XCel9smXUMrLTbVOSYzwbvzIW6DnEJN+Ng3AgUI09C+/ygalAGtFGAOYzKNFTrTT5HjwRtJ46TnCofWVcQ85DrUqH7gT8hpvZag3FNde+Wd3dQzGI11s8mTbc/jxmUIyGvdFoC6gl/Pq9ma0WUGRX2xTe1dwvJ9MZY/n2X+leFzBVpYTC+j+9gxEoH3XJFNb8rTGSag/oOpHMlhxUeg+LzXw6n8By2O/BZtDnkytO+AI/lg/+jYyPDB7YZWfR8SwC99cD34oDoAelVYgKakKnY06HgbpWzhZH78gacWBOFIDysfWBFZQn1sVk4yIrduOFceSB/Y6vI9ZKnu33FM5a0ImDCiRC67ik5E1R3BdXiIS7b6tPkixpGce0mT6wWeJPaTRdBgYfKN2nF/c7oZdRfDxLPHJaLGteAw0TGzOqZ5ik4zQu1fu+LdBxq+pI67g7r9u/aw/7Yg==', 'is_continuation': True, 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='Fig. D. Examples of Business Card (top) and Receipt (bottom). Due to strict industrial policies on the private industrial datasets from our active products, real-like high-quality samples are shown instead\\n\\nA.6 Preliminary Experiments in Smaller Resources\\n\\nIn our preliminary experiments, we pre-trained Donut with smaller resources (denoted as DonutProto), i.e., smaller data (SynthDoG 1.2M) and fewer GPUs\\n\\n16https://github.com/clovaai/spade.\\n\\n28  G. Kim et al. ', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 27, 'orig_elements': 'eJzVlE1P3DAQhv/KKKdFWrJJNl/LrYUWoaoVKvSEEJrYk12rie3aE5ZVxX+vzUe7rbhwQr1F8xW/zzv21c+EBhpJ842SyREkRZkX2GdVtypLlCV1PeZt1olVu5S07EUyh2QkRomMof5n0quBNI4Um6XRE99YtORSK/tYG9O8sw9ptHZQAlkZvXhKD6jXE67Jh/xVQnqdXIeoDZEbPY0duRAvmvsQY7rjOOSjWqdwksKHOxztQB5MD+8nrzR5D8foJMzY2ANALeErCVKWYdYZZjMehMaJgA14dkowKC2n+IkDWBOOpuI4DbwhsE7dItN+SZTsiT30zoxgJgcoWN3GWiMnwX4OjnA4HNR3go1abw5/TDgo3oF/Oio6Ar8xWx3GeiaUEcEznSBscnSMNvJJguR/ranLmgQ2os/6us3rrBWN7Nu8KFa9FHVXvbk179Iazh0NalQa3S5YFP6mogIfBMPFiMNALrjiAzwRBu+pv1Q80EuqBTWFKOuwh01dNsu2L8smz1a4XMqyqovVm6h+CLlXWLOP6Uw/LI/dI0V/SM1hGzeKDtlhWGoJJ1EDbBVvwD8RdM8EYSZJGw5V6B8Lz51hczAHlVI6/90Q4cDsYqd5c2JOIU+Lz483pKdtSJ+ef/vLjC/oHMbVvoxHfsGUKqvaplpS23ZVtcSuzkSNlZCYd3V4L/5DU/J6w2z90WKxDqSnLhVmXIjB3CKqhbcoKX0dIdGJrsxykWdZQx1hUS37XoY3tSrbomrfglC7L7hoAU5T+KRGIAYcUtjXdzaG1uT++he+f/vj', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='Fig. E. Donut training scheme with teacher forcing and decoder output format examples. The model is trained to minimize cross-entropy loss of the token classifications simultaneously. At inference, the predicted token from the last step is fed to the next', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 28, 'orig_elements': 'eJwtkM1uAjEMhF/FypluBS1ly61S22NP3BBCJnGWqPlT4m2hiHevs3D0fPbYnu1FkadAkffOqDUoi5rmi9fnVW/ndnno6WnV42q5nNuX3lhr1QxUIEaDjNJ/UdZ5ihioDZsUR95nzFS6bKbehvmcJ4w5e6eRXYqPd+wxDiMOVIVvFcVB7UTNouzjGA5URF/0V9GYTtxMPt3QwUcH720XcEEXXRyg6qO8Ab+Oj8CEUhWwqejGMBowpJMRLY2cZU5QQAY6YcieagebI0GQDg+u3lzJACcIYh/cH4EuqdYHCaqkfAYvBSQLLGOcvimC9lirs/f/KlQXRs8YKY3Vnzt4Y3DRUqGoaTbN5ULGaZ72NAdbUpiAODFUptxusbc7mh5bBC2Ke55fWIps+6FNA9fdP5RWn3c=', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='(8 V100 GPUs for 5 days). The input size was 2048×1536. In this setting, DonutProto also achieved comparable results on RVL-CDIP and CORD. The accuracy on RVL-CDIP was 94.5 and CORD was 85.4. After the preliminaries, we have scaled the model training with more data.\\n\\nA.7 Details of OCR-dependent Baseline Models', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 28, 'orig_elements': 'eJzFkd1uEzEQhV9ltFcgpYv3Nw53JZFQJaBRFHpTqmjWns1a8not25sQqr47dqEV8ALc+OKcY/vMN/ePGWkayYSDktl7yApedCtipay6tiwrRjWvWtYXrGlLqiqZLSAbKaDEgDH/mPVKk8GR0mU5mTkcLFpyuZV9yiY7XOyzjdZqJTCoybz7bWs0xxmP5KN/n5E5Zg9RtVE5mHnsyEW95E9RC/Q9pEfecLgrGIOP268e+slBAxIv/m0O+4FAGTsH8OoHwRk9lKzm32bG5LJoqjaHGwNhUB48haDMcQGb1HjrpjABah8PMSg6kQQxjRYddprAkZ918DAZ2N19ulpvbraARsL6drf59SsKMTsUl78i6f9VnTev2WeFN3mdw3UfyMUqBNaRVqMy6BT5BZwJBjwReIE6tkiJcZKkIThUJlaGswpD1BxBWkGeIL7w/YLORbon2idWEdq/y2UclyUVgi1FWa/aqudNLUS96pEz7Ej89+Ve50vYxAJKR9w93K53V5IsGRlngA/oIytD8DkR8X9OvldBU/b08BNzSuYY', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='In this section, we provide a gentle introduction to the general-purpose VDU backbones, such as LayoutLM [65] and LayoutLMv2 [64]. To be specific, we explain how the conventional backbones perform downstream VDU tasks; docu- ment classification, IE, and VQA. Common to all tasks, the output of the OCR engine is used as input features of the backbone. That is, the extracted texts are sorted and converted to a sequence of text tokens. The sequence is passed to the Transformer encoder to get', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 28, 'orig_elements': 'eJxNU8tuGzEM/BViz05qb/xsT0XaQ4C0RYs0F8MIuBLXFrIrqXo4SYP8e0ntOu7JEDkznCG929eKOurJpgejq49Q1ZvZrF0sV4t6StjUqqmblZ5dbWoi1czUsppA1VNCjQkZ/1q1piOLPQlZO5vTg0dP4dLrVrDSTi++tNH7zihMxtkPY7tDu8+4p8j9bUV2X+246rnyYHPfUOB6vS6lcDY5XeOqppmarlQ93yyv2vVirtR80+J6ig2p6o0ZiZ6TgG8spIOJEEnJ5Ak8EfjgjkYTIOxZtSMwNgWnc0FAcswgaVHA7sLn4F0kuP/yGxpUj42zFCcQszoARrjFF5fT7TfYLhc7QKvfK8eaa/PdJdw5aAiiJ2Vao4oDevYdGgsH91SGKWePbIXHY3eeArzJ1oUetHuyMQXCvthIGB/jJ66qfAFyPVAdxijqOIS8+TopXu5/fr6Ea9f3Qy7suoE8KVPZps8JXFteP65/AZ/AWN5HhBxJSz5jBdISphzY0Yg9WeRwB0yMHwR55wFVYqasPwJfDaILUhA3JWV5iRc+yZ9MVlERZTxXH8lG0aRzk714DjeQZMhdQBtlLRTYr3Kaf7m1pyQDRChjZ/4yYcx35NO7MOqOj2Itj6pCFWVN0YR33oDXpm0pDE4sEPLVZYOM9WQ1L8RKks7sDwl6p99vIA3RHPZ3jhpOjZwMuxyx7f/3OPmVL+T08XzHEBh7pDv5X7/t/gGAO0s3', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='contextualized output vectors. The vectors are used to get the desired output. The difference in each task depends on a slight modification on the input sequence or on the utilization of the output vectors.', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 28, 'orig_elements': 'eJxNU8tuGzEM/BViz05qb/xsT0XaQ4C0RYs0F8MIuBLXFrIrqXo4SYP8e0ntOu7JEDkznCG929eKOurJpgejq49Q1ZvZrF0sV4t6StjUqqmblZ5dbWoi1czUsppA1VNCjQkZ/1q1piOLPQlZO5vTg0dP4dLrVrDSTi++tNH7zihMxtkPY7tDu8+4p8j9bUV2X+246rnyYHPfUOB6vS6lcDY5XeOqppmarlQ93yyv2vVirtR80+J6ig2p6o0ZiZ6TgG8spIOJEEnJ5Ak8EfjgjkYTIOxZtSMwNgWnc0FAcswgaVHA7sLn4F0kuP/yGxpUj42zFCcQszoARrjFF5fT7TfYLhc7QKvfK8eaa/PdJdw5aAiiJ2Vao4oDevYdGgsH91SGKWePbIXHY3eeArzJ1oUetHuyMQXCvthIGB/jJ66qfAFyPVAdxijqOIS8+TopXu5/fr6Ea9f3Qy7suoE8KVPZps8JXFteP65/AZ/AWN5HhBxJSz5jBdISphzY0Yg9WeRwB0yMHwR55wFVYqasPwJfDaILUhA3JWV5iRc+yZ9MVlERZTxXH8lG0aRzk714DjeQZMhdQBtlLRTYr3Kaf7m1pyQDRChjZ/4yYcx35NO7MOqOj2Itj6pCFWVN0YR33oDXpm0pDE4sEPLVZYOM9WQ1L8RKks7sDwl6p99vIA3RHPZ3jhpOjZwMuxyx7f/3OPmVL+T08XzHEBh7pDv5X7/t/gGAO0s3', 'is_continuation': True, 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='Document Classification At the start of the input sequence, a special token [CLS] is appended. The sequence is passed to the backbone to get the output vectors. With a linear mapping and softmax operation, the output vector of the special token [CLS] is used to get a class-label prediction.\\n\\nDocument IE With a linear mapping and softmax operation, the output vector sequence is converted to a BIO-tag sequence [22].', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 28, 'orig_elements': 'eJzVUsFu2zAM/RXC5zRTbMdVetu6HQoM22EFdgiCgKZoV6gsaTYddCj275PcFB02bJeedhT5+N7TI/ePBTse2MvRmuIKCtw2pFSl2w1tStPUXVtXmhTrqm6oqlWxgmJgQYOCCf9YdNaxx4HzsAl+lmPEyOM6mi5jc1u+x6WNMTpLKDb4N+e2Q9/P2POU+vuCfV8cUjWmytHPQ8tjqpd6KY0vJpXGy5I3pC6prHdN1eltTVTvOtQKW6biR5oQfpAMfh9ozh+Ea4fTZLuzA3grIHcMk+AoELrlYX2cBSb+NrMnXgHCFJksOpBwzx721x+/HMBOkL7C3rBZw23mOA/kTkwibBJ+IWyR7tvgOb97flIMs2SVE5OEcVrDVyt3SclZzzjCkKit7wG9gSl0MuADhBToYnr1J8Gz9b8Ync9msjgC5QguXMrIQRzZWMqs67yJ5yV9wjFrnfg255eC/P1CGl1qo3hXpRXwrjSKUvpbVJo2uqLuf76Qmw+vXcavl0DBn3iUp/wR3t18vhDsXyD7sjz8K/rDT9MLMmE=', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='IE on 1-depth structured documents When there is no hierarchical structure in the document (See Figure B), the tag set is defined as {“Bk”, “Ik”, “O” | k ∈ pre-defined keys}. “Bk” and “Ik” are tags that represent the beginning (B) and the inside (I) token of the key k respectively. The “O” tag indicates that the token belongs to no key information.', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 29, 'orig_elements': 'eJxdUsFu2zAM/RXCpwRIM8dunWbHABuQy3ZYgR26IqAtyhZiU4IkDwuy/PtEtV3Rnmy+x/f4ROnxUtBIE3E8GlV8hgJ1RfWmvtcNpt/2rtk0d7rU5bbebrGsymIFxUQRFUZM/ZdCm5EYJxKxsjzHo0NHfu2Ull6h49llGp0bTYfRWP70Qo/I/Yw9hcQ/FsR98ZRQl5Ajz1NLPuHVLkP+LeSuaVRXVztdVm17WzfU3jZdqajeqS0qwuKaFJH+RGk+fAHLsLlR5OIAIfq5i7MnBcp2s5w8wM+BGOJAnsAEYAuDIY++G1La8U0CJnf9F8LiBxF8Nb1w++UqkxF7CBTFSJE2nAZhgMuvuSo33f6Uv2oFz/XhQ/39uYS/cBKkKu/Bebp5NTrROVzX8N4LkNUHO0jLkiAhJcIInpJJkMASsKXeMBvuYbFfZrGghoNRBIvDEqI9pXVYnfE0MmVJckddNL9pPK/hIeHv88qhDSu5XHoZmneRnVoaLUsWK6sVQ8Pa+ik/hLW8gtcH8g29R5nyIHd3ffoHVzLbqA==', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='IE on n-depth structured documents When there are hierarchies in the structure (See Figure C), the BIO-tags are defined for each hierarchy level. In this section, we explain a case where the depth of structure is n = 2. The tag set is defined as {“Bg.Bk”, “Bg.Ik”, “Ig.Bk”, “Ig.Ik”, “O” | g ∈ pre-defined parent keys, k ∈ pre-defined child keys}. For instance, the Figure C shows an example where a parent key is “menu” and related child keys are {“cnt”, “nm”, “price”}. “Bg” represents that one', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 29, 'orig_elements': 'eJx1VEtvm0AQ/isjTo3kUIxTHCr1kqppfWkOidRDGkXj3QFW4AXtLnGsNP+9sws4dtyeYOf1zXzzuH+JqKENafeoZPQZoiyZF9lyWSTJMl8vlovlZSHoYjH/NM9lkqV5NINoQw4lOmT7l6hQDWnckHeWre7dY4cdmbiThbf1arfrghq7rlECnWr1x1HdoC57LMmy/j4iXUYPLO1Y8qj7zZoMy9M8iMxbknmWSbFI8yJJ1+uLRUbri0wkkha5XKIkjF7Zw9Gz88arb9Bq0OeSOleBdaYXrjckQbai95Vb+FWRBleRIWAYqBQZNII/FlRQvLnBh1siuFal//96Ngvaq9XNucPSBm9JhdIcvmgNEIpqH24HDT1RE8PKx1QWLAnPxQy2BPTcNchgCAItwTYk42MPabfFQQrsquELpDHcsQEDcyTnpRM0Wnj53afJXFyV8VUdfuUM9qLVO9Hq1Gp1anUzPOEPlF6SJpfQGTqfQIcOQU07O4P6XxbMaCODwWsM10yP0tahFjSwOJEKtmq3zKVmUnDTNRMbeADhqx2y4g72Y2KoJRhq0B1hhaaMdAjtjmvSm+N3Z5SgQcQ5ToSN8Q1xNTZMjKvQ8VgRlKbtO/igYopnxxnannuPp2mecSfROCbJ5zux/R8Iz8sAwQWLVjule6XLGG6JobjSZgeFaTfjGEKYQh4W/zzIZczvXT2n+Pt53gcKE7wn812g+jRQPRW5bU09rA4fB8/DuElhvDmwn+C5n2AubNyfkisDPi5VK3252NgWat1utXf/Hlg4tPSow4Y4VM3Q5+CDT/zGNQ8OY/7Y8okB3g/kzbtP04fY353pJP1EwyyqJ7rz1+L14S83iqg9', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='group (i.e., a parent key such as “menu”) starts, and “Ig” represents that the group is continuing. Separately from the BI tags of the parent key (i.e., “Bg” and “Ig”), the BI tags of each child key (i.e., “Bk” and “Ik”) work the same as in the case of n = 1. This BIO-tagging method is also known as Group BIO-tagging and the details are also available in Hwang et al. [22].', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 29, 'orig_elements': 'eJx1VEtvm0AQ/isjTo3kUIxTHCr1kqppfWkOidRDGkXj3QFW4AXtLnGsNP+9sws4dtyeYOf1zXzzuH+JqKENafeoZPQZoiyZF9lyWSTJMl8vlovlZSHoYjH/NM9lkqV5NINoQw4lOmT7l6hQDWnckHeWre7dY4cdmbiThbf1arfrghq7rlECnWr1x1HdoC57LMmy/j4iXUYPLO1Y8qj7zZoMy9M8iMxbknmWSbFI8yJJ1+uLRUbri0wkkha5XKIkjF7Zw9Gz88arb9Bq0OeSOleBdaYXrjckQbai95Vb+FWRBleRIWAYqBQZNII/FlRQvLnBh1siuFal//96Ngvaq9XNucPSBm9JhdIcvmgNEIpqH24HDT1RE8PKx1QWLAnPxQy2BPTcNchgCAItwTYk42MPabfFQQrsquELpDHcsQEDcyTnpRM0Wnj53afJXFyV8VUdfuUM9qLVO9Hq1Gp1anUzPOEPlF6SJpfQGTqfQIcOQU07O4P6XxbMaCODwWsM10yP0tahFjSwOJEKtmq3zKVmUnDTNRMbeADhqx2y4g72Y2KoJRhq0B1hhaaMdAjtjmvSm+N3Z5SgQcQ5ToSN8Q1xNTZMjKvQ8VgRlKbtO/igYopnxxnannuPp2mecSfROCbJ5zux/R8Iz8sAwQWLVjule6XLGG6JobjSZgeFaTfjGEKYQh4W/zzIZczvXT2n+Pt53gcKE7wn812g+jRQPRW5bU09rA4fB8/DuElhvDmwn+C5n2AubNyfkisDPi5VK3252NgWat1utXf/Hlg4tPSow4Y4VM3Q5+CDT/zGNQ8OY/7Y8okB3g/kzbtP04fY353pJP1EwyyqJ7rz1+L14S83iqg9', 'is_continuation': True, 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='Document VQA With a linear mapping and softmax operation, the output vector sequence is converted to a span-tag sequence. For the input token se- quence, the model finds the beginning and the end of the answer span. Details can also be found in the Section 4.2 of Devlin et al. [8].', metadata={'filename': 'donut_paper.pdf', 'filetype': 'application/pdf', 'page_number': 29, 'orig_elements': 'eJxFUV1Pg0AQ/Cube1akQKv4ZtL4aGI0+tA0zcIteBH2TlhqTdP/7h7144ljZnZu5nZzNNRRTyw7Z80tmHyRVWSrFKt0UVOeLoplWmKaLRGLplgW5gJMT4IWBVV/NI3riLGnOGw9T7ILGGhIgm2iNtLyFWYaQ+hcjeI8X/3QHXI7YUuj8htD3JqtokGRHU99RYPiWTlDw3/IcrWydZ6VTZpVVZGvqCpWdWopL+01WkJz0gmhg0Tx2tdTLAgvj3fw6uQNEDrHhAP0mshxC8gWRt9IjwfwGn6OeAHyRuAnCZPAnmrxA4z0MRHXBG6E2vOeBiEL4tVyDMiXgu2fJoF7nYgejqOF+HdiZS/hzJ/9e2+pg8axHef/ilrH/BsqIqRf38xH5PGThvmqBNa6BddpDmTAbvQ6Co2fVO14Vj9pZu0BRZJFgzXttTaQqDqBzc02iQv43c0DDrH2np7js5223+V+ssU=', 'source': 'donut_paper.pdf'}),\n",
       " Document(page_content='Geewook Kim1 *, Teakgyu Hong4, Moonbin Yim2, Jeongyeon Nam1, Jinyoung Park5, \\x0bJinyeong Yim6, Wonseok Hwang7, Sangdoo Yun3, Dongyoon Han3, Seunghyun Park1\\x0b* gwkim.rsrch@gmail.com\\x0b1NAVER CLOVA        2NAVER Search       3NAVER AI Lab        4Upstage        5Tmax        6Google        7LBox\\n\\nThis is a slide for a paper “OCR-free Document Understanding Transformer”', metadata={'filename': 'donut_slide.pptx', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'last_modified': '2024-05-10T05:19:09', 'page_number': 1, 'orig_elements': 'eJzlU21P2zAQ/itWPqK0s/NK+DRgE2xjgKAwIYYqx76kVmM7chxoQfz3XdJ2Ypv2fdKiKM49d/fYd8/5/iWABjQYP1cyOCABT6r9JIvzVJZpRRNIylKy/SgTNC6oEGUQkkCD55J7jvEvgeAeauvWcwmtXyBEMaJSDRiuYWCU1vR+3jVKwrRt/SrY+v26Hf28bRuFLMqad49GTm0LZqWbyjrNfTexVaUESCv64ZTT1kGH6xium1/Mgbjhpu55DR0y3wdg6uBhRDs/11aqSsFYZUSjZELTCaMzmh6w4oAWQ3aLmXPT6xIcRrFXhDys/JBxAvBk7ZJ8UZqRvZDMgC/rdU9OramTkHy11pTKkDulo5B8BkTX+CHnXDO0lVnb3tTkkrtlGpLvPaW0HNAhcEjKQvLNmg5wh9MnrCEPyTUu0lpy15s4JB8GRtyEnPLBvAakW6x7M1KyDeEeqZ+WSk9d58Tifa25aqbC6o2TnR/efrwix2cXt4dk+0Qb7Bo4JmyxeIMdfiJnvNwFJjdt57E5Ozudab7aGdmJtXXz05efHdlR5J3AN2Y7I+oZ5GzoJzb297ljWcSLlCV5JKIYaJTLnMdRnuZlFpcy+2/nbrZQHcGXk7ESgsfD/5a34HCMIsrExfHVpHIAOCKbs5IbI8GhXkYqnK6Z46YbqgI3Jsi32pxz57CIR/ibLkUSJYngmahYXmXAqoLLuEipjKsCskL8ocu/JMHbRr6t+hLDjhze4OD14QcDk59d', 'source': 'donut_slide.pptx'}),\n",
       " Document(page_content='Geewook Kim1 *, Teakgyu Hong4, Moonbin Yim2, Jeongyeon Nam1, Jinyoung Park5, \\x0bJinyeong Yim6, Wonseok Hwang7, Sangdoo Yun3, Dongyoon Han3, Seunghyun Park1\\x0b* gwkim.rsrch@gmail.com\\x0b1NAVER CLOVA        2NAVER Search       3NAVER AI Lab        4Upstage        5Tmax        6Google        7LBox\\n\\n         which will be presented at\\n\\nAgenda\\n\\nIntroduction: Background and Motivation\\n\\nProposal: Document Understanding Transformer (Donut 🍩)\\n\\nExperiments and Analyses', metadata={'filename': 'donut_slide.pptx', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'last_modified': '2024-05-10T05:19:09', 'page_number': 2, 'orig_elements': 'eJztVW2P2zYM/iuCP61FLpVk+UX5tFxb9Npdb4ddrkPRFQEtUT4htmT4pUla7L9PzkuR7W7Y7duwNkBskCIp8nlI88OXCCus0fVLq6MZiRTDTEOcAIsNx1wwylSaIk2N4JnmOpqQqMYeNPQQ7L9ECnosfbtdamz6u6CiwcLYCh3UOEbU3g39squsxmnT9JvocN5vm905NE1lQxTr3bNPTk99g25TV8a3NfTdmTfGKtReDWOW06bFLrx35nX1J3EMXIErByixC5E/ROjK6ONO2/XL2mtrLO6q5JSLM5qcMbqgyYzJGZWjdxM8l26oC2yDFf89qHrc9KPHK8S19yvyk60ZeTohC4RVuR3IhXelmJC33rvCOvLe1nxC3mDQbsODXEHNgmzd1g+uJNfQrpIJ+W2glBajdjQcndIJ+dW7DsMNF+tQQzYhN+GlvSfvBxdPyIsxYriEXMAo3mAId7cd3C4k2wd8Ssr1ytbTtmvV3Y9lDbaaKl/vD9nV/N3LX8jzy5/fzcnhx/e6G4TgcNDFe938NbmE4mgobpuuD+Ac5WRRw+YopK+8L6uvZ9nlud+RfCT41h16xH5GvRjxDMD+te+oTJVUBmnMMm60SYxJUKLI0CAqmn+rfXdElazvbOBobauKFEgO96Mm0J9ifQVtG5L6hH+HsxRcCAWpMiwzKTIjQccyoTo2ElOp7uH8X4L0FJjTqq+D2XkbJvLBijOlWc6VyWkm04wVUmY5FkKrWPEsSb6hzopPAZyX6DScwriwfYUPQQiaxga4jBMpVJZzEzOFIEReFKB09ojhZP8bCHea9l+01ynmr13fej2oMbMZOQe1KtuwGMIUh/9bHwb3a9L/TApDoWKtUwBtcp7HWVjSPJYi5ZCK7P4kfyflYVKuW9/4DqpZWLL73Mmt09iGjee0Dft50YLrxiqxJT+8GEEKG1znsQpPk8onj6JLagMomMgzrotMhc9tqiGlVEguODXyO12PpOvlpsHWjjl3u7GZO6i2XUjsPgsf/wCTMIN3', 'source': 'donut_slide.pptx'}),\n",
       " Document(page_content='Conclusions\\n\\nThis is the table of contents. Let’s start from the Introduction.\\n\\nVisual Document Understanding (VDU)\\n\\nVDU Model\\n\\nUseful Information\\n\\nVDU aims to extract useful information from the document image. For example,\\n\\nExample 1: Document Classification\\n\\nVDU Model\\n\\n{ \"class\": \"receipt\" }\\n\\nA document classifier aims to extract a category information from the image.\\n\\nExample 2: Document Parsing', metadata={'filename': 'donut_slide.pptx', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'last_modified': '2024-05-10T05:19:09', 'page_number': 3, 'orig_elements': 'eJztV9tuGzcQ/RVin1ogVnm/+K2NW6BAW+TBzksUCCRnKC+6N+xSgQMj/16ubMOKLERqn4rIgACB3OFo5szROcsP9xU22GKXVzVUl6Ry1KL3TCjpmKUOrJcioqaYpJIiYfWGVC1mDz77En9fRZ9x3Y+fV4BDvi1brESkusHOtzhnhL7b5NXU1ICLYch31ePz/HnYPvfD0NQlS913P33qYNEP2N21TerH1ufpok+pjgh93MxVLoYRp/K9DW+br5Zz4sZ3641f41Qyf6iwW1cft7tTXrU91KnGbZeccnlB1QWj11RdMndJ3Xx6KCdX3aYNOJYosd0Zd7AxEZjlMVlqnDYsOGcsBglRRG6Uqr6UExnv8hz8tu9is5lKXdOc+qnb6zo3OAfuA288TTZ4lAl4FMFCWVCP0RturKLhOPD0+wX+GCl3gb++rSdSPvkWSfahQdInEvsul3zTgvyBebnhlLmJTNmPmaSxb7fBv3d57GET55YWuzP7y49jafQTXs8/cWB2TnIpo9cxMZM0suQ8CKcoiORQu/hidv+nMe1it9v1uxL2y4j+70MdK+qD0kajk1Jyk4QDbwoMLECKMYozYqvcBfB9PW18Q64e6yA3HeBYeNZB3a3JD++vbn48UQ2kAycVJtDCofCCMRuolTJZHaw/IxmW+2pwlHtfDeTqhvzZAzYnwc7QR5OiEDqh0QEUD1IzmYxUzHBHX2E/EfabCdOmKZL60MhjfUfxR8t1sTwpBIAGJVNwNinJAjJDQ2LnJCv7+B/l5j7tfd0WF+xJ2Rp9zGTzMJP6eSbP3vfUIanbUsWC/NaP5ZxvhwbfvFrhN63QpmBCeSNE4YRxiWPQnMkQo6JUQkhnxFm1C+CvD/Qh7PLZDt+WvFPJGf+FJPjkOaQUNS/q4xnIpKUBbQqrWBJwRpKs9iXhKPX+uxO6oGa98QbLTQRM1OXFO3pXdDgARcNfYT8R9nuyrOJM+2VZLasRI9ZDXlbky0lzCKFcchIFGpiQgUcTOViUArhUFNUJ9/HvR11eXAuPcXR3Dj8/e1x8VCEcX3ikJ0/4HfbJB3t8dcRvOiIAo8oEBGq11EnPfxFJk7bMoRXqnBxRH3JEvuOI7/w4lavhAS34+A+z7WcP', 'source': 'donut_slide.pptx'}),\n",
       " Document(page_content='VDU Model\\n\\n{ \"menu\": [\\n\\n    {\\n\\n      \"nm\": \"3002-Kyoto Choco Mochi\",\\n\\n      \"unitprice\": \"14.000\",\\n\\n      \"cnt\": \"x2\",\\n\\n      \"price\": \"28.000\"\\n\\n    }, … }\\n\\nFor another example, a document parser aims to get a data in a format,\\n\\nsuch as, JSON or XML, that contains full information.\\n\\nConventional VDU Model \\n\\nInput\\n\\nOutput\\n\\n{ \"items\": [ \\n\\n    {\\n\\n      \"name\": \"3002-Kyoto Choco Mochi\",\\n\\n      \"count\": 2,', metadata={'filename': 'donut_slide.pptx', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'last_modified': '2024-05-10T05:19:09', 'page_number': 6, 'orig_elements': 'eJztmVlv3DYQx7+KoGfb5X34MSkK9EhSoE5RIA6MITn0CtWFXcqwa+S7Z7SbtOsc8G7ahyLRvmgljijO/8ehhqNX9zW22GFfrppUn1e1jAKUFNmzAKiljTob78FZHzF47+uTqu6wQIICZH9fRyh4PazvrhKOZUWXOFnkpsUeOpx7TEM/latN2yQ8G8dyW79rL3fjth3GsW2ol2bov7vp09kwYn/btXlYd1A2p0POTcQ0xGke5dm4xg0dt+Zd++B07riF/nqCa9xQz69q7K/r19urm3LVDanJDW69FEyoU6ZPObtg+pz7c7Z1bKQ7r/qpC7gmK7O9sv5Hm5Q40zZgYs4oQ7pkgYpl47hHJ3Wu39AdBW/LbPz79y+rZ0PCdu74va8XTWlxNvtQdlAkutQp8uS0iZYZoYwBoVNgSUT+uOzsq5X90Sm5L/t9dUlC9dPldiR70r/s30nW/IXpYrb+BAYZs7bgEiJjSToVnDcWMyqbogLvFgwHYqjod3+0/GCNAmmACZmD8c57lYJwPmVnogxikf8I+SuKhL6b4+CyloyJ05/vhjJUT1dDHGhliqvmsj45HpF2KUrJBAAGJk22njMrnY4oWBBsQXQkoqlvyrgmF3akuDpjjH0JGUZvCooSBkYzFjDOC5fxkU4YUxQ/C5kjycS+7Jjcii+KFGm5BqaUFlZZ6xTkJLIEnjFZJdLC40gee1Ei3C5KjobiMQEXmgJCI0sgRPTbMVjGQVhhFyhHQKHzy4meZao3R4NwMcgUAIxNTiRphRJZUcrlncOoYYmOQ0H8MKwr6IeywnWFt9CNLZ5UUL13pKKeN9QETbep6PV/jWVuJVWrpqd/O/cfrG7PYb0mR2/wc+xozxM5MgeGQ9ZJaOulVcp5JW2IiAu7A9ltpriqYHNS/fTbi+cVgfzj2S8nVVlBqeJAI276TZWntiVUO1fJg7OjUHkllIpgYuY2G+TZQ5Je0+Yme6Ts4CNU/ycq+1Lte/0rmT1ZI/z5yclpaNOsaCORgjJJUz5kg7IoleLOsG9qD2H3BXw69Df0IHoMtNXfxYnqoOqESyw6RPDMeiMi6cqdTIpTpimZjQekll9NUch+GPCPzrd9CD/241QOkjw5DSEHG2US4C1yHhPjgvOgHPPJLJIfKPmLqRyquYjSRW0D5ezWiGxcRKsIRI4qcJH0ovmBms9FuKZgt9lW4Q5bY2iT6iHq5JKNwmoDwdGkNwwzaHD8Wyq9fST+oxPz35felAM0OmUpTQ4uG1DKS4mesjoucsyL/EfIvy29kQz/dfHNWBONAx0YHZXMMXodhcA5SijdPODjzALpYYlnmHZFHvEYjNdvAaW+rNI=', 'source': 'donut_slide.pptx'}),\n",
       " Document(page_content='      \"priceInfo\": {\\n\\n        \"unitPrice\": 14000,\\n\\n        \"price\": 28000\\n\\n      }\\n\\n    }, {\\n\\n      \"name\": \"1001 - Choco Bun\",\\n\\n      \"count\": 1,\\n\\n      \"priceInfo\": {\\n\\n        \"unitPrice\": 22000\\n\\n        \"price\": 22000\\n\\n      }\\n\\n    }, ...\\n\\n  ],\\n\\n  \"total\": [ { \\n\\n      \"menuqty_cnt\": 4, \\n\\n      \"total_price\": 50000\\n\\n    } \\n\\n  ]\\n\\n}\\n\\n≈\\n\\nHere, we show a representative pipeline of visual document parsing.\\n\\nConventional VDU Model \\n\\nInput\\n\\nOutput', metadata={'filename': 'donut_slide.pptx', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'last_modified': '2024-05-10T05:19:09', 'page_number': 7, 'orig_elements': 'eJztmU1z2zYQhv8Kh2fbxeIbOSY9NIe2OSS9xB7PYrFwOJVIVaKcpJn89y4Vp3HiZCymM51OTV00IgEQfB9g913o5buWV7zmfrzsSvuoaWOqLijwmIC9ixYigKEK7IoJsXB70rRrHrHgiNL+XUs48tWwfXtZeDO+kksgLWq34h7XPI1Yhn4/Xu5WXeGzzWZ8097cH99uDvdxs1l1Mko39D9c9+Vs2HD/Zr2qw3aN4+50qLUjLgPtp1mebba8k+9D8/Xqs5/TwCvsr/Z4xTsZ+WXL/VV7cbi6Gy/XQ+lqx4e31ErbU+VOQT1X7hGkRypNvTfS87LfrzNvpVU4XNl+0sZ67YoN3pZsfXFGYcg2sLGikldZt++lx8hvxqlxc/ict5utTP9pX4fzg1xTi5sXf96NK5763GGgsmOFtkAskaN31rlEJdWaSWH09zNQ/1sG967PuwwmCvu+G59NJCYKYJVSJ7dRvOhvJOz+5PJ86v0VLMglp2S89jVB8dWkBFWTZw8cTYkLltlYNh+R6ChIZhPJgN4H5bO1hGBkf5JlRkekMtWsFiKziLyfDSBIQAoqYDVBAhYYRnKgbLVRBwusFwAzABz0nUvAVSXiV+XRVW18kExkyKkQsUj68EtQmrcFzttJhikinbegFDSnzZNXAw3N431/3s7PGc4KCkMaEprsM3iOtdhkTBIXkcyCZy4eGvb9eEji35HAU3baazbFK0bmHF2WJ2ZX2QSvaPG2/4K3JU1ibl3WkRy54Gu0xqbgkKp1xS0MZjG44221/sJIfZOE8qwVAWknschJEtG+hAAqlCQ5BB+2ebpvld5nZ7+kcFR8KjGpYCuIp3KOQgWXveNQlANQQEfU3guRf2RnK4TsEFzUlokdZF2m3RcSk9TkJiwAZgCQ32dnZ7MZQGAukcjb7GLOJdeESLqmRAGTPyJBLAxuGFzMt0jKGg/JKlY5ZEPiXIPoD0UqOyg6HLEDlvR8o/55Ow4jrqZ08LJ51xyVk5PPKhWFJQcP2VlMwXMiDga1NsfU04v8nzlUmfX+j/HtJX0oGuzJcRyKj8xB5UhZFY051sTAwYLKSCqahcNMDoetcPm3P3LqWJdqk6KaXEQO7KMYVeuZyFVrtDc2POh0cO8qvZOSm9kJISEVSQLsUFUFYE1OqlaPVkua0ACL/kfrf/Ed59tAPpTspBLQDrFAjZpyomxBwDzo49VZ4s8vBXKewlyxLMUA1YBEOqIuRWJRrSrgIv2R0p/vtbZxtv4GqGadiAJZKcd0jNWZIpHfobYOHvTpxCz9f+ItnzSvudm9Gl432Gz501yvudl0G151PTdDba673R5Xzcd3bOShu66/+qyG+wW320PPbyYMq6d/4zxJMV09g5QPxSSnipFZ+nT3kPW/xOi2cLff+pk0e7xl/P2rpwZa7Dp5MpgrSHDIILVY4mKyzoWCfUBLNd4W8MnQX8uD5DGyqH778UXz81B4dZwBV8k5RjsVQaXaBBkMOGelJLLTUUx6QAY8frn9711vtyE87Tf78bjak2RYtDEyaLDVVi4eM2YleZBcrovkR0r+6378uuYXfwFPwuwz', 'source': 'donut_slide.pptx'}),\n",
       " Document(page_content='{ \"items\": [ \\n\\n    {\\n\\n      \"name\": \"3002-Kyoto Choco Mochi\",\\n\\n      \"count\": 2,\\n\\n      \"priceInfo\": {\\n\\n        \"unitPrice\": 14000,\\n\\n        \"price\": 28000\\n\\n      }\\n\\n    }, {\\n\\n      \"name\": \"1001 - Choco Bun\",\\n\\n      \"count\": 1,\\n\\n      \"priceInfo\": {\\n\\n        \"unitPrice\": 22000\\n\\n        \"price\": 22000\\n\\n      }\\n\\n    }, ...\\n\\n  ],\\n\\n  \"total\": [ { \\n\\n      \"menuqty_cnt\": 4, \\n\\n      \"total_price\": 50000\\n\\n    } \\n\\n  ]\\n\\n}\\n\\n≈\\n\\n…\\n\\n…\\n\\nMost conventional VDU methods share a similar pipeline.', metadata={'filename': 'donut_slide.pptx', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'last_modified': '2024-05-10T05:19:09', 'page_number': 8, 'orig_elements': 'eJztmk1v20YQhv8KwXPszn7v5pj2UhQBckh6iQ1jdmfWJiqRqkQFcY389w5VJ7WTphYDtCgq6iKIWi6X77Oz8w6Xb+9aXvGa+/Gqo/Z505boCG0AH2PmCM5hCFYxW4UkH2yfNe2aRyQcUdrftQVHvh62t1fEm/FGDilpUbsV97jmqUca+v14tVt1xOebzfi+vf9/vN0c/sfNZtVJL93Qf/eup/Nhw/379aoO2zWOu7Oh1q4wDWU/jfJ8s+WdfB+ar1ePfk4dr7C/3uM176Tnty331+3l4ehuvFoP1NWOD3epQdszcGcKXoN7rtJzSNPZGznzqt+vM2+lVTwc2f6pTdWUQ/HFYK6qVsjKOkpMJutMJdj2g5wx8vtxanzXXLTdyOvdxTSUZur+4x2/7sYVT40/Fx9S8FqjzTbaWnMggJwg6KIRgjP+afHhfyv+kxPzofiNfO4eSv6mv5eq+43p9dTqL+RXqWoTIRZigoqoTSbNKbFjDRHtIv8M+RuZ/5MM0/S/aA2APvvpdhiH5vuboQzNy6HcdBfts9mQbMgBTHUhT2OhoqbLQ2SVTC4upAXSTEhl2PfjREnPh+FM0g7IGYM6QgUuplBAqAlTlBVyyRZHZouPMDZbGf6PfR0uDnIdkzSitzoEkZxZFW8TmgKsvKZSvIvMpxwQT87PLxlMFPZ9N76aSEwUlAWA+aGhwFjlVAkeYsRQgwQlZM8hS4jW016nvhHL5iMSOQlgNpHslcqcvaMINjiVXaSodUw1lhyVXojMIvJhNgCdyHiJADEDskpJqgrV5FSizqpaa+oCYAaAg75zCYRILIkiWR9rNlQ12wwpa7lEkHDIC4FZIfDQ4SoA1Zzdu9sX+/5bvG2JhlIK0SKrENAn7QOF4Nh7QmKz4JmJ55O3VfNhePFqmMVdOccxqVCsNyGCAmdLkKy+eNt/3tua6F2wPvigpNQzho3WyjE6xclpOsJELQz+zttq/ZmR+iqJnCBSzphJUQXrAipjjYollSw8yikvTU/O0qfs7OcUjqu9c6qqOKw2SxJXuppobaXgUpH0QSf9sPAbiMy3syHF5LMJDiQ7Y1BJE2sjNZ83U1I66QpvNgD5fX5+PpsB+OpCteQICvucnHgmSzhFgFYJw8LgaAaX8y0SWojaBptCQZUYKyfj0MZQc2UqRxR0S3q+V/+iHYcRV39sFt0dt11EQU/qRwfZmeIUFJ2jceAAPPMx20WL/I8cqox6/+t4e1X+KBrss+M4aKaQQKGWq5ACyj6K/EWWIK0LliO80cLhEYdDKFx98kcyn490qdW5YKzz3plqTQQmHakmZywxq3rSBfSTs/SLlNzMTwjJGyRrLEK2GCUBR4/ae6u0AWGz6H+0/pfznx6J6SdXtQ4qVEkNJklKiKgyc9HRHfHqxiL+Qfz5pYAjMq4oGyHVLNYf0KeIVMR4YdBxkf5Y6S/2Ymni/J0FLTKzVMSlqKoYqnZUspmK4+QrnvQe6Fz9QfvZ+rOI723wviiuku4lBLLKqFiTk/5Oel/h39B/egHAJGSpv6TbFACUqUAVInilq1sc6JEO9OWwG5sy9O/kfBkcrpqff3jTiHQ3A+2a3Y103GCz69bdCrfNptvwquv50XOLr5rTZLW1BX2pkp+92NGEJDkayNTEPn1ZJvyXeDwU6eHdvpJmL7aMv7QfLn8HtDh9Cw==', 'source': 'donut_slide.pptx'}),\n",
       " Document(page_content='Conventional VDU Model\\n\\nDetection!         \\n\\nFirst, a text detector finds all text boxes.', metadata={'filename': 'donut_slide.pptx', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'last_modified': '2024-05-10T05:19:09', 'page_number': 9, 'orig_elements': 'eJzlUk1v1DAQ/SvB5+7iOM6H9wgVNxCHhUtVRRPPeLFw7Mjxrraq+t9xApW2BalwQ+CL5ef3nj3z5uaekaORfOotsl3BWhQtoR5kU+rB8LalhnPetTiUnUJo2VXBRkqAkCDz75mGRIcQ73qkKX3JEM8MYx15GGlxxOCPqZ+dRdpOUzqzH/fpblrvYZqczS42+Ncnj9swkT+PzoQ4Qpo3wRirCYM+Lr/cTpHmvK/00T05LsYO/OEIB5qz8w0jf2C3KzqnfgxojaW1SsGF3PB6U/I9r3el2nG1qKes7P1xHChmlnrIUKJzWhRvgz/lh/Iz4IrP15+K9wHJLaLHOvY2OWJZ87ylte6IK0RVd51UJIwAwUFJaBU2A6mXW1r+My1dkfgH43aZwTUl0su3XhWP67cCkCAk6Uq2FQxK8MaUoAaqRUW60crQ/zTTzwN4cTgvA3hn45yuCigWoMA1jhALYz3OBTj3HR/CmebtZTIfIMZczon2i9EvElJSSKmh0aZsTUOlUYCVqjlWRlGj9E8J/U1hXHbosuqPmfYmEnxlD7ffAAg3wpQ=', 'source': 'donut_slide.pptx'}),\n",
       " Document(page_content='Conventional VDU Model\\n\\n{ \"words\": [ {\\n\\n            \"id\": 1,\\n\\n            \"bbox\":[[360,2048],...,[355,2127]],\\n\\n            \"text\": \"3002-Kyoto\"\\n\\n        }, {\\n\\n            \"id\": 2,\\n\\n            \"bbox\":[[801,2074],...,[801,2139]],\\n\\n            \"text\": \"Choco\"\\n\\n        }, {\\n\\n            \"id\": 3,\\n\\n            \"bbox\":[[1035,2074],...,[1035,2147]],\\n\\n            \"text\": \"Mochi\"\\n\\n        }, {\\n\\n            \"id\": 4,\\n\\n            \"bbox\":[[761,2172],...,[761,2253]],\\n\\n            \"text\": \"14.000\"', metadata={'filename': 'donut_slide.pptx', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'last_modified': '2024-05-10T05:19:09', 'page_number': 10, 'orig_elements': 'eJztmEtv4zYQgP+KobPjzpDDV67bW7G37F5sw+CQw0SALBm2sps06H8v5aZAWhRobQN7qXwxSPGl+Ux+HK/fGulkL/24a3Nzv2iQs7eJOAQbnDa5gAfrnVbOkWaFzXLR7GWMOY6xtn9rUhzlcTi+7rIcxqdaBbVFaTvp416mEfPQP4+7U9dmWR0O40vz/nx8PZyfx8Oha+so7dD/9K3Pq+Eg/cu+K8NxH8fT3VBKmyQP6Xla5epwlFP9Pjffd38pTgN3sX98jo9yqiOvG+kfm+259jTu9kNuSyvnt1Sg6A7MHcIDmHsM9xCm3ofac9c/71mOtRXCb7VulJdx6vJp6L/Vmeo8sVt8/fnL4vOQpZt6/fkiD+3YSVP7/D2mKQkwW0vASfnoPZJOjkK0lCCK/K9ieq46XvCD+wjhbbFpvg/HfNpMa6mR+hD+L/172NpfJT9MHf4BhQdnQiBrQGLxYsU7KVJZgAmOBWYU/xXF4sNn07R5IoLLi4EAItlImoQd1DkjoWGvPULKvoCdgVwHhHl4qUjWa21hqYD8drlarZZrbcxSoXLb7eWsvBUg61hDtgFNgEKCuVBWYDCmMLO6jtVUO22fTaMB1N0vr8M4bJqL8agKoqAuoe4mwFiSMDHroKxLHCLNeC7Fcw7ypRjIagws3mZEXw2fY7U++aKFbLCuzBhuUYy6/NgKrCkzeK0ZpWSQAHWjeC2aTdazYm5WjAesinH0rphzEXW4RjGcKJOJlVPKiZwlE8Rl1DFjiGU+w25XzKenIV1jF5ORIzJSyQF0Dk7E+Szea2vqQuJM5ofYJRuGVIxACMIaHXNkiKC4JjOBop8x3GIXffmJZWMdPkbKOpBLYhSUbE09/sBCqIaZgdxoF6xp4Ue9/FFGuiqFiblE41Wwuki9FxRfLwIcoyYKxRrWM61b/fJ5SE/tFX6JShRoCRlLTlal4JMtZCQQJfRh3kc/xi8OHCpTMxYxCTFZy4gOoinJKBTkGcMtfqHLT6xiMBv0RmpmD/XSlVVEtMqbgp6NmrOXW/3i7JSuOPWul3NRGX2NXYp1gCKRcbqWefHMKUjN+dFpQjf/0X+zXZBWAPCvetn+DlebX8I=', 'source': 'donut_slide.pptx'}),\n",
       " Document(page_content='        }, …, {\\n\\n            \"id\": 22,\\n\\n            \"bbox\":[[1573,3030],...,[1571,3126]],\\n\\n            \"text\": \"50.000\"\\n\\n        }\\n\\n    ]\\n\\n}\\n\\nDetection!         Recognition!          \\n\\nAnd then, a text recognizer reads all texts in the extracted boxes.', metadata={'filename': 'donut_slide.pptx', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'last_modified': '2024-05-10T05:19:09', 'page_number': 10, 'orig_elements': 'eJztlstu2zAQRX9F1dpR+RaZXYuui6JIV7ZhkJyhI1QvSHLgJMi/d+QYbYKiQJRVF9aG4uWQ4twjPtaPOdbYYDvtKsivs5wZbsBIBBdBBVkmG03QTDKq8BJUvsryBicPfvIU/5hHP+G+G+53gP10SxKjiFTV2PoG5xGhaw/TbqwrwKLvp2N+bp/u+1O77/u6olGqrv1410LR9dgemzp1Q+On8apLqYoIXTzMsyz6AUcqT+FN/ao6D1z7dn/wexxp5HWO7T7fntRx2jUdVKnCU5aCCXXF9BVnN0xfc3fN3Ny7p5679tAEHCiKs5M0/DGHB7AmquCccaXUkJhlxpZSlKWSQfD8iXpMeJzm4Oz8kLY50AfNitya2895/2jP1lUPCDdzp6c54jUOYKULSroQiYmKKiaQsnTKoxSpdOyCYymO+dnkFWxIFGK1mEhAkVzSRukYUEmVrNGWeeaASe1RXIi8j0gI3ZGYrNdcl3IlacfZroqiWM11vpJcmO12OS3kIlhJ68Uxh5bb0oI2NkKAULKU4oXW+2jN6ryCNrlmBWNsky9GQ9uXZVqGALwUUhrPvXEWohbJluI5gwuaRSfNYgaca8asDoo5WnVMmWQgGo7SQOBB8wuDJQy2i/23YCKPVgCXLPjghfMqUkEApCX54v9b/V/+7wcllKRjWyVvLVMx0kGRlJZOGmeksxfv3+r9F5wwzvP68PuE+I6x27fVazFbvj+VCQ04GSJqlBYTBEOblDYmCOmVvjB6K6NPLWTTLbarzGezlg3PhB5woFcPY+br+tQyZlU7h2b0Pvg4IWR0NcOxeEnvqx8GSvMO/0XO0epS0ZuYiKFBnpwH6TQDmRwa9/fF63+C9NK4l1l/o7DP5NbP/Gn7C6mdzTs=', 'source': 'donut_slide.pptx'}),\n",
       " Document(page_content='Conventional VDU Model\\n\\n{ \"words\": [ {\\n\\n            \"id\": 1,\\n\\n            \"bbox\":[[360,2048],...,[355,2127]],\\n\\n            \"text\": \"3002-Kyoto\"\\n\\n        }, {\\n\\n            \"id\": 2,\\n\\n            \"bbox\":[[801,2074],...,[801,2139]],\\n\\n            \"text\": \"Choco\"\\n\\n        }, {\\n\\n            \"id\": 3,\\n\\n            \"bbox\":[[1035,2074],...,[1035,2147]],\\n\\n            \"text\": \"Mochi\"\\n\\n        }, {\\n\\n            \"id\": 4,\\n\\n            \"bbox\":[[761,2172],...,[761,2253]],\\n\\n            \"text\": \"14.000\"', metadata={'filename': 'donut_slide.pptx', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'last_modified': '2024-05-10T05:19:09', 'page_number': 11, 'orig_elements': 'eJztmE1vIzcMhv+KMWfHFUVJlHLd3oq9ZfdiG4YkUskA4xnDnuwmDfrfq3FTIC0KtLaBvXT8CXFEScPH4it6/dZIJ3vpx13Lzf2ioVKcJ8ikk4Eo3nqmwtqD8sqLtc1y0exljBzHWPu/NTmO8jgcX3csh/GpmlTtUdpO+riXaUQe+udxd+paltXhML4079fH18P5ejwcuraO0g79T996Xg0H6V/2XRmO+zie7oZS2iw85OdplavDUU71+9x93/2lOQ3cxf7xOT7KqY68bqR/bLZn62nc7QduSyvnu9RKmztl70A9KHsP4V6FyftQPXf98z7JsfYC+K3aRnkZJ5dPQ/+tzlTnid3i689fFp8Hlm7y+vNGHtqxk6b6/D2mnFExKWOz5iyxAHiHYry2GFyM8X8V07PpeMEP7iOEt8Wm+T4c+bSZ1lIj9SH8X/r3sLW/Cj9MDv+AQlMioizkDWWdOUQn2XgfvCQLimYU/xXF4sNj07Q8EYHlxUAcRlcoFjGOICnKVqODukc0Sib2M5DrgKQ0vFQk6zU6tdTK+O1ytVot12jtUoOm7fZyVgGsylRfIUAGQ5ENEZcY0HJggZnVdawm67R9Ng0qpe9+eR3GYdNcjMcUqyK6gsagKVzhIJJyIuxRs9IznkvxnIN8eUYLQbERX0qOnoKA01ZcCCXaCsPMGG6RGH152vJokgTjraXiFbCpm4SMSqC9MmbeFzdLTA1qlRgy7xJzbgKGayRGg9WF0PhAdR3KRw31rYCA6keez2c3S8ynpyFfoy46UsjOEqBFZQIGlQpS0q4mOB1Cmsn8EHUpWYWcciYvVIJzHHMWq2Ny2gDmMGO4RV3w8owVEVTSbDmDtSFMlaUWVSvMXLIzcS5gblUXUGg/yssf7Vp+XKMvttQ6M3BlIwWDx0qopi8Iuprrc/4r5mZ9+Tzkp/YKfUHtM6KTJJqii2jEpIiqio4Pyqc5sf0YfQnsHGCtXQwkUGApIjMWj+BTMTxjuElfzOUZqwq7UqEIoQoQEKDOWsHEBMDg3QzkVn0hN5UrpN/l5dzUFq9RF2Ax9VzMwgiajU8EUEyQIrqeCEyZWd2qLmBWSql/lZft7+3GZQk=', 'source': 'donut_slide.pptx'}),\n",
       " Document(page_content='        }, …, {\\n\\n            \"id\": 22,\\n\\n            \"bbox\":[[1573,3030],...,[1571,3126]],\\n\\n            \"text\": \"50.000\"\\n\\n        }\\n\\n    ]\\n\\n}\\n\\nDetection!         Recognition!          \\n\\nOCR\\n\\nThis two parts are also called as Optical Character Recognition (OCR).\\n\\nConventional VDU Model\\n\\n{ \"items\": [ \\n\\n    {\\n\\n      \"name\": \"3002-Kyoto Choco Mochi\",\\n\\n      \"count\": 2,', metadata={'filename': 'donut_slide.pptx', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'last_modified': '2024-05-10T05:19:09', 'page_number': 11, 'orig_elements': 'eJztmEtv4zYQx7+KqlMLOF6+Hzk2eyvaLRbZXmIjGJIziVBZMmw5TRrsd+/IDVpnH4i1p6KxYMDQiKTI/4+cGc3VY40trrAbrptSn1e1IEeh+KgSRfBZWWW89kYW/kkHrp5V9QoHKDAAt3+sMwx4028erguuh1s2CW5BTYsdrHAcsfTdbrjetk3B+Xo93NdPz4eH9f45rNdtw6M0fffmrivzfo3d/aqlfrOCYXvWEzUZS5934yzn6w1u+X/ffNU+ux0HbqG72cENbnnkqxq7m3q5t26H61VfGmpwv0ollDkT9kyKS2HPZTwXcey95p7X3W6VcMOtpNybNv+K44lc8DJ7lYwEDJaVoqKCFEEEtLb+yD0GvB/GxtXTxbbFjl/oZqzW+Pxp3R+6J+maP7Fcjp0+ji2e47A6KEuBLIFhGh6L9eCMjJQw64gnHFNxjNeibsqCjUrNJhMBjYQp+FwMoC8CQHgAmyMGL4qxJyLfRiSl/p6ZXF1J6/VMCy2Ws/l8Phvv5UxL5ZbL6bRKEhSL04kIRZIExJ5NZghYQowWTrS+jdZoHU/QorZiLoRY1NNdm0cIIgKyN9MUgy6RY41NKnlyiP6EZnKkmcwAEaxTphRNKmZMBZWN1mbD8YVQqRODKQyWk/X32oKzmRyYYKMAnijKFEIOyUSn4kn/Y/WfvvepGO+FCJxGxcwOxxUUMkRSQKSSEyftj9X+LQ6Yx3l990+EeI+5v+ma58ZqMqMkiiAfinDEkyiBM16QoFUkToStOCJGyBOjPaN3F+8P1b9shha/pLgrkpJjnYk8eGcSJs0JmcreO1lSftWn4sXteKj45W2zrYY/+oqHGLYVj1NBu+2rDG2LpYJt9W498MLb6uIWNpAH3Bwem+p7RvbD/BDaL7DZ8Erv8GvHJRplTAaXSY45lORP+KKjFRzeI7r4Obz/EqdD7Q5X/Ss3+3GD8PuX8/tsBfjiSFmHEXXOYIWVNqPMRoTXtF3VoYIXfXfHb+L38Ab77e2H6ue+YHuUC0guGxKa+EPXZE7Es9RJayU1YkA6pubw/3G66lMX8OKGO4TwOFYaBlxtx0+lq+oo9dmjB6GtRE5OlI4mgfTaGWu8syjhVaXkn6n/4tb8NCWfXnEzRmFxAUBm0IFzcEAvpPAikE1eppP+U/QfawWjDn/XCrQQ6uynh37oOeb1uWenlG+bRT29ruNjNJYTdqYjrYpKeOdIBiClg3akT5SmUsr9rtuXdF6siS7/ApD1tPY=', 'source': 'donut_slide.pptx'}),\n",
       " Document(page_content='      \"priceInfo\": {\\n\\n        \"unitPrice\": 14000,\\n\\n        \"price\": 28000\\n\\n      }\\n\\n    }, {\\n\\n      \"name\": \"1001 - Choco Bun\",\\n\\n      \"count\": 1,\\n\\n      \"priceInfo\": {\\n\\n        \"unitPrice\": 22000\\n\\n        \"price\": 22000\\n\\n      }\\n\\n    }, ...\\n\\n  ],\\n\\n  \"total\": [ { \\n\\n      \"menuqty_cnt\": 4, ', metadata={'filename': 'donut_slide.pptx', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'last_modified': '2024-05-10T05:19:09', 'page_number': 12, 'orig_elements': 'eJztmE1v8zYMx79K4HObkXpXj9tptx26U1MElET1MeDYXuIM7Yrnu4/OOmzPC57WG7DD6lwCyaQs8WeTf/ruueGOD9xP+7Y0N5vG6VQCZy62JqyFEqhsVQJMoDVb11xtmgNPVGgisX9uMk38MByf9oXH6YNMoVjUtuOeDjyvWIb+PO1PXVt4O47TY/NyfXoaL9dpHLtWVmmH/rtf+7IdRu4fD10djgeaTtdDra1sZ8jneZfb8cgn+b+YH7pPhvPCHfUPZ3rgk6x813D/0NxfZk/T/jCUtrZ8OaUCZa7BXiPcgr3BeANx9h7Fc9+fD4mP80HUZer4V3BKyhbIF1eVdRxZ50wWLNrMmA2E5qN4TPw4zcaby2/XjEfZ/499HXaXeM0WLye/baeOZ5/PIWiTXPGFTeFkU8XknSdXbJIxQsbXIcD/F8KrT+iXEGYM576dfppRzBjQAMDV31n83L/EsP2Ny+3s/RUuSkHA6qP2kaLcP7rsjQslRNC+RrVyWc5l/JOJCsJkMZIYCECzKpYhy6viDOQAiRQlnYxdkSxE8nExgcpEDq12WLCkaCFGHRM6pyh6yZIrgSUELgFeioC9xZK5GnQ0VyJCCb4qLjnwUjPKimDZS7Br5jjMSWnXIABurjc/fBjysPn+3O+a5XUjGe3ZehEMqhhHchByVUfwiq1StPJZzCcP5366VPLlNAroEpMLKQIFBb54riIIHBifnM51lbj/gcRVEG1wXFNARA4m5ZpVSZC0UMCaVwjLIHwhcUWrfiqnvtFtAFWUTGSqZCU2om0doEqObETi9y2hXn1OX1O1n2N4U4oyHCg4Y1LVIWCyumpdiWHGhJrf0IWvSP6dqgVrVI6ovavktJN2vBajIVIIRSf9vlXtYgIy3m63y3UTO6crWkPJWtGzWAUKkoPENUZ+399BFkK4/weylSupKIIscrZGmupYs81OhSy3zMGuNfrtNXrXTMNE3VwS7jbPmzcV5hgheUYGEaUGvJTlmhIaJ6BB1boK1cVCVbZ9/mV62uc/mgdz9TUQ978DZrVVeg==', 'source': 'donut_slide.pptx'}),\n",
       " Document(page_content='      \"total_price\": 50000\\n\\n    } \\n\\n  ]\\n\\n}\\n\\n≈\\n\\n{ \"words\": [ {\\n\\n            \"id\": 1,\\n\\n            \"bbox\":[[360,2048],...,[355,2127]],\\n\\n            \"text\": \"3002-Kyoto\"\\n\\n        }, {\\n\\n            \"id\": 2,\\n\\n            \"bbox\":[[801,2074],...,[801,2139]],\\n\\n            \"text\": \"Choco\"\\n\\n        }, {\\n\\n            \"id\": 3,\\n\\n            \"bbox\":[[1035,2074],...,[1035,2147]],\\n\\n            \"text\": \"Mochi\"\\n\\n        }, {\\n\\n            \"id\": 4,\\n\\n            \"bbox\":[[761,2172],...,[761,2253]],', metadata={'filename': 'donut_slide.pptx', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'last_modified': '2024-05-10T05:19:09', 'page_number': 12, 'orig_elements': 'eJztmE1v4zYQhv9KoHPizvBryFx7LPaWnmzDGJLDRIBtGbayTRrkv5fSJuh2USCRDexhIQGCwRFJiPOYfOfV8qWRrexk32/a3NxeNRgtGMoWhIlMMeQx64RACpEdQHN91eyk58w91/4vTeJe7rvj8ybLoX+oIaw9SruVPe9kmDF3+8d+c9q2WRaHQ//UvD3vnw/jcz4ctm2dpe32v33d50V3kP3Tblu64477001XSpskd+lxeMvF4Sin+jt2323/0xwm3vL+/pHv5VRnXjayv2/WY/TUb3Zdbksr4yoVKHMD9gbhDuwthlsIw+hDHbnZP+6iHIeFqDF0/Dc5OSYLTNkVZZ0E0SmxBYs2CSYDvnmtI3p56ofOV+O1avqu5+3mcKyrWNW4BfiWxff137X9VoaRP6IgDsWg19Ebp7xBYBfAxqKTU0WcfIwCfl0UH/5Pf0TxevV90v/cvyWr/Vvy3dDtfwAYI8Foa6LDYBzWpCvWXKznhCaGT+yFGcA7gPXk7AO5pAyE4jhyVBi9F8kUAlEqIjhn/7PZf52c++Bs0Va5FAokwCiQWWUU9mCTYp5z/9ncrx6VMn4yAGXZaWe85nrok/eRmYzKjoQYxdAM4LMAXqoE/9Ud82kQ32XN1GQUkLSiYkMyJumkJUeSkkviApS9nVFMkeH3a9W0eTVWi5OBoAbOMVpNIXNIgxrkgqK8dchB5rroTCAxdk8VyXKpHVwrMH59vVgsrpfa2muFitbr6ayUTTEWo4yKFgFytKaW0DmNe6gGZ1bnsRqiw/ZZNRpA3fzx3PXdqpmMR4jQ+5RT8CEHh8Ra0JrqN1Stc2GusSbjGZM8FYOOGZUk4JIrDydUi696S6yS40WHGcMlEqOmH1tULV5OhI7J13NKnE+FnDE6oKVEegZyocR4wCoxZN4kZmyiDudIzGAKRRV2qVSfApgsuWx1rQecry80n2EXS8zvD106R12ASWqljLGI4wACYiBXTxlVqhupuJnMT1EX9GjYBxOLh0IpsihVLWQFIqkknL3kReqip59YJXO2pqq9Dll8hGr1OUKuZTEkn9W8Ly5VFwRtv5eXb200Z1kYopjYFUxc3SUoi6h98iixgLVoZ1oX68uXLj2057gXCexricyco4Zivc8+kMmqVEoxz+by5+gLmahJZUjkvKlVsueoGFBsUNp5STOGS/TFnPHRhVz1KYNRSWCzKxRZW1MtpmcQVPMXy0v1hdxgV0i9ycvYVFZ/rC7rfwD2v9dr', 'source': 'donut_slide.pptx'}),\n",
       " Document(page_content='            \"text\": \"14.000\"\\n\\n        }, …, {\\n\\n            \"id\": 22,\\n\\n            \"bbox\":[[1573,3030],...,[1571,3126]],\\n\\n            \"text\": \"50.000\"\\n\\n        }\\n\\n    ]\\n\\n}\\n\\nDetection!         Recognition!          Parsing!\\n\\nOCR\\n\\nFinally, the OCR results are fed to a following module \\x0bto get full information of the document.\\n\\nConventional VDU Model:\\n\\nDetails of the Parsing Stage\\n\\nB-name        I-name  I-name B-price I-price   ', metadata={'filename': 'donut_slide.pptx', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'last_modified': '2024-05-10T05:19:09', 'page_number': 12, 'orig_elements': 'eJztl01z2zYQhv8Kw7OsYvENH51MZ3pom0mdXCyNZ4FdKJxSpEaiXLuZ/veCip3KbjK2nEunli4UlssF8D7ALnDxqeaWl9wNlw3Vp1WtPVpPDiAnlyOlxGS9DsIr55LPWE+qeskDEg5Y/D/VCQde9OubS+LV8LGYRPHITcsdLnmMSH23HS43bUM8Xa2G6/r2/XCz2r3H1aptSpSm73646mjar7i7Xra5Xy9x2Jz0OTdlEH3ajqOcrta8Kc+d+7K91xwDt9gttrjgTYl8UXO3qOc762a4XPbU5IZ3s5RC6hNhTkCcC3MK4VSE8etV+fKy2y4jr4sXyJ1p/Y84EI3QjoxgdE5n7TyQSiCcBEArRP1X+WLg62F0rvZ+s511VsyzGvRUCDGrxw7vNHjf3crY/Ml0PgYokR6isVaw8iGh0tJYUYAoiSiy0VoBRH1EcyiaYpttS4d2FPtgHFoCBspSsMnSQwrKE+tA4MBmf8TxzJ3S0LhPpJwcTMSQNg59xBil1MF58laGrLIDJ52zRyLPIxJjf12YXFyAcWqihBLzyXQ6nYxtmCiQdj4/nJYN0kgVDUlLwucclSEMwkonswx4pPXdlcaIZ1YaDMFIljInidllmUn7GLwfx5NKyjuiObjSHMwgoWeVfRZGAmPwiUBmYImGvMpZHhkcwmD+jPREUAggaYtGBS9yQoPSiciW0B8Pwk/W//C178gGIZnYkyDrisYmUAStYw5s5fFo9WTt3/DAaRzXqy8V4h2nftE1943VW1xvmm7x6vBaEa3wWpuQIJQ6oZFS1E6I6LEAU/FxVvD/ZUUxGYFlOedyY+PAKpUkIgyYxJC08PdY/fr63b76583Q8lcVt7oUA9RZhSDASBcze0AlwYUU88veHY8ux33Ff2w6bNubSTV85KrIX5WhbtthU5WIVWaqhr7CKvdt2/9RNkdVxrdtudwZy7EqlncLHqq8bduq6T5rUaZY9XkX7osc+0x/wfW6eF3xt3ZT0FLrhDZlcNky5ICkghGkxswX0r/Y/pcw7ku7P+u3xe1szfj7Vy9tDsotAC2mWFCRwBgUkQFFwsSkXtQ5R+0r+LrvrkpPpR9sqw9v3lc/98Tt6ZNShM/W5wRU0oOIFkxUKegowYL1CtC/pKSsHqaIR1fcgwKKTbu529W3ZbL6bSidPC1bG4nkwQSbAZGlK/+UIxSUrTbwstb3QxSPrtN9FGcnowJ3B5afblu3z7OT1bpMqzQ/P6vqkZPM/G/ZQEhH', 'source': 'donut_slide.pptx'}),\n",
       " Document(page_content='Transformer Backbone\\x0b(BERT, LayoutLM, …)\\n\\n… 3002-Kyoto   Choco   Mochi   14,      000     … \\n\\n(Off-the-shelf)\\x0bOCR Engine\\n\\nFor example, in most methods, BIO-tags are predicted by a backbone.\\n\\n{ \"menu\": [\\n\\n    {\\n\\n      \"nm\": \"3002-Kyoto Choco Mochi\",\\n\\n      \"unitprice\": \"14.000\",\\n\\n      \"cnt\": \"x2\",\\n\\n      \"price\": \"28.000\"\\n\\n    }, … }\\n\\nConventional VDU Model:\\n\\nDetails of the Parsing Stage\\n\\nB-name        I-name  I-name B-price I-price   ', metadata={'filename': 'donut_slide.pptx', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'last_modified': '2024-05-10T05:19:09', 'page_number': 13, 'orig_elements': 'eJztWNtuGzcQ/RVinxJAcnm/+FFOCgRNmiB1+hIFBskZSovsRdhdGXYN/3u4kgJsXRey81REXUgYkEsOOefMkDP7+a7ACmtshqsSinNSqKAZsxRUdElGFRMmqSlKJrSW1NtiRooaBw9+8Hn8XRH9gKu2u70C3Azr3MXyiFRW2PgaR43QNtvhqq9KwLPNZrgpDu+H283uvd9sqjJrKdvml+sGztoNNjd1ldqu9kM/b1MqI0Ibt+MuzzYd9lnuhtfV35qj4so3q61fYZ81fy6wWRVfdr39cFW3UKYSd1ZyyuWcqjmjl1SdM3dO3Th7k2deNds6YDcaInZd3QQcw7gIXvsYIP+oD04AKCaAqhAFL+7zjAFvhnHwZeebfrQCO7Lw8WtoG1xuKaXhxeL1x8sZeetv2+3w9t2MLLd5Q/rluIXvqFyWQ4WjvocEcc6czkuj5woyU5ZpKzEIx0JUzKvjBNGfmKBj3jslaI86EZTy+W+37dASQi7WbRzluzauyyyZnJHdk3nbycOkKVWfmgPE5V8Il6PyR2hLwkUDwaVoIQgKIgWapAHNpdUG+P9x9dS4evE+pfmwxnm/xiq93IfU+4uP5HWzKht8UhClpDkY6xSzFqMXnGlhteUsaiW4lycdREdddcrGr21H8MbXmwpnpGxI3fYDydCtW+hnZPHm/Xzwq55k/SRbAWUcEEi4JZ6Ew5l4NmXsd9912cxr/Lc4cpJLGb2OiZmkkSXnQTg17tKhdvEfzP2XSJoCN7X6Qx626NB/fcziwJOJ6JCGAEZw6yF6SDZQ61PQQpySr8opgndkmQ1vtsud5mcfyIE7mqIGR23yaJXPlwRPQHNXUni6sI633N2z4RQ6Moo5IFmk+YxgiVJw+YS1OnIPVp80nDltKJp69NNlMUk39snGLtVYFrNnQ65MUtFHY6mhjnGepEWtwaLyQWrmTx7ybVMOmy5vaY88k2c5VfgRpPMWGHVJiEQBIRllRBJRolGRWgNw8kjHZthjfMN/BF/PtUWm8/Umg7UgvPdBQ5AxKMjvTqqmeRTfiRdzu/fi5zuxAcPyX4/JLiiaXE6ccq5rJEQeDJ40yPffi3By/2xgZbTe2KStEhFUopabGIzhwbmMuKYnC+xF21znlfI6viJ/vvqUrzrA6vxJFZrOvmrACk+docoGxlRwGKlgKAy1TwD156mX5cMK7ajHTVl4lVEqq560ieSqmXzwXV82K/JHrsueViwLZ0TMOXJUjsUkaIyCUY1Ji1wXIj2t2+8hFUf9dErFYj4iQA7Pm0PrIBfz3Smfm3tJjn1k+vINDbz7tw==', 'source': 'donut_slide.pptx'}),\n",
       " Document(page_content='Transformer Backbone\\x0b(BERT, LayoutLM, …)\\n\\n… 3002-Kyoto   Choco   Mochi   14,      000     … \\n\\n(Off-the-shelf)\\x0bOCR Engine\\n\\nThen, the tag sequence is converted into a final data format (e.g., JSON).\\n\\nConventional VDU Model: Overview\\n\\nInput Image\\n\\n(Off-the-shelf)\\x0bOCR Engine\\n\\nBIO-Tags / Answer Token Span / etc\\n\\nBackbone\\n\\n(BERT-like)\\n\\nOutput\\n\\nOverall, the conventional VDU methods can be summarized as shown in the figure.\\n\\nConventional VDU Model: Overview\\n\\nInput Image\\n\\n(Off-the-shelf)\\x0bOCR Engine', metadata={'filename': 'donut_slide.pptx', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'last_modified': '2024-05-10T05:19:09', 'page_number': 14, 'orig_elements': 'eJztWEtz2zYQ/isYneIZS8H74Vvt5uA2iTuJ0kuS8SyAhcQxBaoi5Ucz/e8FZXcqO2nFTnLoROIFQxBYcr9vd7Ef338aYY0LzN1lFUcnZERjooYiFTwJzqKyInAeo4oIiDHB6JiMFthBhA7K+k+jAB3OmtXdZcRlNy9TrKxIVY0ZFthbjE1ed5dtXUWcLJfd7ejheXe33DyH5bKuipWqyc+vc5w0S8y3izo1qwV07bhJqQoYm7Duv3KyXGFbxs3yRf3otjdcQ56tYYZtsfx+hHk2+riZbbvLRROrVOHGS065HFM1ZnRK1QlzJ9T1u5dl52VeLzyuekfkZmr1NzgyWDA2aatEiCpRy03wxnDvXDRM09EfZUeHt12/eLqC3PZe4IqcQrjyTcYPa0qpf3b64s30mLyEu2bdvXx1TD6sywfpo/4T/kJlWnU19vaeEsRdUErYaJ21aD013KHmCmkMiTupdhNEv1+CdkbvNkH3qBNBKR//fNd0DSHkbN6EfnzVhHlVxv4Vm6vwthkfNm1T9S4/QFz9jnHaG/8CbYpRCIwZDFxw40vYoDJBgUsJkNFwyKuhefXsIqVxN8dxO8c6Hd2n1MXZG/Iiz6qMg5LIcuGUT5FRjEw7A+C90cxwVOXO071Oop2h+qjKzTEfk0IH6WBGWvxtjTkgqVoSmnyNqw4jqXLJLSCpylCTHlJy7zZ5hpPZ5Jj89Pbi9dFkm7jXsFoVb6/xn9LJSS5lAB0SM0kjSw5ioZRGkUo5dJ+n0/+Jq238tr3+pSw7XSFcfcljdNKD1zRG65NPYLwqB0ASqbDjIfl9Clm1jeBZH2e5f0+Jrl9/fFdqd8T6hFyU6Luu8GZQQdCJW++t0dJKVkoDaga+HBkURDlEXNyn8qyeFoSdobdNx3lerjtyvig2hyEvkkIbpEGvOWfGMaddlMpGGhVlA+L6gPy3OxhRC/DRSs50ZFIbKSIXmkvloBzSeGBjMBun5xfjKcxa8pz8kNubIgKmzRVm8nYJucxhFwYRUiqSEtqXXHCUOecBlOMuaUptkpQPaPcPhDwQ8qDBBsFOlZIuep6M1qF0hxG9YShFsMlaAHeAfXBV6sXuuK6ucJi+LW2cDxxKHRJJsqCFYBhs0XIyCg/hEPCDkb9Yd+UoHgQ6s74ortLcJ+8kKmeQlnqfGJPGWKbkXjWXT0HfGZGPQC9dJ9T1vSQKT1vTguG8iUUblTPAI2nXiwVsfhsQaEk7b25yEUubramarVd4EEX/Lopi0aYRg6SWcq0AglaKSs88xAAJBvxV+X7iVn9zUQT97xFBA41MFxFkglHaiERVAMpMsPtUivVnVWFX6H2NKLKcOuOEpkqCFTFpk4KJ4HykorToe9WGfx3y/10UffwTPzQcjA==', 'source': 'donut_slide.pptx'}),\n",
       " Document(page_content='BIO-Tags / Answer Token Span / etc\\n\\nBackbone\\n\\n(BERT-like)\\n\\nOutput\\n\\nFor each task, the backbone predicts a desired set of tokens/tags.\\n\\nConventional VDU Model: Overview\\n\\nInput Image\\n\\n(Off-the-shelf)\\x0bOCR Engine\\n\\nSTART                     END  \\n\\nTransformer Backbone\\x0b(BERT, LayoutLM, …)\\n\\n“3002-Kyoto Choco Mochi”\\n\\n… 3002-Kyoto   Choco   Mochi   14,      … \\n\\nFor example, in order to conduct VQA, \\x0ba span is predicted over the OCR-ed text tokens.\\n\\nConventional VDU Model: Overview\\n\\nInput Image', metadata={'filename': 'donut_slide.pptx', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'last_modified': '2024-05-10T05:19:09', 'page_number': 16, 'orig_elements': 'eJztWE1v3DYQ/SvEnhLAa/Obom/5cAGjSdw6m1ySwBiSQ1uwVlpIXMdu0P/e0dpBt05aq0APRby6CKLIIee94eMMP3yZYYNLbMtZnWaHbAYpRpulQO2FR68MVwDSV8BjTMnx2R6bLbFAggLU/8ssQsHzrr85S7gqF9QkqEeuG2xhiaPF1LXrcjY0dcL91apcz+7+l5vV5j+sVk1NVuquPbhq0363wvZ62eSuX0IZ5l3OdcTUxfW4yv1VjwO9N92XzV8+R8MNtOdrOMeBLH+YYXs++7RpHcrZskt1rnHjpeRSz7mZC77g5lD4Q+7H0SsaedaulwH70RG7aer/BCcFhwmj5hWX1gBEawzXQQRCDTLE2e80ouB1GTs/Pz6ZL+B8YAfsWTt8xp4tukts2dsVtNSGJY5TfkVhUZcGx/H3CTHJeR4DaKiyiUaZKmeVK6kzF1nLHSHTCYF4GboWJ8HuggB60mg0Bh1NqnJKRnigpcpqB/tk2J88PzpdzJv6Ep9OQt4GYXJEL23KmqP1Fc/aIaHPnRDB7ZCfivzJuqzWZRLoEDBYzCp4aZJSPFnvlJVovCCVsf5h0PmPC/qDEbkN+k9dzxDiBSswXO6xcoEs3AkPo2WnOpaBAUs41PTFBiysy6yMR8NwUOi82N8m7A30PXl5hYvR/HeI81pqHcHGLFy2KLKHpLzhSWVPK43fEPd/4mgbt22vf6Fuz3uEy+96HGQMKSo0OvHkbA48aitpVyR0FLKPKVTdNoIvuvaKZqJ5oGHvX75jr7uEzSE7ucL+qsbPk4RAA+17imunoxRJUzKYHDgSGSEheA2PSX3dfSF4MPS26ThuSXzZ8ZJsTkI+pUDam7TKKUdLaQbmyJUBdIp7G8wO+anIPznJeU7COx8usMlPP6455+HkxSk7as/riflfdllKa2VMgc5VrjBZqSBIh45nJcSjUplvspCHQnWbjbeLZ6cL9r3n6M1LxrbZeNfeoVj/hunvTjzgMljwIpgYqyCcA42BptUJgtPVbp9M3ieLHtph9IJq06/F0e1m2eTse+wV3HTr8ur1Hvu4pgXZaQk86OiQ2NHah2yVly5j0DqgDBkE7AiaTNCIuoiKczn/+aYrHXtx0cWOzvV4UW/+pWlXCMChMlQuRGuVRu0gh+AqA8ZUKclHldx/Q8iD0XqfEGnZFiPsjhN2ywq9hd67lbe7zv9a4BAVCZutjEwCfFW5DNV4K8d98EGICVcPO7q2arFrWK4a3GN1y7o+kdQRa7Fr0zoW9v7XZ6O2jZIHbBgv5urha5VGxVl3NXanCo6Shzl9j3bvSrVdlfbPVRoiF5CRXKyirHRM5GeVFGIyQbsp92c/ThBX/3mVZiK3USmrpK2kE9FnFwyVazwbkNFXj+mIre5LxIOhN71K+/QHNCce6A==', 'source': 'donut_slide.pptx'}),\n",
       " Document(page_content='(Off-the-shelf)\\x0bOCR Engine\\n\\nBIO-Tags / Answer Token Span / etc\\n\\nBackbone\\n\\n(BERT-like)\\n\\nOutput\\n\\nAlthough such OCR-based approaches have shown promising performance, \\n\\nthey have several problems induced by OCR.\\n\\nConventional VDU Model: Overview\\n\\nInput Image\\n\\n(Off-the-shelf)\\x0bOCR Engine\\n\\nBIO-Tags / Answer Token Span / etc\\n\\nBackbone\\n\\n(BERT-like)\\n\\nOutput\\n\\nAlthough such OCR-based approaches have shown promising performance, \\n\\nthey have several problems induced by OCR.\\n\\nConventional VDU Model: Overview', metadata={'filename': 'donut_slide.pptx', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'last_modified': '2024-05-10T05:19:09', 'page_number': 18, 'orig_elements': 'eJztWE1v20YQ/SuETg1QOfv94Vuc5uBDqyJVe0kDY3Z3ViJMkQRJyTaC/vcMhRRV4wCmEQMtbEIQBC53ZzXvzc682Q+fFljhDuvhqkyL82IB2kgReBbBMO65FNYGxbkBi5JloRc/FosdDpBgAJr/aRFhwE3T3V0lbIctDXGakcsKa9jhaDE19X646qsy4VnbDreLL++Hu/b4Htq2KslK2dSvD3U6a1qsb3dVbrodDP2yybmMmJq4H//lWdthT7/H6bvqX4+j4QrqzR422JPlDwusN4uPx9F+uNo1qcwlHr0UTKgl00vO1kyfc3/O/Li6pZVX9X4XsBsdcceh7h9wEBmHjJolF4VTMXk0LknEpIOyji3+ohUD3g7j5B9WOS+HLS77LVb51Z97xlhYvX1fvKs3ZY3jfn9DsC6HCsfFX7MRo3EcdHTMMCPok4S0Ilhw2evIxczGVDYuLlfLNWz64nXxpu5vsCvWzTXWxW8t1DSGQ5xEiHecBSWMzEI5q60LUXLBHRAZWYU0EzKZEIjXoZl4DiTzSZssvdEEvkuKaQ5C5KxMMihn2KdnpYt379fLqrzGV5OQN94kGX2isJde2cC1USAT2qgNiDRnoMnIr/ZDux8mga5M1Dxqpnx2QYaEMesQnMrCChTWPAw6e76gPxiRp6C/qYZts99si34ftwVV32WAHlNB/nYNxC32xRYOWPTb5qYuaGxX9mW9KVrsjr7XEcnaCWe/QNeRowdcjzt8g7vks08OWXZOxJxEllEKDgZQYE7ezdxN5Y60090XdvCAHVQjP4HA7ouyTnvyqQh3I6dnjyLIK6FUBBMzt9kgzx6S9HSGZabj6+M9gv5PXJzic+r1rzTtokO4/pbHIXohosggELkGoQIXFImUt7T2Ae57/IxD0p8i+LapD7QT7UPB9cdPvxc/Nwmr82JF4XYo8WaaRtc2y8Qw2cyD5NEBBbcAABtCMpq/pArpvz7wD4beKR2XNRXI4nJHNieWSbIIPvCgvcocUUrUGrRKghokPSHVzsg/Xa+aJUYtjCdJRF8blVLEh0Khow/W55mNqWw8Ua+KmludHbMoAaPTmjmTvSOF6khL6hd1lfOdhDymV6UeNQRQpHJ08mC1Y4ZkvBDKWM+9gxn2yVnpkb0qxmgVQ+mi8yGaLEEbY0h9k76UycyVeDLyj+hVSTuzZILPRkQEY6IBSAQ35Biy8/ZFict7FwQPReR/3KtGqhBMJ6oMLqMAC1rAeHvhZDRCy5m7qdzNveoT9arAY/YSrdFZconJKe4pRxEQmiHwCSry2YSkYN/Zq378DPCyAMM=', 'source': 'donut_slide.pptx'}),\n",
       " Document(page_content='Input Image\\n\\n(Off-the-shelf)\\x0bOCR Engine\\n\\nBIO-Tags / Answer Token Span / etc\\n\\nBackbone\\n\\n(BERT-like)\\n\\nOutput\\n\\nhigh computational costs\\n\\n First, OCR increase computational costs.\\n\\nConventional VDU Model: Overview\\n\\nInput Image\\n\\n(Off-the-shelf)\\x0bOCR Engine\\n\\nBIO-Tags / Answer Token Span / etc\\n\\nBackbone\\n\\n(BERT-like)\\n\\nOutput\\n\\nhigh computational costs\\n\\ninflexibility of OCR on languages or document type\\n\\n Second, OCR makes it hard to handle various languages/types of documents.', metadata={'filename': 'donut_slide.pptx', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'last_modified': '2024-05-10T05:19:09', 'page_number': 20, 'orig_elements': 'eJztmMtu3EYQRX+FmFUMeKR+P7SLHQcQkESBrWTjGEJ1V/WIEIcckBxZhuF/T3EUxbIjQDSkRWDNimCzWGTf04+6/fbjghpaUzue1bg4qhYuRaGccRZBOhNyICQjTVBSFg8GFs+rxZpGQBiB4z8uMoy06voPZ0ib8ZybJEeUuqEW1jRlxK7djmdDUyMdbDbj1eKf5+OHze45bDZNzVnqrj28bPGg21B7tW5K169hHJZdKXUm7PJ2+suDTU8DX3fh6+aL2ylxA+1qCysaOPPbBbWrxbtd6zCerTusS027XiqhzFLYpRSnwh7JeCTi9PaG3zxrt+tEPUcpsWvqP4sDMpeoyTtbtNSEwchIVkSjrCCQZfGJ3xjpapyCj9vNdqyO15xzyn3T3dN6bGgK/Fr5IihryA5UkDEbm8n6lAUoQkAMYq/8XOV/OCllOZ7Tcjinpjz7ayuESCcvX1ev2lXdzqORYwGJzkCRWiYZi3cCUuTpETNYVHsac2m8OD5ZnsJqqA6rH9vhPfXVaXdBbfVmAy230ZhnAQHvFXkFvCClUDBgihCjNx7Qu6DNHshsIJAvUjdzHqBGa3PkLcBIkOC1wUnzqFTBgmD3ss9elV68en26bOoLejZLeU8GREiOlyFjfBaUVInCSJkzIzFlr/xc5U+2I2/Fs0SX3gBGY43DJAIEg5aUccFF7VRI+n7Rxfcr+r0j8rbo5/XqvMrdmpXf/R40fDeMw20Mv9TDeDzS+i4SCh2GXIqSmFEyAitBZaOdK15kn/Yk5pKofq77YXxeTRVQ3eaeYKC7yBzcRvMb9D0/vaTTKcsdfHiqGZPB5SJ9cSRLBNTRCtQlkov5P3z+Tyhuy3O7179z2AsW6OKuHgsim4IQQEYUFzzqaUXIKZDDCHZGgf79jEh5W8GXXXvJX7oeSn/+9Ef1a4fUHFUnl9Rf1vR+XsGNXNvpLIxNNmRJkZKPxJ7IIbf4J1Xfya/n+71D7yHGE4vILsZo2PJgDtYk57UUHoRRktfdvfJzlX8M42k9GpGNkLy0WwrIXwMuvIWKFpwvfk9jLo1HMp5eU3FKRC0cb68Wii2Za0PH/kfLIvfTYz6QbzGezuUYKRZUImCWTpnE8yIY3idsoPykDmAeuCp9o/F0bKh4D9DaJiSK053VoXChpy0q9aQOIh+m/DcYz+JL0DYKiRooKOUFmKygCC3Z1ooZbv87Ki6/Fv3eEfmYxpNBOszReaU9RZ1SsoJLI8f2BnQKMwrRPYlrEnVbGrqqU93U44eqKzsH2rXVv79YdX11069qp8ZMRkWxyzUuQdJkfSnFGYE8G1NEUE4+LSv2IEbVG8pdi9enA2u4YCj1WJ1Dj9XY8bXFhqpL6OtuO3wGdzjJMkxIb7q8Pz344vTg3d/06nHB', 'source': 'donut_slide.pptx'}),\n",
       " Document(page_content='Conventional VDU Model: Overview\\n\\nInput Image\\n\\n(Off-the-shelf)\\x0bOCR Engine\\n\\nBIO-Tags / Answer Token Span / etc\\n\\nBackbone\\n\\n(BERT-like)\\n\\nOutput\\n\\nhigh computational costs\\n\\ninflexibility of OCR on languages or document type\\n\\nOCR error propagation\\n\\n Lastly, OCR errors are propagate to the subsequent process.\\n\\nProposal: OCR-free Approach\\n\\nInput Image\\n\\nToken Sequence\\n\\nDonut 🍩\\x0b(End-to-end Model)\\n\\nOutput\\n\\n To address the issues, we introduce a novel OCR-free VDU model, Donut 🍩.\\n\\nAS-IS v.s. TO-BE', metadata={'filename': 'donut_slide.pptx', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'last_modified': '2024-05-10T05:19:09', 'page_number': 22, 'orig_elements': 'eJztWctu3DYU/RViVgngcfgW6V2ceGEgrYNk2k0SGJfk5ViwRlIlzcRG0H/v1ThxJ45bK+imsGczD4q8JM+5j0Pqw5cZVrjCejgv0+yIzQrMSWnLC+8lGKGlzIFHh1ZnlZLwswM2W+EACQag/l9mEQZcNt31ecJ2uKAmTj1yWWENKxwtpqZeD+d9VSY8bNvhavb1+XDdbp9D21YlWSmb+sWmTodNi/XVqspNt4Khnzc5lxFTE9fjKg/bDnv63nZfVd/9HQ1XUC/XsMSeLH+YYb2cfdq29sP5qkllLnG7S8mlnnMzF3zBzZHwR3y7sZZGntfrVcCOekn5J7UNeDWMQ1419YZmonmgYr+//o390iSsjtjZBrtNiZ/H8d+2tCiHCmc0+i66DnQByK03PnlVBGFRWw+ovYs6aPMwuuLxoLtt6n7C9XbpOK3b9cBOV2RzEvLBKh6CILvgo/LJxEgtXOjAUTuV9shPRf7ZWc7z4QLn/QVW+fnHNec8nL16x07qZVlPY0OBVaiChMhtlAKi88FoozBsgwP2bExl4/j0bL6AZc9esJd1/xk7tmgusWbvW6ipDYc4iRCRIhTBZ5W5AJvAuGikFJZSlFY2FHtCJhMC8TI0E+MAVAIrKYAUpGSKZK1yKmshs9decL2HfXJWOj55t5hX5SU+n4Q8p4prIigqxNnKGDBqdBZTktpmW6g98lORP1sPVIonga4daqUEAS68jCpYRNACORgtAyTxpMTlXdAf9Mhd0C/K5QWLzYqQh6+aNDb90O/S8Kbsh9MBV/fKIQfCC1A6Fio4brJWaEV2EVwobJ4gh/ZM3DBR1rnCqzKUVTlcsyazUQo1NbtdIms69m1fbIvGRI7G1esYMWsk5coxZgAdlRDaJOGS23M0laOREuw6IqLtGrJ8u8YpNIAKSWXlHRilCyGCMQAYCirTFEUwQRrtabihgb2hWavrA3bLR8/I2C0pyIaG0cmC9evQ4x/rMWDoWcS+P9xl61foOtrfBhej4Xso83qMHLAxiyJbFNlDUt5w4tGj9fEHyv5P7Owitrvrt9TtuEO4vPdA5U1hnc6UL8hfXXTK2xALSJ6IFVMOVI/HSdUugm/Jt5oexouaV+/muUNkL1tyKogXkySLV15bLzVkqo9OOukFEp4KgDKBTk9KJ6q70f+g1/2XGxtlY3ApaoMgXCwwgQxynKFALrjYIz8Z+a83AtuMGqeBnwyiUhCTDg6p5NF0Qntw0RsJQf2YQffg/wP4r8d9s4/r5FSkz2z9zY3Zs5M6zYdmjnW6uUqedm6V0XHnMilCSkEyypBSITHkQlGCIsr2xEwl5ifOrQl4ocGkQtks0CoHNgSkQqBlNlLyJ1Vd74L+oEd+JwEXDYOUaIH9VumVfb/G/oB9pp/10DVpHZEBq5sNVn9X7PF1y2qMkQP2YzjtpeG/S0MBhU50rien1TxkLgSEGIzCyIGDnPDO6fE4r95F8OX7+el7tjnsD9nibH58ck8q+PQXU6Vl0Q==', 'source': 'donut_slide.pptx'}),\n",
       " Document(page_content='Input Image\\n\\n(Off-the-shelf)\\x0bOCR Engine\\n\\nBIO-Tags / Answer Token Span / etc\\n\\nBackbone\\n\\n(BERT-like)\\n\\nOutput\\n\\nInput Image\\n\\nToken Sequence\\n\\nDonut 🍩\\x0b(End-to-end Model)\\n\\nOutput\\n\\nWithout OCR, Donut directly processes the input image \\x0band gets an output that contains desired types of information.\\n\\nOverview\\n\\nOutput Sequence\\n\\nDonut 🍩\\n\\nConverted JSON\\n\\nInput Image and Prompt\\n\\ntransformer encoder\\n\\n{ \"class\":\"receipt\" }\\n\\n<class>receipt</class>', metadata={'filename': 'donut_slide.pptx', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'last_modified': '2024-05-10T05:19:09', 'page_number': 24, 'orig_elements': 'eJztWF1r3EYU/StCTwlknfn+MKFQp35woXFIDH2Ig5m5c2ctvCup0qzjEPLfe7VOqRMbohICJdHLLhqN7uieM3POvXrzocYNbrEtF02qD6sahOegHNpohBComUNhnHAguI7J6PpJVW+xhBRKoPkfaggF193w/iJhXy5piNOM3GywDVucIqau3ZWLcdMkPOj7clN/ul/e9/v7oe83DUVpuvbpdZsOuh7bm+0md8M2lHHV5dwApg5201se9AOO9L+fvt18djkF3oR2vQtrHCnymxrbdf12PzqWi22XmtzgPkvBhFoxveLsjOlD7g+Zn57u6cmLdreNONAsofZDw7/g8GBVguBcMIrFzDgPEaKWCCywIHT9kZ4oeFOmySdtvyvVyZZiTrH/SfesKRucJn6JPIucJS29cJxlFiU67aKyNksdkkluQX4u8o9Oc16VS1yNl7jJj893jLF4+vxVddyum3YeG1EYtFwzA1yapCF7FaTGQIBE5YxY2JjLxtHJ6eosrMfqafVrO77DoTrrrrCtXvehpTEsMIsQSMYbUiOFPiamhQtOaR2iiclZgLAQMpuQAFexm3kOcsZkpSTYDTA0TIBKmSmpMJtomFpgn61KR8evzlab5gofz/MD71MKzDN0kfscFTidpREcyBEAFj+YjfzprpAVzwJdgc0mG9BKIXqhtIQslOfCR6l9WGT/u5U/ImHKHp00jmWY9nqQtBCL2huvJFuQn4v8J3vFv3bYwjzwIXpvwOuchGKQrOYhIWek84FYEGkBfy74v015V+e75CTQbzb+tvx8dNymVelW2Kbqjy7hZp4JcGWCEgqUpaqT1mVMah0xkBxxCU4uxHwHE6BiX4OAGGTWiTErjLTkxgbBJ2ElfB109gOD/rUdeRf0P5ty2dF5oObrSXV7NFIzIJTN+6ofOsBxxLGiVq1q9nbRTHZR3Z6YQCdljWWsqE3o9uTRxFAq6CjXph2rhCPFStUE4lh1mWLcAkY4HNwl+kUYBhq9xrPptR4g3FNCCoKBzMn+kWcfkvSaJUmOZPx9wv9P3N7F+27WL2na0YDh6qGMrSGFd5oDD17F6JN00qED0EKARfMzbXH9mUxc43Dd4LtZQkGGGTW3Vk3fBoxFkgxlGFeBpCILMwPFH0ed9ZdC8dUtdl+d/1vRYmx0SnvBpc3OOuktk8HmkAyT1DvlBf256N8vWmYR4JRgKSJSN0qNqeaCuiSUQkepbGAWFwLmEvC8a0l3CpnZ769PX8wCPyBal2i3UzgK6xCBQ9Ca06BKtwwu4M8B/06nWk1Fx8uh2/bzKkVrk4+A3MlA2s+VzZmKz5itUTJ69lOV599GQhlCO05Z4FCRA1CHNMz7YMNstNwgNarOq6SMt4YW4Vx7BS79VB9svo2BD9V5DbTyeF4fntdUpGPTl/O6+jiLB52ohDYKvXHMiRQymQCaQIKEjuW0yNFsHp7tSfjlEwHPnt5ePkDC278BZIyQnA==', 'source': 'donut_slide.pptx'}),\n",
       " Document(page_content='</classification>\\n\\n{ \"question\": \"what is the price of choco mochi?\",\\n\\n   \"answer\": \"14,000\" }\\n\\n14,000</answer></vqa>\\n\\n<classification>\\n\\ntransformer decoder\\n\\n{ \"items\": [{\"name\": \"3002-Kyoto Choco Mochi\",\\n\\n      \"count\": 2,\\n\\n      \"unitprice\": 14000, …}], … }\\n\\n<vqa><question>what is the price \\n\\nof choco mochi?</question><answer>\\n\\n<item><name>3002-Kyoto Choco Mochi</name>・・・ </parsing>\\n\\n<parsing>\\n\\nThis is the overview of Donut.\\n\\nOverview\\n\\nOutput Sequence\\n\\nDonut 🍩\\n\\nConverted JSON', metadata={'filename': 'donut_slide.pptx', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'last_modified': '2024-05-10T05:19:09', 'page_number': 25, 'orig_elements': 'eJztWMlu40YQ/RWCZ9vqfTEEBcjklCAzAcY5WYZR3V1tEZFIDUV5ieF/T1G2E3kZmPIpGEsQtDSbvbzXVe8VT29LnOMC6+68SuVxUUZlAaUCVComZrMwUgonpEBuuOCiPCjKBXaQoAPqf1tG6PCiaW/OEy67GTVx6pGrOdawwH7E1NTr7nw1rxIeLZfddflwvbtZbq7DcjmvaJSqqUeXdTpqllhfL+a5aRfQrQ6bnKuIqYnrfpVHyxZX9L3pvpg/+dsPPIf6Yg0XuKKRT0usL8qzTeuqO180qcoVbnYpmFCHTB9ydsL0MffHzPd3L+nO83q9CNhSL6E3Te1/4FjDITnNIwevQvBJOunQxaiFiBZNeUd3dHjd9Z3Ho0jzrmjO+81N+hkeN31SdXPsuz/HP9BY0UdmGHdKJDBJKWksqiiFZmEA/uzHxf/Nw7mN/20xLb+tcdUvbUoN0/JqBl1RrYpuhsWypV0VTS7irIlNsWjirPppWh5sk/QZ2pZ2dokn/ZCvkGWlkZZzzqRjDkSyznHhwaCmNTov92QNJasoiB+oV1fY3nPF1QFjbFoWd9uM/Fk/IFj9jel7rBjruGOSMVqB0tEjsMgYoFGWWa7YnpWhrNyTMB7dEzMZjy6/wWRnPiy4mLzy2QdluVbGR21dpkBBJ6LJe0kZLCnvURRw1riIkGKA6DKlKZGTzWBkyj5B2MM/FP6upUDod4FtkTA2iQYdwkBWqH0OpBAxMDQgvclSKUtab0JKAxj4gRPSm8fzuaZXHS5WvUic3k7LHpB7waB8Lw5/u2m6pvi0kfTfe0nfVdGTUQKcktqhgMAFeJ+TCUYbIVw2fE/VUKqKzWtaxmZddz1F4mBn5fA5E/ooI7fJGlDK6Whttlo5ZXL42P7qXWys66rbWN+eEfJCjACarmkF5u7s8dc7LBd5hwDKU6WCmLi1ztisEoLIhgUl/J6ooUSNe4s1fqxbJi9Llp3SGTm7qKP0ATBazThERtGUMgspUjzZvfYP1f5nleJ49C9F4wd3PMgKUGQITtY7cxWYC+TDAxX7lt4okjQf2wq8eVqfBErvAybjHobJ69o/Hm0uTteS5bD9WYxHNO+qqi8mO0WTBy9Y/yQmBskFggGGnNZoGbHI8scuLHcj7134B+OMzJJ7mbSmKEKTgkHhovHaI4M9/kPxP5mRpjzISnOJ7WWFV/3DsF96PI52CwolKJ+BoZRms0GePSTpNUsye6RS/wUp/yf8tzHZ3vUf1O3nFuGvV2sE7RgD4wKT6GwArikTZJuYtNlxhR/pGJptBL88nKRhD7rRJycUME86bJ1VHMk7Oh8M0CX+oR5LmOfB/OYRewL7uluuu+IrkiWpIw5CX4tsfbYhMcuyMs5TaUU2SAZJQcui2aM/FP1NyqTKKTkZ6TMbP4iAmHQKLHDmPKAApbm0VijhTBRSh5dpc0/Adwj41NSUdzpMxa9fv3x+BfyzfwCD6Eyh', 'source': 'donut_slide.pptx'}),\n",
       " Document(page_content='Input Image and Prompt\\n\\ntransformer encoder\\n\\n{ \"class\":\"receipt\" }\\n\\n<class>receipt</class>\\n\\n</classification>\\n\\n{ \"question\": \"what is the price of choco mochi?\",\\n\\n   \"answer\": \"14,000\" }\\n\\n14,000</answer></vqa>\\n\\n<classification>\\n\\ntransformer decoder\\n\\n{ \"items\": [{\"name\": \"3002-Kyoto Choco Mochi\",\\n\\n      \"count\": 2,\\n\\n      \"unitprice\": 14000, …}], … }\\n\\n<vqa><question>what is the price ', metadata={'filename': 'donut_slide.pptx', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'last_modified': '2024-05-10T05:19:09', 'page_number': 26, 'orig_elements': 'eJztl0tv4zYQx7+KoHMefD8CIz30tCha9JCe4sAYcoaxAOuxMp1NGuS7l7ITtM12d+U9bu2LTIoccuYnzp9z+1zThlrq8qrB+qqqSSppHHMhWuTonbMerGFBOATUjNdnVd1SBoQMZfxzHSHTfT8+rZCGvC5dvIxIzYY6aGmyiH23y6vtpkG6GIb8WL++z0/D/j0Mw6YpVpq+u3zo8KIfqHtsN6kfW8jb8z6lJhL2cTft8mIYaVue++Ht5l/NyfAGuvsd3NO2WL6tqbuv7/a927xqe2xSQ3svBRPqnOlzzm6YvuL+ivlp9lBmrrpdG2gso4TZd41/Bwe1YwyMC0ySswG4BgPJIpM2Oa6ofikzMj3mafCHbtjl6kNbbFbQYfX72LdDnpZ58/ymyZv9nM8g2BJ2x0TAwBGilIoBGBOAgVNCmxOEuRDyCN128oLGirrYYzE6iwCiSYoHsEDcOnAQRDkZGpCr6CmeCMwl8Fwt61hW3i7rq2U9UqRmyMu6epnFASKZqBEdRYzGSwpCFR4oohCMSXbiMJfDYg/h+hXA4vLQnAVBgqcQVOI+JRaZZVrYYIFJZYPjQZ0gzIZwCHtZ8+DcvPg77hQmG6IvuZ8xnriQEVPwJskiCzM0mf248f/mx/k+GX3c0XbaWslHpfVpDblqtlVeUzWMxauqT1Vc97Gv2j6um5+W9dk/If0G41g8e6CbyeR/KYchKQ060MADS5gEtylIBwTKaJtOsObCqqrCp8j3JxoPrLg6Y4y9144/utcINn8SfokKhCitUkEGTjFFy0GBVDy5qMkp8icqc6kcICwuD2CuF5cPH+H6aB4sOm+cMRojllMhAmM6aE+KlerDBTxJynG6fqSiEMYotS03KlskJSnjPPdacCJhUXN3Cv/3FBhI8wsMUepri8ahNFIE7zEkp5nhqpw+FdgMAj9wQvrm5/le05tM7VRgVLfPy3oKyEEwJGPi/JenPvfVz3tJ/3WS9GMVnZf6A4BZHUkKLCkq+BBJWSsMCG/+39pxFKpq/yv1YL/r8oRInB2tHEp7JpNR3mhdBIrACIkUUggChEJ9onEsjV3X5P3VdyLCVVH3s2q5KzswL3dv/77jyhWB+yAFyRQ10y6qcl6Y9CGQpBj4jNL9BOpV4qcr1uKtbrn+vGT5Sjq7+wtgTv42', 'source': 'donut_slide.pptx'}),\n",
       " Document(page_content='of choco mochi?</question><answer>\\n\\n<item><name>3002-Kyoto Choco Mochi</name>・・・ </parsing>\\n\\n<parsing>\\n\\nThe visual encoder maps the input image into a set of embeddings.\\n\\nOverview\\n\\nOutput Sequence\\n\\nDonut 🍩\\n\\nConverted JSON\\n\\nInput Image and Prompt\\n\\ntransformer encoder\\n\\n{ \"class\":\"receipt\" }\\n\\n<class>receipt</class>\\n\\n</classification>\\n\\n{ \"question\": \"what is the price of choco mochi?\",\\n\\n   \"answer\": \"14,000\" }\\n\\n14,000</answer></vqa>\\n\\n<classification>', metadata={'filename': 'donut_slide.pptx', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'last_modified': '2024-05-10T05:19:09', 'page_number': 26, 'orig_elements': 'eJztmEtv20YQx78KwbNt7fthCCrQ9JIWTQLEPUWBsbszaxMVHyEp2W6Q756R5KC2ktZ0T0WiCyUulzuc/29nd2bffSxxhTU242UF5XlRouZZJCZstExKDJYnw0LgyLQDq315UpQ1jgHCGKj/xzKFEa/a/u4SsBuvqYlTj1ytsAk1bkeEtlmPl8OqAjzruvG2vH8+3nW756HrVhWNUrXNbNPAWdthc1uvctvXYRxO25yrhNCm9fYrz7oeB/rdda9Xj263A69Cc7UOVzjQyO9KbK7K97vWYbysW6hyhTsvBRPqlOlTzi6YPuf+nO0c6+jNy2ZdR+yplzC7pv5vcUA7xoJxkUl0NgaugwnZApM2O66w/ERvjHg7bju3uUjXbWqLuk3X1U/z2Yc1DtvvXMxDM9xgv9ia/KLCRTWudu8fArFBc5QGwYWguLNGMufJplI5ZhDyaSDs+wXy5Gx9CGRejVgv5lsZFpIxcfrbXTu2xYsdpN+3kOaz3cPlWrIcH16L+YzsDlVz9Qjaq9D35OkGL7YmvgFPhwQ2Gs65QMW9pLEU0wZoGjmXvDvCmwzvP+kfvQIIRujgKWCV0cYEi5lpLbR0Uh/1n6r/xTUWm2pYh1WBTWoB+6IO3VCM1F413Xosqprs0X8KqVAMOBa0ACIZByBsw9mzuHkllErBpMxtNsizDyC9ZiCzR+PTV9z+T4geyvbQ6zfU7ecew5/f8pgmKQtZB5qbUtmkveSSefoOl3P25muPv+OZah8q+HqD/abCm0mbpePZGQMoICTOMtdGeZd0UmhzlDb+SNmLPYz3J6fYI9nX4zao3yJlLU3CSepToIZMNkJ0RtJyK4OQQiPZ8HSvzVH9qer/svW7WK7ByUTXbPwkAByyFk4xJZFFLjyqpBUlHsJCjBwm5IpHAHsAL9qG1p0Rofj17etXE8U3wEWEBFJm0CQ8D1K6rIRISXg4ij9V/Je7fOLlLp8IDRRv+rbuxmlLUIaYZDQyZ8pjJHcAlNXYzLzyIsZjBEyGMPZUpW69oEzvPuObFgZeWgdOqeBYNApJdZ2dt5gsteFxE5hM4GOxLBNZHpbl+bLsMWHVjcuy+DSJQ1aGyxiz9gBUwVqEkCkWQHMJXogJpc+Rw33puYOwuAcwn+1vJ0FILidBCalhFrjnnqoXKrmsp61B8eiPEKZD2MtONvfOTdNf54QOogoBmMnCUk3gGENmZUZBadIPVVUd6v/k5DxcjL4cYNJ6RHc314GK/v0JQNeTV8XBieeyPHlW0W9dRIicaogkraWKQTDLrLMm28h0nrBzHGHtYRUF8dkfMu9ZcXXCGDvcO/5o7hWs/kL4JyogLQoRZUxURggKHS54CtJrTcWJtuxIZSqVPYT57P70fz7bfAiLZ/MQjioMhSkGli0kluivUQ4BksqBTThSPm4pD/f1f91R3n8GKrG7mQ==', 'source': 'donut_slide.pptx'}),\n",
       " Document(page_content='transformer decoder\\n\\n{ \"items\": [{\"name\": \"3002-Kyoto Choco Mochi\",\\n\\n      \"count\": 2,\\n\\n      \"unitprice\": 14000, …}], … }\\n\\n<vqa><question>what is the price \\n\\nof choco mochi?</question><answer>\\n\\n<item><name>3002-Kyoto Choco Mochi</name>・・・ </parsing>\\n\\n<parsing>\\n\\nThe textual decoder processes the image embeddings and prompt tokens.\\n\\nOverview\\n\\nOutput Sequence\\n\\nDonut 🍩\\n\\nConverted JSON\\n\\nInput Image and Prompt\\n\\ntransformer encoder\\n\\n{ \"class\":\"receipt\" }\\n\\n<class>receipt</class>', metadata={'filename': 'donut_slide.pptx', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'last_modified': '2024-05-10T05:19:09', 'page_number': 27, 'orig_elements': 'eJztmFlv20YQx78KwefY3vswBBVo+pIWTQLEfYoMY49Zm4hEMuTKdmr4u3eWdlo5cWC6T0UtPVA89pzf7sx/9uNNDWvYQJvPmlgfV7UGGpgPwA1I50kiITkFxCqlCONJ1a+qegPZRZcdlr+pg8tw3g1fziL0+QJfUSyRmjW0bgOlxdi123w2rpsIh32fr+v77/lLP313fb9usJWma48u23jY9dBeb9apGzYujwddSk2A2IVtGeVhP8CI/1PxzfrBY2l47drzrTuHEVv+WEN7Xp9Ob8d8tulikxqYZskIEwdEHlByQuQxtcfElto91jxrtxsPA5Zieno1/GMcpxhxSbpEJBc6SMspJxZHZ1JKVoX6FmtkuM6lcB5cO5ZZwFBFCF3ERsvn+2mfNHkNpcK3BAjQmIRhXFvKbYzBmRh54tZw5Wm0TxMg/18CTy7PXQI31apuMmzGVRnLzaouBin3q5oTwg5++9Llrnp90YWu+r0LF82qfrWL6K0bBpzXJZyUBh9BZbRPkUnsWxpLqZRglOciBR1pECHuUc1FVU2/VR26bZsLIvaAxB/tveWaPyH+iIaWSRltpZTUhQjYD9VOOkOp0IpB2NN4Lo1t2+R+wDkUIlQQggZabXEE6vb06111+2xQkhLOtVLaUIUOTQJRnEWjFICHJOQe1FxQi8vPbrn4vIWxDG55deFy1YxVvoBq4lY9y535pATuFu+cEpEJa6PT0QQiHfjkmd/H/rmxv0tVmMLKpoSVnxZHfyNaoCi4gmE5Swow6zC+JOM0YSQFjDFaCA7AQ5ISl8iL3ihPrtYHG6XogOWimGH5eOxfHE0fV1tOkt+9Vosj7Hds2vPls3ZTtF4A8SkQq4GBY4Jb1HDeBZfKaPfwZsP7V/b3GqOK0VIxA0Yk6qjjiRggAozHx73959r/BMNJud+69ddMBsNLF2Ac4S7YNBvsrwLsLkYENVaujaXIps9V7j5BOx4+i50VTIjgVEhUJwU0WRe5lQTzIAvKfi/l/kuYdk23O+v3WOznAdynx2ZMBWDSoIg1GoMMt4IEJtHbk0gCJdq8pNVqdi347hKGywau5uXOPhEfcbMHUJQxzW2gKjh8Eb2m6UUpGPPtnn9yiT0w+zb321x9AFQubYBZ1ufW+GCLMsJEGIWrSi4ZTMcwBkaUtDN0/d76d9b/pcwbE6xoeMBrUnYWAMtEkAp9ppSaKMaZkMA904Ezi+5U7wHMBfC6a9HvZIjVrx/evZ3neyxzSRtpktMyCSMEZZjKSSVw6UsJe+PPNf6btrieN5OmKELi/SQkZkEIkqO0U9yHSJkXUnkjUM7YaDmRlMw4PN1D+P74GiPA7ONrH7ngETx4y4NMFvPXZDDtwRRIeQd0T2AugXJ8HbDncny9qgcI0PR5VT88cPshB3RFYAkJ1iEF4q1kXMTkFEs2cJFmCMo9h/v0c4KwvAewOLp7fATC6V8YM8Vk', 'source': 'donut_slide.pptx'}),\n",
       " Document(page_content='</classification>\\n\\n{ \"question\": \"what is the price of choco mochi?\",\\n\\n   \"answer\": \"14,000\" }\\n\\n14,000</answer></vqa>\\n\\n<classification>\\n\\ntransformer decoder\\n\\n{ \"items\": [{\"name\": \"3002-Kyoto Choco Mochi\",\\n\\n      \"count\": 2,\\n\\n      \"unitprice\": 14000, …}], … }\\n\\n<vqa><question>what is the price \\n\\nof choco mochi?</question><answer>\\n\\n<item><name>3002-Kyoto Choco Mochi</name>・・・ </parsing>\\n\\n<parsing>\\n\\nThen, the decoder outputs token sequences,\\n\\nOverview\\n\\nOutput Sequence\\n\\nDonut 🍩\\n\\nConverted JSON', metadata={'filename': 'donut_slide.pptx', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'last_modified': '2024-05-10T05:19:09', 'page_number': 28, 'orig_elements': 'eJztWF1v2zYU/SuCnpP68pssDA9Y97Rh7YBmT3ERkPdeJkJtybXltF3Q/z7KTge3zRC5exka+0GSKYrUPYeX51xd3tW84CW3/VVD9fOqFjaoYKQRPiqDAmWKygPbYIhjlqI+q+ol95FiH0v/uxpjz9fd+uMV8aq/KU2i9MjNgtu45GFE6tptf7VZNMTPVqv+Q31/v/+42t2Pq9WiKaM0XTu5belZt+L2w3KRu/Uy9pvzLucGmTrcDm/5bLXmTTnvui8XX/wdBl7E9nobr3lTRr6sub2u3+xaN/3VsqMmN7yLUoLU52DOBVyAeS7CcwjD06vy5FW7XSZel17S75rWB+BodiQsBO+yiSpoQGmc1kCAApyvP5Unev7QD52nEyzzbsqc++Bmwwyfg75o+gUP3b/GX6YcBJGS0hB4UMKik2iTNgyBQD6OP/zA+D+2OA/xv6vm9bstb4ZXm5eGef3+JvZVs6n6G65W6xJV1eUKbzrsqmWHN81P8/rskKSXcb0ukd3yxTDkA2RZm4O0mjG6QEGAZ5dEzMawsT4nOpE1lqyqKvzEdvOe13uuhD4DgHldfTpk5M/2HsHmL6Z/YwVBEqgQg3OsUHstHYCK2ngnMjl3YmUsK3sSppM9MbPp5PZdnB3NR6JsyAYpUHmMCQNa0llo1CZHkfRJUkZLyvcoigP25J3POJyD99Yb56KLJmWtxijKCf49/P26JMIQBa8rYuyoDDqGgeS0NSlmTgYpgsrBEZlkIpiMoNST3pAeXZ5fa3rT83IziMTl3bweANkLhgKQ57997PquerGT9N8HST9W0Y1CkZxk7Wz02XPgiFo4qwJjAvm07ddRVFW737zGbtv2A0Xy7GjlsGw4AUYQUSpCawWXxCkawl4mp+DExrFsbNum31nfgRGhi7qfVfNteQP76c3nq++wXB7AmIiFGmGMxyxYorEqZ50QijE+ETWWqOlgsaaf65bZtyXLUduZykmDEClqjt4ryskxIWQMGmUYozwn7d/z8lWlOJ38Q9H03h2PsgJFXqxxhXhhrLaELkpSSlAspb40OMIL/8CJ8uhq/SJRBh8wmw4wzB7W/ulkd3O+VZDT4bGaTsq8m6a9nh2VTRSyNElIp6IsR21jFK5UmNIrB1qEE3mjyfsu/IvqA5X6UYNyqdSPVLbUBArdziGQP+E/Fv+LG27PdqJyX8dU3bZfbfsiNN1bbqsNl+2tRd4c556DllpjtMUAuGxZ5BBJBQNUSh62Ab8h6P/ExSE+h1H/Ubr9vOb49sH9nFDKlGIi0CoFMkFJL3yMTqNP+UktyXCI4KtbXt82/H6UKgqnk9RcXKNCX0y/AuNs0qiJNGnOT8mmhG8S+7El9gXsuzSuXt/n7zhPkoueofWQGbjMNHwRRDJKRyjeyIxYwyf09+j/MsRdqijyCssx2zDuC51FlAmFLmiXmsknzF5DNKSDczo8qS90/42AF11b9p2eqfr19auXD4D/5m+aXUkE', 'source': 'donut_slide.pptx'}),\n",
       " Document(page_content='Input Image and Prompt\\n\\ntransformer encoder\\n\\n{ \"class\":\"receipt\" }\\n\\n<class>receipt</class>\\n\\n</classification>\\n\\n{ \"question\": \"what is the price of choco mochi?\",\\n\\n   \"answer\": \"14,000\" }\\n\\n14,000</answer></vqa>\\n\\n<classification>\\n\\ntransformer decoder\\n\\n{ \"items\": [{\"name\": \"3002-Kyoto Choco Mochi\",\\n\\n      \"count\": 2,\\n\\n      \"unitprice\": 14000, …}], … }\\n\\n<vqa><question>what is the price ', metadata={'filename': 'donut_slide.pptx', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'last_modified': '2024-05-10T05:19:09', 'page_number': 29, 'orig_elements': 'eJztl0tv4zYQx78KoXMew7cYGOmhp0XRoof0FAcBH8NEgPVYmc4mDfLdO7ITtE13G3mPreyDLYkckv+fOH/O9XOFG2yxK7dNqi5Y5aUwmJO2MSsIUibHRc25A+d0DLmuTljVYvHJF0/tn6voC97149NtwqHc0y1OLXKzwc63OEVMfbcrt9tNk/BsGMpj9fq8PA37534YNg1Fafru/KFLZ/2A3WO7yf3Y+rI97XNuIqY+7qZZng0jbul337zd/O1yCrzx3d3O3+GWIl9X2N1VN/u723Lb9qnJDe5XKUCoU9CnHK5AX3B3AW7qPVDP227XBhyplXD7W+Of4sgUhQjBhwRKBpe0kyRO7b1VsZ7EeaEeBR/L1PhTN+wK+9RSTOa7xH4d+3Yo0zBvK79qyganPu8h2KyDI/mDdQaMFlFigrpGxyNqkdQCYS6EMvpuO60CR4Zd7BMFnUPAU+is0HMbRUKrjfJCGFurnJOUcdkGswk8s3UVaeTturpYVyNGbIayrtjLLA7GZBRoE5D0LgTNjTK0HQABvQ1eLhzmcljtIVy+AlidHy5nQcgqSG1yRJWlzzpmJyWNGmSWRtm4QJgP4SA7jXlY3Dz9VaozF+TDKkWfVApOOFkLyUHVJob0sf7w39X/w5fzfTL6vMPtNDXKR3T15d4X1mxZuUc2jLQq1mcW7/vYs7aP980P6+rkr5B+8eNIK3vAqynkV2ABcReRG561ityD1BBjdMYSNg8aF1hzYTFGfMi+v+B4YMXVCQC8947fulcFm98xfYuKq4UHThDoAKUVRoGoTKylk95C5HGhMpfKAcLq/ADmcnX+8NlfHs0DJHchYbLcGcO9zDoHymc+SysNZLFYynG+fqSjWEM6a5ujkXVy2YCggy4EsJy+ysMi//cUGAnnFxhQa6TgPoNUGVC7GoMA2nYCEqWl8L9OSB++nu89vSnYTgUGu35eV5MgB8OQAOL0p6e+9OzHvaX/PFn6sY6uQSQH3ikjp3kFyDlyC0nIpDFhXlDNRcX2H6oH+11XJkTi5Gjn0FGKQBvEOyto+/g6ax+zFgTFCmX0QuNYGruuKfuj70SEK3L3E7be0QzMy83bv+84cpG7m0DpjaAk5SNSuW5F8rXinlOu4wuouaBW0xFr9Va3XP6zZPmXdHbzB5QrAZg=', 'source': 'donut_slide.pptx'}),\n",
       " Document(page_content='of choco mochi?</question><answer>\\n\\n<item><name>3002-Kyoto Choco Mochi</name>・・・ </parsing>\\n\\n<parsing>\\n\\nthat can be converted into a desired data format, such as, a JSON format.\\n\\nOverview: Model Architecture\\n\\nOutput Sequence\\n\\nDonut 🍩\\n\\nConverted JSON\\n\\nInput Image and Prompt\\n\\ntransformer encoder\\n\\n{ \"class\":\"receipt\" }\\n\\n<class>receipt</class>\\n\\n</classification>\\n\\n{ \"question\": \"what is the price of choco mochi?\",\\n\\n   \"answer\": \"14,000\" }\\n\\n14,000</answer></vqa>\\n\\n<classification>', metadata={'filename': 'donut_slide.pptx', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'last_modified': '2024-05-10T05:19:09', 'page_number': 29, 'orig_elements': 'eJztmUtv20YQx7/Kgmfb2vfDEFy06SUtmgRIeooCYx8zNlGJZEjKThrku2coOa3tpDXdU5HoQmHfnP9vd2Y5ev2hgjVsoBnP61KdsopHLaQzsnAfbC5CWeTRcgxaaYHZVEes2sAYSxwj9f9Q5TjCRdu/Py/QjZdUJagH1mto4gamGUvbbMfzYV0XOOm68V110z6+73btsevWNc1St83iqiknbQfNu80a234Tx+G4RawzlDZvp7c86XoY6HfXfbO+U5wmXsfmYhsvYKCZX1fQXFRvdrXDeL5pS4017KyUXOpjbo4Ff8XNqQinPEyjOxp53mw3CXrqJcOuqv9bHFWylCnFVLhWKRQTlPTCx+h09gl99ZFGjPBunDq3yPJlm1u2afNl/cNy8XYLw/SeZ8vYDNfQn01LflbhVT2uYRp/H4gwmKWJ2bgcnCwGIQnhfAAplBQ5PQyEf7tAHtytt4Es6xE2Z8tJhjPFuTz+9X07tuzJDtJvE6TlYte42iqO6faTLRe07lA3F3egPYt9T5Zewatpia/AM9pz7QQvwiqVU/CKK1+SEogJuXIHeLPh/Sf9vQsGSXAQKZbkJEajUUtJy7l40P8R+o+XcWQ5NiwBy21zBf0IhdUNHaDICgx1T8VJRrY39YgN23zJ4nBE7b+8fP7spv7kUfyCllrnaDMKhxYEhlhUMLwoDGBD/oLf/wnVbfluW/2Cuv3UQ/zjaxZLGZL1YDBFYWWUWcWSc+ToJeggZsTfb2bHKn5bwee05a5quD4lV11gzX7syWGPkMdtD7MCaUQhPfJSTBHGlegU+QVrXSneO4vwHd1sFL/vCx7cdndQbMduO7KXQDeaJs9TP3AwUWtlS/JGWESRbSbVS0KN1uuD+nPV/3mym622xatMT7RhFoBSELNW3irjbaDYx10BH+kqmUUJ1h4AzAXw5K/wN8W1WeI7J1Dw6IPmErzydCexVhihQwBRtDiIP1f8p83kep5uaE4Wm8Je9O2mG2dBQIhex6KN90JBxEIHQJuUQhbW8SgPEOZCGHv6gp2sgJ5RBKB43M/zQRGMBmUCz3QWAEh3HygiK0vnATEcCMwl8IGtqkwrD6vqdFX1kKHuxlXFPs7LKWhhrPQA2utCd+sg6BvAeEAphBQKDxzmcljuIJzdAFgu9sV5EJSERJ9dImBMIKOl6KAdJk0nxII6xIT5EPay05p74+bpb5Ar7SKne5CJTifuhc7Kp0AMRJTfU27gS/0f3Jz3ndHn5Cb5IypdT9mCemDjJbCuJ6vYvWzoqjp6ZCJASFCQdbbS5iQd91kXLLQ9MmYxI3IcYO1hMUZ89gnoPSuhjzjn92PH782NgvWfUP6JCpdZonTaI0g6lA4K5GyB61ycjYUfqMylsoewXNz8M7BcXL2NZ4/moZwQiBHRWJNFMsajNShkRrpcFXsIKY+M6/8aUd58AlNAxao=', 'source': 'donut_slide.pptx'}),\n",
       " Document(page_content='transformer decoder\\n\\n{ \"items\": [{\"name\": \"3002-Kyoto Choco Mochi\",\\n\\n      \"count\": 2,\\n\\n      \"unitprice\": 14000, …}], … }\\n\\n<vqa><question>what is the price \\n\\nof choco mochi?</question><answer>\\n\\n<item><name>3002-Kyoto Choco Mochi</name>・・・ </parsing>\\n\\n<parsing>\\n\\nSwin Transformer and BART are used as an encoder and decoder, respectively.\\x0bMore details can also be found in the manuscript.\\n\\nPre-training Task\\n\\nIn terms of what that can teach, in what from …', metadata={'filename': 'donut_slide.pptx', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'last_modified': '2024-05-10T05:19:09', 'page_number': 30, 'orig_elements': 'eJztl0tv5DYMgP+K4HMeetmWgsEU3Z6KYotFOz3tBAEtURlj/VpbzqPB/vdSRtpOtmkz6aloZg62RqJokZ8oUh8fMmywxS5e1T67YJk3vJAlWGllKYSyzoAICDpUSnhViOyEZS1G8BCB5B8yBxGv+/H+yuMQd9QlSCLUDXbQ4qKx7+Z4NTW1x7NhiHfZ43i8H5ZxGIamJi11353fdP6sH7C7a5vQjy3E6bQPoXboezenVZ4NI070XsTb5snfpLiB7nqGa5xI88cMu+vscumd4lXb+zrUuFgpudSnPD8VfMPzC2EvuE2zB5p51c1thSNJKb50jX86R0pbFQbzUIEoJEinwDsHPBiJ2oo8+0IzIt7FJBxH6KZkBY7Mo+s9KU3Dj2Zv6thgmvA1gQLBgbK6LAseqhK8yQGMKyonZF5x/TIB/v8l8OL23CfwwLZZHbGdtmktD9ssOSS1t5niXJ7+cN/Hnn23613P3vduV2+zk31EP8I4kl03uEkKn0FVcq8dBJdL9J7nXonADfoSRU7LgiOqg1Gx5bfNXD93MSGST0j80j16rv4V/d/RQGGqqpQatM6p7Qwx1kJyLCvvc2uPNF5LY+7qOIxkQyIiNOekdjvTCoovl7+32JdXg5KgvS6988o4yavgc1lBYUQI0pRozRHUoaBWN59hvfo845QWt77dQWT1xOIO2cKNveo4c8EaIuq5kxDQhsIhSGM0l7wwugrH3H9o7u8Dc0taaVNa+WZ1/geiFRUFtziuDyoFXCEKoQqdax6cRKQaIM+VFUaUzhVevelAeXG3PgmUVAesV8kN6+dz/+p8GdzOiqqu/SdbndN3p7q7Xr8qmrwsuLHGG8XzChU1y9IYpehtFerqCO9geP/K/1oiN8YLCcJoilZRAdVmxuaaPiAFP/r/UP//fFt3bLN3nYHOs3ff/rRhpJLNE3oGE3Uy7JZrzjL+eOU5YWTXgC6Rau7PtjMVEdX7nuZ58n3dTMzRRGimnlXIApV/ntHXUgZroZsnN9ZDPHsVeKulprK8cEGUoUARLHhlc+5VsFhY9xfw/yXG+37ft/oDib0bET49ZzGlCKuc5cJIG5wvVHAKjUHuSw3I39RWF/se/DDiKd3D645OD7aB6dNBadeU0gTIS1/RHSKXUmmwpqJcXJShEvC23Pn1yfHiXtv3//cUyTi2E6OCaClOY3qkiI8IbneSQn3pD2PfPl4o/iHWL38DojoFHQ==', 'source': 'donut_slide.pptx'}),\n",
       " Document(page_content='Donut 🍩\\x0b(End-to-end Model)\\n\\nTo train Donut, we propose a simple pre-training task.\\n\\nPre-training Task\\n\\nIn terms of what that can teach, in what from …\\n\\nDonut 🍩\\x0b(End-to-end Model)\\n\\nThe objective is to read all texts from the top-left to bottom-right.\\x0bThis task can be interpret as a pseudo OCR task.\\n\\nTraining Strategy: Teacher-forcing Scheme\\n\\n…\\n\\nof\\n\\nwhat\\n\\nterms\\n\\nMinimize Cross Entropy\\n\\n…\\n\\nModel\\n\\n…\\n\\nwhat\\n\\nterms\\n\\nin\\n\\nof\\n\\nThis can be interpret as', metadata={'filename': 'donut_slide.pptx', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'last_modified': '2024-05-10T05:19:09', 'page_number': 31, 'orig_elements': 'eJztWE2P2zYQ/SuETy0Qb/n9scemOfSQNmjdUxIshuRwV40sCRKdZBv0v3fkTVBvktYqsoci9kWGqOFY8+bNvKGev1thi1vs6lWTV5dspa0sShZ0QjjLI4DKoZjkpSyOm5BXj9hqixUyVCD7d6sEFa/78fYq41BvaEmQRWla7GCLs8fcd7t6NbVNxothqG9X75/X22H/HIahbchL03ffve7yRT9g93bbln7cQp3WfSlNwtyn3fyWF8OIE/3uzbftvdvZcQvd9Q6ucSLPz1fYXa9e7lenerXtc1Ma3EcpudRrbtaCb7i5FOGSh3n3QDuvut024khWSuyXxr/BUVYHlQIXXoaSslUlKfQeeXYakPPVn7Sj4ts6G/8wx81e7LJXia7Fhhc7znn85kmX17VfY5fZ0z5j++381x/Q2DS1xdnPx4kBBUFwqwSWoILVWjjlOUKO0WBBfTwx/OtNzFHWHiZm07M6QtOxfYYesTfIhrEf+gkZsKnZDu28gOu9UdNdswrTq4vDJP0E40iRvcbN7PIzyQpaap3ApiJcsShKgKyC4VmVgDakT5L1f8rLIVaHUT8js+9HhFefjTgQPR3FjFymZG20GEEkroy2Gow4JXrKQwSfHTJpQ0xaVO0aZc4EYsgRudFcSAsBuSMWRa+XtOGvCM6Pq/0o1w7x/7FjFcftxPrC3txAZXW+JJiXId08YtQI9utl7LfUrumd7H+qdeNAiaIy5iipvRulpBEOXDIco4hwSor5Zal6YMX0UflcPLdgbLBJWaWTjibRlRpzOSnF/CQxR1l7TzFvkPXxd0xzHbBmYrVnJASZQduy2Wi6q55KdrUf1i2WOtvEvtZ+ux6b65t6cZfLzc28ndrgvgQjeeuoPCn2ymAi/R0m3OWe/fz4l7PsLpBdhwEKpBh5VCph0TSW+uC5FNnJpN0pcVzdo+wHyf2VxJdCvr1km7nf47imV037J3RHMS9pJVa4ZGWKThvFuTRRZZNcoF4WtbZenRTMH7eSoxw8zMunAvtb9x6s5g/M/1TZdNwxSUA2yoWSNWHvnco8lpTAI/pTEtkvS0Bflonn3pfWQfOiUXHvtXSCJLXQWVOqk5pqvgzwebhc1mQ4AJccLGIC8i2j4iKkueEkmf2CI9QZ8jvI9wP/IswxphQk+crBeWomBaMMKgI4JzGWM+aLMX9KcrulFs4ej/00sSddHfvhdlESHAQrlZW5SFmszaFAyVQFMUvr0cqTVtejDH0QdY0yCtrqBGihs840PkuvXeaehhx7roLFVTAfTxeRXlolnTeem8A1CEmDJOTCbQAXDMjTHimPEvIBSE9K64s2xqEwXOZYsJDMSq0kzDnmZ9I/+ISDRRivirJROKOMLYZ8RscLgVxSwDPkDz/hKIIcuTIkGFpByDoolC5lRJQ+y/PJaTHmTbessRdLXQuKAhAueWEhRFSOpJtOsHR8OgP+wEdVCyp44SFyx2PgJppgTOHJEbuDDAsA/4qV9Cgb73/nbabPfZf9l4+wL/8Civdu0g==', 'source': 'donut_slide.pptx'}),\n",
       " Document(page_content='a token classification at each step.\\n\\nFollowing the original Transformer, the model training is done with Teacher-forcing scheme. More details can also be found in the manuscript.\\n\\nTraining Strategy: Teacher-forcing Scheme\\n\\n…\\n\\nof\\n\\nwhat\\n\\nterms\\n\\nMinimize Cross Entropy\\n\\n…\\n\\nModel\\n\\n…\\n\\nwhat\\n\\nterms\\n\\nin\\n\\nof\\n\\nThis can be interpret as', metadata={'filename': 'donut_slide.pptx', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'last_modified': '2024-05-10T05:19:09', 'page_number': 33, 'orig_elements': 'eJztWEuP2zYQ/iuEz9kt3489tmhvKQrEPaWBMSSHthBZEiQ6u9ug/70jb4J4Nymsonsosj4JIoeP+b6PMxy+/bjCFvfY1U2TVzdsxbnxKisFEqUTmlsBPAcHPnOXhdSrV2y1xwoZKpD9x1WCitt+vN9kHOqOmgRZlKbFDvY4z5j77lA3U9tkvB6Gerf61F/vh2M/DEPb0CxN3/3wocvX/YDd3b4t/biHOl31pTQJc58O8y6vhxEn+h7N9+2j33niFrrtAbY40cxvV9htV++OrVPd7PvclAaPXkou9RU3V4KvubkR4YaHefRAIzfdYR9xJCuljk3jF3AcBiiQYuRRqYRFl5R98FyK7GTSbvUXjah4V49+sdq/x44lWn2ilR9cZFAZQtqxqeJwPS/6GYd1U1ucZ3hKSUkSPGijvXfZEhkpyKB1ImqKchDOU8K/X0rO6vWUkl/6tu1vm27L6g5ZPzbbpoOWrUfoptk5HF8de2hf2LI6QtPNxs3ECDFkt03dsfXMHo5XZJ/mzon+9njNXvcjskw0NO3EEhDR7dSziKz0hy6zpnuYGbrDlMZmqI+4/xXGkQD7gOt5p9/QQNCSGAebinDFoigBsgqGZ1UC2pC+0sD/ie5TCk69/o3MfhwR3n/LY6+tTFxmUzQmnrTmQQsdvY8gk4vmJalenyK4/izLNyRQcvn+5itNvjlqclFwycUGS2EFSE1B5+CRAzpTZODclggvCuanweWsBk95+eNAq9hT0H/vPoHV/In5n062FSJiBmN9Ms4mCuzoeeKRZ1oh2fySEu5/I6AvixSPmXTuVdCmhJx5slyClVa4DE5mYS+ALwX8dgd1EeTOeiNioItMIJCjsSoV57QMXlriQFwgXwp5xXE/LcPc+zxjrpOmm6rh0Qs0riQlo0MV1QXzpZi/pnS7pxDOfhr7aWI/d3Xsh/tFJFBO9S4Z4RAUj8IAFzor6blHqh+cf9HZ9axCnyG78iJTFDKF7KxWEg13MihHOVaKSBn3cgoWn4K5Jlokeh5ipODuRKDqpATtFQIHVYg/r4xZEHm+Y9GfFeQziD7lGLx3dH0EpzlHoyUq4YlXxQn1BVHnIvp/ecOBHIvzaKSQwVA5JQLQIcBQck4FRLlA/vw3nCS05xohO07XecstiCw0ZB2Dpup1Qel6wfwB86ZbBDhFKFUoRmmXEApak32MCBCSFwH1BfDnLlWTUTah8hZVFiIZNADeCqFDwCCQv+hMelaNjx7Nds3Dq2xE1nQUY2ivlcGjQPPkEfbd3+z/E1E=', 'source': 'donut_slide.pptx'}),\n",
       " Document(page_content='a token classification at each step.\\n\\nFollowing the original Transformer, the model training is done with Teacher-forcing scheme. More details can also be found in the manuscript.\\n\\nSynthDoG 🐶:Synthetic Document Generator\\n\\nFor the pre-training of Donut, we also present a data generator, SynthDoG 🐶.\\n\\nSynthDoG 🐶:Synthetic Document Generator\\n\\nSynthDoG alleviates the dependency on large-scale real document images \\x0band enables the extension to a multilingual setting.', metadata={'filename': 'donut_slide.pptx', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'last_modified': '2024-05-10T05:19:09', 'page_number': 34, 'orig_elements': 'eJztVk1v3DYQ/SuEzt4NSVGi6GMRNKcGAbK3NFiMyOEuEYoUJK4/EOS/ZyTb7do16p6KovZltRoOh/PeGz3wy/cKIw6Yyj646pJVmtu+c9qDqbnxvO9Ma7vO1d4ZcFL76oJVAxZwUIDyv1cWCh7ydLt3OJYjhQRl+BAxwYBLRZfTqeznGBxux7HcVPfr5XZc12EcY6AqIad3V8lt84jpZog+TwOUeZO9DxZdtqely+044UzPNX2Ij16XwhHS4QQHnKnylwrTofq6RueyH7ILPuCKUnKpNrzZCL7jzaUwl9wsu0fauU+noceJsmq1hqY/yelUKy2XrvEKLbdKcaOE6ruuB2l131Q/aEfBm7LiYiV/w8QsnT7TyXcQGRSGYI9sLjhul0MfeNiFEnGp8FQS65TvZdcK7rWvgTdCeVkjKtV0UuruZUn4/1eSF+f1XJJfc4z5OqQDK0dkeQqHkCCy3QRpXsDhdLGuUF8YWZkgpCU5zIwYQ3YdypHtFvVw2lC+XRZnehtwy37LEzJHMoQ4MwskdJwz65H5fEqOhXRXGdJptlMYyyPtP8I0EWFXuFs6fWYGjJJKWWitF9q3KDyhq03DCajB1ti/zMB/Se5zCc5Rf6K0XyaEb88hFo0UrtZO9zUKoaTolJaAqEUD3Ir2NU19c87g59tUju/zB/b7yXW1o19bt5drFEuw7P19h+wDJqSpytM/chlVC7St67wSbevRNLpWXQOibiXU2r8ql2meusyLw/jYZab1W6cWN394SPYkDPFxwa7xzhruETBgC6Xs8KDWBXtW4Te/+Hu/8NY0PW80BzBowTXS9ULzznYaa+zka5rf9l/wC9O11njde+i9t7rVQlLAyF60UksDr4rvp37x4jA+KxDEiFeBaJpX/yCeMDlM9pbRvTHCdMDNbCEio28gsgekLAwLDpKXc94D3TWI0T7eF6EzMM3LxbNksprhFEuIYcEe2Yyl0N83Zzl3lq8/ASMhUhA=', 'source': 'donut_slide.pptx'}),\n",
       " Document(page_content='Pre-training Task\\n\\nIn terms of what that can teach, in what from …\\n\\nDonut 🍩\\x0b(End-to-end Model)\\n\\nAfter the model learns “how to read”,\\n\\nModel Fine-tuning and Inference Overview\\n\\nin the fine-tuning, we teach the model “how to understand”.\\n\\nModel Fine-tuning and Inference Overview\\n\\nThe prediction target is set to a desired downstream token sequence, \\x0bincluding some special tokens.\\n\\nModel Fine-tuning and Inference Overview\\n\\nAt inference, the predicted token from the last step is fed to the next.', metadata={'filename': 'donut_slide.pptx', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'last_modified': '2024-05-10T05:19:09', 'page_number': 37, 'orig_elements': 'eJztV0uP2zYQ/iuETg1gb/mWuLcWbYEc2ubgWxIYI3LoFVaiVIlebxD0v3ckJ4h3N4iTQ4Ai3osEDYcDcr4HqdfvC2yxw5S3TSiuWcEFRF8aLL1RymvkQdZWg1XgJIfaFCtWdJghQAbKf194yLjrx3fbgEO+oRCnjNi0mKDDuWLo0z5vp7YJeDUM+b74MJ7fDcs4DEPbUJWmTz/fpXDVD5juuzb2Ywd5WvcxNh5D7/fzKq+GESd6L+ld++BzLtxC2u1hhxNVfl1g2hVvl+iUt10fmtjgskvJpV5zsxZ8w821cNfczbMHmrlN+67GkbJU+S/FMt7necqrEdd5hCY1acc2MN3OEz7uYdPkFgtKf9xOCzoYz8uorRTIq1DWNdiIGLXQlXIX1c4lNH4D1077/zKxjGM3sT6yww1klueHhzkM/mbFmnSMx7Hv2Js9rcmeQvQXjCPt6w43c8HPQAWllyUKYQyAAUIHpEReSjTIvZZ4HirxDNUC1W/zvgmCUClPz2jdmz3nvP7p9xTWuV9jCuzPPmD74qs0VEcelfRB+Aqt5E4r4RRaTyg5QCEvWkNnWXsKzC+RJETCQdbN7WctwpimRSzC3/QHlns2IoQlEFbfJB+npdYerI+ijBZFdBCUMzyo6NA6/wSl/xMgp0063fUrSvuVOnL7WW+PClVVahQmKK1NBS6gFEFWvnTKiEviZXXawUXb7I8m0Ym5X85LIMW/TBGJuB7Z33c43jV4+Cr1Bx6lrkoMtZRgoo1llF5VTnFjEVR1UV1+rP6zFDyFhQ7IWfrxEy4rdsDj8XliCg/cYJ8CjlMm/I6ecPXsCV/0BCCiWgxBRlC1xFJzHzyBVFdCaSn0JbHVfTdPgNIG6W0wQYNCX3knZIwBRWWkKdVFOa97eiM4Q8FTWDYke1peaPy8OJZh3GFmzcQmepH+gQWcGkpgoT+kKRPvO4rfYqKMf/YzdCt2vN01ybf7MCM79R2yaUDfQHtMnp5948u+oei+VtXRqICcvJxYbCzdsoUQqpJBPd3xj8tozb+bb3DjqJmS8xIqV3njrLZW0mnJVR2jCxfV5ce+cZaCD/4kyCQ+YrBaLg8fXISc4mgPy2/4PDCvj00Zh9lW4jK+xBOVeraFU1t4+x/uXGio', 'source': 'donut_slide.pptx'}),\n",
       " Document(page_content='Model Fine-tuning and Inference Overview\\n\\nThe predicted sequence is converted into a JSON format.\\n\\nExperiments: Samples of Downstream Datasets\\n\\nWe conducts 3 VDU applications on 6 different datasets. Some samples are shown.\\n\\nExperiments: Document Classification\\n\\nTo see whether the model can distinguish across different types of documents, \\x0bwe test a classification task.', metadata={'filename': 'donut_slide.pptx', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'last_modified': '2024-05-10T05:19:09', 'page_number': 41, 'orig_elements': 'eJztVk1v00AQ/Ssjn5uwttcfmyMEJJBokVrgUKpoP2aTVe218a6TVBX/nbVJkVsKhRtSerHl8ezzzHvjp7m8jbDCGq1fGRUtIEqpQMq0zqSUOhFlkjDBkGjK4zJNhI5OIKrRc8U9D/m3keQe1013s1LY+k0IkZChTYWW1zggqsb2fuUqo3Detn4fHd77m3Z8z9u2MgHFNPbF1qp506Ld15Vuupp7N2u0NhJVI/uhynnboQv3Mb2u7j0OwBW3656v0QXkywjtOroao86v6kYZbXDsMiEJnZFsFpMLki1itiBsON2Gkyvb1wK7kEXjbyHmce+HI+8bhRW8MRZnvrfGroFbBW+txg6tRDjbYrc1uBtw7lq7ML7CKKA8ZFmpWFOKaaE5yXipA8GJSFBkWZnnUuNRsTyGun8YwaksFxuEUJ4y0qMCh1/7UQ3jQDY2SDJEjfUNcHh3fnYKPxqeT1U65V0XWtvixYD5iFqMJpRKHoSJC51jrBlXKcuISjXDnMlf1PqfhJmSNe36Q0h72SG/fqxjKlFSOsxkLgqUkghFCk3iPFYsLWh5TPOZTBl8vW+xM0MNbgHnvG4rdNBoWDY763ygs4Zl4MShd39lBFRI5FlSCpXpTAnGNCuLMtakzEWGiTgqoh8awZNTOFXmMw5/vOqld5DCp+VHmHQaNLKQQ6hotGsP6iDSHM6bGsEdlAxfB7cJWj4bxJ8NQqUCi7KIg0Q0ZSUpVcEEKTgjqdKMZMc0t+lvDWJ5KAdeBWgXYOXPCp50hkTwrMBc6FJwTRTRGId/gGa5zLUUeXxUDD90hifH796K0IS9AGG3Qb/BDsIF6nGZk9wGS3A+LHO9cRvgsmucm7jEwMxo73dduxP40hNCxA7Bo/Nhq5D3pIVgK9fP5jE1j6vvmtYbTQ==', 'source': 'donut_slide.pptx'}),\n",
       " Document(page_content='Experiments: Document Classification\\n\\nThis is the results on the RVL-CDIP dataset. \\x0bDonut achieves state-of-the-are scores with reasonable speed and efficiency.\\n\\nExperiments: Document Classification\\n\\nThis is the results on the RVL-CDIP dataset. \\x0bDonut achieves state-of-the-are scores with reasonable speed and efficiency.\\n\\nExperiments: Document Parsing\\n\\nNext, to see the model fully understands the complex layouts and contexts, \\x0bwe test document parsing tasks.', metadata={'filename': 'donut_slide.pptx', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'last_modified': '2024-05-10T05:19:09', 'page_number': 44, 'orig_elements': 'eJztVU1r3DAQ/SvC5+xWsj5s7bFJD4USQll6ScMiS6NdE1s2lpzsEvrfO94kZZsE0h4KbTfGIDQaPb95Mzxf3mXQQAshrWqXLUgmCyescUp4W+WqyitnLBcllIUXwMo8OyFZC8k4kwzm32XWJFh3w27loE8bDFHM8HUDwbQwIboujGkVm9rBvO/TNns4T7t+f276vqkRpe7Cu5vg5l0PYds2vhtak+Ks87624Do7Tizn/QAR13162/y0nYAbE9ajWUNE5MsMwjq72kdjWrWdq30N+ypzmosZlTNGl1QumF5QPd3u8eYqjG0FA2YJ8Q1jCbZpuvJh28NQTxzigpw90CGnCB0R1v5g8FjWsk4NZIjwVOEKcs04MGstd0JL5YCKnINEoXnO5VEpvA8NvzF+hy1ZbupI8E0bIMhybFIkXdhvP3/5NDs9+3hBJhUjpDn5OlJKq7NJKmLspoYbiCRiXYACzPDODHmQaDtEIrd12iCkiV0wVYPhHsARExyBSawagt3ND7t9boYBJbqB5cTtha5rkQusTFnPCq+AeW0c15I67jUobZ91/W9q8KHoh1VfYNp7lOn6pYqZsCUXwIXw3tqSOgOiskwbXziw7qjmXP4RJ+GllVQrMJIxBk5yLaj2+CjJWc6ez9T/rPBTJ3l1/N6c5F9xkkJwWTDJi0Iq5llFdV46o3MFoEpl1THNuXrdSS7MEGv8yq9YCAOrCyYKZqU0VOaUmtJbUPjLld5xdlTSPrWQV+fusBfnuJ6Q1JEIsDcOJAAN8WPT7MgYHAxoEcHde4zt2r6BLWnMrhvRayY/sF2YsOLJg8HcIgzERB61IP19Xwna0HV884xDz7j6DoAjPK0=', 'source': 'donut_slide.pptx'}),\n",
       " Document(page_content='Experiments: Document Parsing\\n\\nFor all domains, Donut showed the best accuracies \\x0bwith significantly faster inference speed.\\n\\nExperiments: Document Parsing\\n\\nFor all domains, Donut showed the best accuracies \\x0bwith significantly faster inference speed.\\n\\nExperiments: Document VQA\\n\\nQ: What is the Extension Number as per the voucher?\\x0b\\x0b\\x0bA: (910) 741–0673\\n\\nTo validate the further capacity of the model, we test a document VQA task.', metadata={'filename': 'donut_slide.pptx', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'last_modified': '2024-05-10T05:19:09', 'page_number': 47, 'orig_elements': 'eJztVk1v2zAM/SuCTxvQdpItfyiXocO6Y9ECwXZoi4CSqESov2DJaYJh/32M10PWFuiK7VAsvdgwRVLke/STrr4nWGODbVx4m8xYIi1UWGpZgCy1MM5qq3NhcyWkylwqkiOWNBjBQgTy/54YiLjshu3CYh9XZOLk4XyNLTS4y2i7doyLUHuLJ30fN8n9etz20zr0fe0pi+/aD+vWnnQ9tpumdt3QQAzHnXPeoO3MuKvypB8w0Htyb+rfPneJa2iXIywxUOarBNtlcjNZQ1w0nfXO49RlylN5zPNjwec8nwk142oX3VPkoh0bjQN5yfIH2SJu4i7kbNPj4Hc1hBn7fF8Ou4AheNpl53jfz9zHGhMKfQitdgWI1IDOqwrQVJktSlU57qAoqtTZg4J2Mg0vmLt9Lr50A4O6ZrZrwLfhiPggIFhYdXdoWVwh0xgiA2PGAYzHwK5Hzrm+83HFgl+2VK2BNtZb5qh+HJhvHVI1BlnoEe3JPqHnMAyEwhrnu+2fIFbJVEoDhXGidAUKp8BmKuc2cwoLZR4R+5o43Md1v+sLcvs0INw+1bFMyxSNKZQixlxWQFVy60qtU3RCFPkhjXL1b1WC4CxlntMUZTRO3FidmirH0jpDIPPsoKB9pBLPzd2bSrwileAgqxJLrhRmArAoBJhUpi4XXOS6kIc0yup5lfh6efpHCkHHYQVGVKUDAIm5zrnRdKPQCEpi9niQ/mdYHyrEszO3z8PljH1bQWQ+THJwtonYBiqTnU8bMAiMiJrW1t1oVjh8/CUR+8/TGXunBH/PSimux5SLjBdl9iJpcCKvZK7RWCglWiNJ8IlauhaiU6k9qJvh3zE679gaqGvCZ6LNjQO9BmagJ4mPW9a5yU51YX3E7shrOgOY3fsLWYRw+ybu++J+8xNIrajv', 'source': 'donut_slide.pptx'}),\n",
       " Document(page_content='Experiments: Document VQA\\n\\nFor VQA, Donut showed a high score on the handwritten documents \\x0bwhich are known to be challenging.\\n\\nExperiments: Document VQA\\n\\nFor VQA, Donut showed a high score on the handwritten documents \\x0bwhich are known to be challenging.\\n\\nAnalysis: VQA on Handwritten Documents\\n\\nAs can be seen, the OCR errors make the performance upper-bound \\x0bfor the conventional baselines.', metadata={'filename': 'donut_slide.pptx', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'last_modified': '2024-05-10T05:19:09', 'page_number': 50, 'orig_elements': 'eJztVU1r3DAQ/SvC5+xWtiXZ2lv6RU/9IvSShmUsjdYitmQsbXZD6H+vbDawSQNpoYfAxhej0cww897z8+Vdhh326OLa6mxFMiYKVoNqKmClEJUsDJaAFS0hVwbrKjsjWY8RNERI+XeZgogbP96uNQ6xTSGaMozt0EGPU0ft3TauQ2c1Loch7rPDfbwd5nsYhs6mLta7NzdOL/2Abt93xo89xLDwxliF2qvtNOVyGDGk95zedw+OU+MO3GYLGwyp82WGbpNdzdEQ173X1lictyxowRaUL3J6Qfkqlysqp+ohVa7dtm9wTFmc/kqxiPs4lXzYDzjaaYawIu8P45Af386nwvtdLmzsMEtlj2GVFQVqCqhZybnIS1pDmR5dl7yogKqTgnUOjf+guWMePvpxQv0scZAAIKH1O9QESGs3LQnKj0i8I7FF0oLTu9HGiI7cLxrIzy2ltNm1VrUkDUGund+lfE8aJKqFLiG8sW6zPKb1M4xjwuIGL6YhnqKXFYwpEMrklRGYGwm6lJzq0kgU8k96XxKTx+geb/01pb0dEa6f2ripag7AdSGMkVxi2hVBUuAck8wbfkqCzv+fT3CgjBe0qsokJS5MXfAyV7oSwrBGifykYH3sE89q7tUnXpxPiFoUFEHRnCXqoGFV2bBG5olJyaQ4qR9fcYzguYPuNthkEkmlkxQ/Hcnw3jfCX5lGyQyTVSOVLFVR1IWpuOCYS13xZBoSTgrjx6bxrAAfkBKIAjd94gHRnc328OXdd4Lj6MdAerjGOZYsft7VKSTbIZ0Wjd86fTCNdDVnKe9u0iBpTehIAwE76zC8OsaxY1z9BiqLHuc=', 'source': 'donut_slide.pptx'}),\n",
       " Document(page_content='Analysis: VQA on Handwritten Documents\\n\\nOn the other hand, Donut seems robust to the handwritten documents.\\n\\nAnalysis\\n\\nNext, we show some main results of our analysis on Donut.\\n\\nAnalysis: Pre-training Strategies\\n\\nWe tested several pre-training tasks. \\x0bWe found that the proposed task is the most simple yet effective approach.\\n\\nAnalysis: Pre-training Strategies\\n\\nOther tasks that impose a general knowledge of images and texts on models \\x0bshow little gains in the fine-tuning tasks.', metadata={'filename': 'donut_slide.pptx', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'last_modified': '2024-05-10T05:19:09', 'page_number': 53, 'orig_elements': 'eJztV01v3DgM/SuCz8lUki3Zyq2LPeyp28UG3UO3GNASNWPElgxLzgeK/e9LOyngJkHTAj0sdnIZj2mKIPnIR+rj5wJ7HDDkfeeKC1bIRrRCoNAcSuSmUaYRNbSla1TbALjijBUDZnCQgfQ/FxYyHuJ0t3c45iOJOGn4rscAAy4WXQxz3qe+c7gbx3xbPHzPd+P6Hcax78hKF8Ob6+B2ccRwO/Q+TgPkdB697yy6aOfFy904YaLnqj70X70uhnsIhxkOmMjyxwLDofi0SlPeD9F1vsP7KLmszrk6F/ySqwthLrhZTo90ch/mocWJtFT5D8ky3ublyNsA/V3q0gX78MdbFgP7DYK7mbqcMbBfH9xLi5UvgV12uceCbDzOsRLQSF3W0vnWeOkqVJX0lHOJsvItP6kcr6LpBwpwC8rvgeUjskg/EzsSIGcEBaWCJcQhsSm2c8osx1XruAHsS6xpt0XsHUwThXmNl4v9Z5AzlawqC9p6UXuNwhtwpVHcld6gNvYJcv8lkLaJ20b9ntR+mRCunou4KX2FKGRtWmmtqy0XtahbLRDqUpXilGq1eo4Pvqvja++NaayUQpkWlKyapqlAWle2zmpzUh1fPe74F0tsm/Z39DxjN8jSMd6wFAdkA3SBkcdznxOLnsV5YvCAzkLUKyO89vm3+9yiqLjiGrnXleNYlWApC0rwylqpn0b8P65Q9fzcfz/heZ6o2LpwYH/SP0pAh9858vUy5K0wZe2V9Vp7WIaZV0IaW7uTWqvUYwJ4sfa2ePyFLGPK6GjGX+MEPRu3uGRIV2nH/p455y3p+jgHR+Mf8roDjFMcY6LDix4jfliEQ6QlIXXD2CO7w8zQe7QLPTBK4hTBHl/Z44UtgTunaJE1uuIt0TYg2NYoXznNSyFOqrz1z2cPQFciltIBR69BQ+tk6cpGOm6Ql81JpffJ+vBS7X11YVhvCitJ3JMCdT3xAQN2wLCyyVWINz26Ay6rRDcsbtM2QYRBFtZ1glzGPj0wzLqF9HSnIOo4ELiJdfdXEt8FwnvecNIrhWwo5NO/KEdxfw==', 'source': 'donut_slide.pptx'}),\n",
       " Document(page_content='Analysis: Pre-training Strategies\\n\\nFor the text reading task, synthetic images were enough for the CORD task. \\n\\nAnalysis: Pre-training Strategies\\n\\nBut, in the DocVQA task, it was important to see the real images.\\n\\nAnalysis: Image Backbones\\n\\nNext, we study popular image classification backbones.\\n\\nAnalysis: Image Backbones\\n\\nOverall, EfficientNetV2 and Swin Transformer outperform the others.\\n\\nAnalysis: Image Backbones\\n\\nWe choose Swin Transformer due to the high scalability and performance.', metadata={'filename': 'donut_slide.pptx', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'last_modified': '2024-05-10T05:19:09', 'page_number': 57, 'orig_elements': 'eJztV01v2zgQ/SuEzo6XpEhRzK3pB7CX9CvoHorCGJJDm4hMCRKVxCj2vy+legHXyTZdIIeijg+WSc5QnHnzHsefvxbY4BZjWgVXnJPCYu2N00aYijJqhUAKpXXa1bXnzrhiQYotJnCQINt/LSwkXLf9buWwS5s8RbOFDw1G2OK0o2vjmFZDExwuuy7dFfv1tOvmdei6JuRdQhv/uIlu2XYY77aNb/stpOGs9T5YdK0dp1Muux6H/JzNt813w2njBuJ6hDUOeefPBcZ18WWeHdJq27rgA85RcsrFGZVnjF5Rec70OdWTd5c9V3HcGuyzlVR/57mEd2lyeRGh2Q1hOCfvejxLPYQY4pp8zL9yAkJ+42S8j+kqpAaL7H6cXsoZUmPQeQpUaoslrbjhCi1jgkt1Uumdp/r/UXuHeLxpe5I2SKYx6RHchEaC4XpBhl3MKylYErbTYckt9kgwtuN6Q/ze7+XbD69m+yU5RO4S+gxouMGr6T0PIKgFF8JCZT1TvkLmNbhSS+pKr7HS9h6CvxJYhwk8jPpdNrvISbx+KOKylBUzwCitjeWKa8gf7ljJauoMslOq2frpJcErJTUDp8tKqppBWYHXQnurLJXU+JNK77EkPFp7h3hcjGlBQpzp/aq1n96/2AtCSOQWhqwGXdsniImklgyIs2Gu+mavE8tnIfihEAAamwNTNZQaNFhZW1R1JVktrK2r6pQqVT8sBH9OhUQuwF6bNv5sT8AkloZJK5yXTNG6BOFBARPgDbcnJQD6WAAerblDHC7zc5FvezKk0e1I13ZjA/03dhObjzTk43yLm5h/MXpm/Y9ZLzyi5ZwaLa2uKl0q4b1TilNZ51yYEyrPij4d67Guq1ILKpTlucFVRiqZu1ifO2Ao3c+klf0+aT1m/aM1d4jD2xvsoWkW5PUUQci7XGL6xAlERz7e5n7gqoc4TGFiT9oxddhPg/n2b/NX/70E/Cdip0f9qvQmq6yudQmaa8dBUsG915SXnJn7Ef/G1GdPR/2cO6pRSLTCK25KxpBXBrhgPncCGk8qrcfUf7TmDnH4K1/rm7Yd8D7R3YhTmz+RfBPy//7BQgMmNCHtZmXYqwBEi88dwKEMfPkHSJ6dDg==', 'source': 'donut_slide.pptx'}),\n",
       " Document(page_content='Analysis: Input Resolution\\n\\nWe observed that Donut grows rapidly as we set a larger input size.\\n\\nAnalysis: Input Resolution\\n\\nThis gets clearer in the DocVQA where the images are larger with many tiny texts.\\n\\nAnalysis: OCR Engines\\n\\nNext, we study the effects of OCR engines on the traditional baselines.\\n\\nWe test four widely-used public engines. \\n\\nAnalysis: OCR Engines\\n\\nWe observed that the scores heavily rely on the OCR engine. \\n\\nAnalysis: OCR Engines\\n\\nThis shows why we need an OCR-free method.', metadata={'filename': 'donut_slide.pptx', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'last_modified': '2024-05-10T05:19:09', 'page_number': 62, 'orig_elements': 'eJztWE2P2zYQ/SuEz7FLUvwQ95a0PfSStsGiPQSBMSSHtlBZEkR6d92g/70jJUbd3UWcAHsIYF8seDgccebNPM7o/ccFtrjDrqybuLhhCzDepbry0htAC8kLk4xTwoN0wdmweMUWOywQoQDpf1wEKLjpx8M64lC2JOKkkZoWO9jhZDH23b6sc9tEXA1DeVh8Xi+HYV6HYWgbstL03Q93XVz1A3YPuzb14w5KXvYpNQFjH/bTKVfDiJmes/qu/d/fyXAL3WYPG8xk+f0Cu83iwyzNZb3rY5ManL2UXKol10vBb7m+Ee6Gu2n3QDvX3X7ncSQtI/8hWcGHMm153UF7yE2+Yb90w76wd5j7dn9879GZ26a0uKB9j+OqDDoTeJ1A+iBsElJytLqO3nKI2lxUXGfR+A1JdwrEn8h6n3G8w8jKFgr7aQoE24z9fWYjDE1sDwwyu0eWsTBgLYwbHFkzw5abv3F1ithbGEdy8w5vJ/vPIOeUVCqACYlgMyiSg1g5zWOVHBr3tCK+J5BOA3fq9W+k9mZE+Os5jykppYcqBlNFCCGIGittva6lq4QU8ZJytXpBDtCecyFSQgG1Sl5VUqBNlXUqRBHTZcX1MQecTbpTIG63TWYbLJmFFsnKVN1EBkhcEP74/TW73+KIs6DZTSdmpHPkgfumbNkOugMrzfRDJvOVEc4wgkcUKilntErBGGtULTQlrUWpo02XlLnqeUb49cd37Odu03T0lq9qCJTUuq5iNLyuQi0txVVhStFXDly8KDJQT8jgXL6dYvCWnq/m677s42GueoojBmKHPs244CdcWP+JJMoIsZkcgZZ5yNhOi99GAVoJLmtDp8RALVwUtdNBJ05dS+LUy1zR+1r0qJ0rmAtL/X7i5ojtYbnP1NsNe08OH7FbsStFf5GidaotjRRccnA8ee4tYJWsscBNiEpdUkbql6HoWgAIi0C1TeOb5KmCiBRNFRXXPsFFhfRxkZ/Nty/ObBMN59DTidkW4a6hkW2kyj8y9H+kfa37M3VfoVfaRm+c8yZapbT3WAnr6HYCbfklJal5mbqXvkK6raTXShED8Nq7aCXwpAzJ8KK+gZnHdX82357MaXk7fZm53x6mHq1DogDoJkyWaURkFMhtH68D2GmVf/gXFRsYmw==', 'source': 'donut_slide.pptx'}),\n",
       " Document(page_content='Conclusions\\n\\nInput Image\\n\\n(Off-the-shelf)\\x0bOCR Engine\\n\\nBIO-Tags / Answer Token Span / etc\\n\\nBackbone\\n\\n(BERT-like)\\n\\nOutput\\n\\nInput Image\\n\\nToken Sequence\\n\\nDonut 🍩\\x0b(End-to-end Model)\\n\\nOutput\\n\\nSo far, we have introduced our new OCR-free method, Donut.\\n\\nMore experiments and analysis can be found in the manuscript.\\n\\nConclusions\\n\\nInput Image\\n\\n(Off-the-shelf)\\x0bOCR Engine\\n\\nBIO-Tags / Answer Token Span / etc\\n\\nBackbone\\n\\n(BERT-like)\\n\\nOutput\\n\\nInput Image\\n\\nToken Sequence\\n\\nDonut 🍩\\x0b(End-to-end Model)', metadata={'filename': 'donut_slide.pptx', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'last_modified': '2024-05-10T05:19:09', 'page_number': 67, 'orig_elements': 'eJztmEtv1EgQx7+KNSeQmNDvB7eF5cCBzQpyAxRVV1UnVmbsWdsDQWi/+9YEVsoCK5wNSCviiy232+3u37+7Xq8+rHjDW+6m05ZWj5oVJ66uxOKwZg2WqlUp6ZpyCaGmklYPmtWWJyCYQPp/WCFMfNYP70+Jd9O5NCnpUdsNd7Dlw4jUd/vpdNy0xEe73XS5+vR+er+7eg+73aaVUdq+e/i2o6N+x93ldlP7YQvTuO5rbZGpx/1hlke7gUe5X3Xfbv7xeBh4A93ZHs54lJFfrbg7W725ah2n021PbW35apVGGbdWfq3VifKPdH6k8uHrnXx52u23hQfpFeKf0jbx5XT45Enf4WY/yn/GQ9e/Z3/SThteScfPQYaaM1MGqt4UpwsVn6vS2ZEKRnv1bZD65wF51TTcYJddJ/+s2+2n5tlWxpxHPgP6GgopIJXAqeiiMcEkciEY4IX8XPL3jmtdT+e8Hs95U++/3iulyvGTF83T7qzt5qnhIpUUHThX5Qqp6FIZ2VWGoAK5RY25ajx+drw+gbOxedj80o3veGhO+gvumpc76KSNJ5wlSDReq8AEKSMxF+WTchbFNiknd70IMlsQwIvSzzwHKmufimdGZE+FtYcaE0dL6NHHGY51wf7JKj1++uJkvWkv+P4s8qByFCNEyrtUbAnVa7amhEQFKSlayM8lf7yfxBXPc8IKFFkNkYvR0diio02uKNntLoBZzP4PC3/QBa04KNZGofMq6VSyIbDZF+DkF/JzyX9yr/zHnjucBz8bVLkmqwApJJ0L2mKDidZaxSXmBf5c+L8e1t283lOyKNca8sfw897TjtZTv+aOmuc98WaeE5BIU1WD4AOSMyFYbWyqVVUdg642LML8ACegfI0GciZrapQHyYSZFZtgxQ5lY+5UMeFz6N/ckdehv+ybCsOD5h035/CWm7abhp72spam3w9Nx+8aycvWdWBuhOh5Tw+aqxN0dF2o32AYZHVv+eQw7NfMV7XJmlxNLmQpUdWQHBjjZEfIxrhb1Z9bCfa8H7jhyx0P7WHmYwNir6CDzfuxHRuUjK1wU/u9tLZdIzl2s4VuP+LQ7m6qmTPOIQSUedTAumYg8fUSfdXMIeMXmv2f5LmO7Pqqf5dujweGi6+alaqih6J1EYMSrEdtdSQLQQRx2ce7tEvTbWqUPiRvVXUSp0QFOhmbk/FYbAoq6jID5M/jFNPnx/2bu+w2Qbq4P8pggiqovFE6V2eBXJRfOOVTWcjPJf9dapSJgCOikTCoWkaXkkPrTfZoItByDmar8Z1qlJLAVgtcs0WbQVOsFDxgkhQqhIJ3qoR/S0FuUqMsxuqMcgS8x5zQ1KwUOBHAaZVdWbDPt0o3rFHmKLaGEQldYHLMGtmSnILIHkNd/MFs8jdITykVo2UMCXh8sCVIIJ0KG+OVFTuT7AL9R4U/Wcy5QW9zyKWUFHPRZNBlRaEUH+9UNeZ25P9DjdIn1oy6ukAm6+KsghAYJI+1iMxfJowL/H+Bf6sa5Zu/AKfMXIc=', 'source': 'donut_slide.pptx'}),\n",
       " Document(page_content='Output\\n\\nSo far, we have introduced our new OCR-free method, Donut.\\n\\nMore experiments and analysis can be found in the manuscript.\\n\\nConclusions\\n\\nThe proposed method, Donut, directly maps an input document image into a desired structured output.\\n\\nInput Image\\n\\nToken Sequence\\n\\nDonut 🍩\\x0b(End-to-end Model)\\n\\nOutput\\n\\nThe proposed method, Donut, directly maps an input document image \\x0binto a desired structured output.', metadata={'filename': 'donut_slide.pptx', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'last_modified': '2024-05-10T05:19:09', 'page_number': 68, 'orig_elements': 'eJztV01v3DYQ/SuETi3gdUnx28emPRhomqLxLQkWI87QFiKJqkQ5NoL+91LbBl0nBiwjPRSxD5IgajjivDfzhnzzsaKOehryvsXqjFUejUBpDZjGG2WBIlqjdHTcO94YqE5Y1VMGhAzF/mMVINNlmm73SGO+KkOiWMS2owF6Wj1iGpa8n7sW6XQc8031z/d8Ox6+wzh2bfHSpuGH6wFP00jDTd/FNPWQ512KsQ2EKSzrKk/HiebyPJj33Z3X1XEHw+UClzQXz28qGi6rd4fROe/7hG1s6RBlzWu143on+AXXZ8Kfcb/OHsvM/bD0DU3FyrjD0PQvODxyq6ERotF1Y6QOQgqLEoyopfLaVn+WGZlu8mr8asnjkle3nyK9aHNHq83noHNtvdKNQ2GAO+fQOkQy0pNFa6N9GHT+7YL+YEYeg/46sQjTCftA7AquibVDnhIuJRaWlokN9IG9evH7Lk5ErCB6lfCE/bRidXpM1K8wTSW6a7pY3d5DWGEchCjLcMI3zkTfSAM2Um2DlSDpmbCthL1MEzG6GWlq15XPDAYsF3S3czuzAANriMW0lNF2YPmq0AbDMoepHR/JmVe1UgFMiMJGQyJ6QOk1Rxk9GR++4Oz/RM8xZMdR/1bMfpwI3t8XsQvCFTIIvZWWTJESgKBqoRrCINA9pSz1xwi+SEPolrn8Z96kz45b402tvSNR0tkQNBq9dL5WNbnonxSQn5f7g1l2jPxFKeBxSmOaiyLfEeAThu1EIXe3pcLHVQdKwZcGyj5Fydq+rGRV9MSAIc3FHNmcpyXkZToI/Npw74jCL+2czzP195GqGlReChdsXTpvQ8Jp48A6aS0Cig0a/u3sdL6O1PMDT+crPZvKKVpuLXcKtJOhbgRaWf4XnTIg61qJZ+Q3l1N6TwN7TX8sNIRt4IcYnVbaUA3CiihrrZUSSmiH2qN8Bn8z+AfZYm8XLFlc7tH4twvnvPnu5wF3Oe2o7FleJqTu+03EeL4eIkStQohUHnU02kNolHRCcqOfidlKzCNOXqZxMhoVHDpbcy8ceu5sA15Ypcvm8El39gcz8r/t7H9Xz6P6+5Pf9L/7C813w0c=', 'source': 'donut_slide.pptx'}),\n",
       " Document(page_content='Conclusions\\n\\nThe proposed method, Donut, directly maps an input document image into a desired structured output.\\n\\nUnlike conventional methods, Donut does not depend on OCR and can easily be trained in an end-to-end fashion.\\n\\nInput Image\\n\\nToken Sequence\\n\\nDonut 🍩\\x0b(End-to-end Model)\\n\\nOutput\\n\\nUnlike conventional methods, Donut does not depend on OCR \\x0band can easily be trained in an end-to-end fashion.', metadata={'filename': 'donut_slide.pptx', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'last_modified': '2024-05-10T05:19:09', 'page_number': 70, 'orig_elements': 'eJztllFv3DYMx7+K4KcOyGWyJVtWHtftIcC2Dm361BYHWqQSIbbk2XKQoOh3L+2mWLYVqIftYUDvHu5smuKR/59I+c37gnoaKOZjwOJCFM6WjdEEqGvTgOwqhQ2VHUBTG23BFmeiGCgDQgb2f184yHSdpocj0phv2CTZw4eeIgy0RsQUl3yc+4B0Po75vnh8nh/G7TmMYx84Skjx+7uI52mkeD/0Pk0D5PmQvA+OMLllzfJ8nGjm38196P90uwbuIV4vcE0zR35TULwu3m3WOR+HhMEH2qqsZKUPsj6U8krWF6W9kFthI688xmXoaGIvIz+wLdN9Xpc8T9H1y8z/M6+un7O/Crmngh3/KqRVKKlUWHUajK/QoCqtLzuSsla2bb8pITfT9A922VPlr25IjFMa00woWLKbhGfix1WMM4FhIpf7BzHAOAuIIsRxyeJzlSIMnAkbcxIgkGZ2RzHnaXF5WS/Tktn//CnRn8OcLzMNX4JaVSU5p5qmLmtrnXNetxJrzhi87BScoO6F+jr24ZaES/GOI3B+0D+ynR/hMkSaRUx8QVw0s4rixfOXDBmFY9IEc2DwHYk8QYgMM8R1B7DrIafDusLDfMOhd+NFrTqUjVX8QYVOaeV9B1Svt9LQ1/GWJ7wb3sutDS/X7ts1LRV4dLK0lfSooa3BdLYxYLVUvqvljml5Uv5xWqZbiuIV/b5QdPvEt6WWrSInKySjpPVUkzOVk9pbj9idxN8r/qfB9XbBVjn+9o19u0gpu2c//TGUfklI/Xe7wFSl8lojuZp7oOzq2vquJZItWG4WUicwe8G82M75XaIbMFZJAFnVpq1Im9IiyY7bQhmS/tt+cfvqjvxvzvhPXfMvT/pfYZpYlju6WvP50tzTldYOGudL43nveAuobC1ReUuNdX8j/X+C+lTop1X/xm4/TAS3xYd3HwFOh4IK', 'source': 'donut_slide.pptx'}),\n",
       " Document(page_content='Conclusions\\n\\nThe proposed method, Donut, directly maps an input document image into a desired structured output.\\n\\nUnlike conventional methods, Donut does not depend on OCR and can easily be trained in an end-to-end fashion.\\n\\nWe also propose a synthetic document image generator, SynthDoG.\\n\\nWe also propose a synthetic document image generator, SynthDoG.', metadata={'filename': 'donut_slide.pptx', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'last_modified': '2024-05-10T05:19:09', 'page_number': 71, 'orig_elements': 'eJztk01r3DAQhv+K8Hl3K/lLVo5NoBRKW9otPYSwjKXRrogtGUsOWUL+e8dLFjZpDttTC83J8uid1zPzeK4fMuywR582zmQXLKtMba0wUjSFVpg3Zc6twlYJK6umFiZbsKzHBAYSkP4h05BwG8b9xuCQdhTipLCuQw89zo4m+CltYucMroYh3WdP92k/HO5hGDpHLi74d3ferMKA/r7vbBh7SHEZrHUaTdDTXOVqGDHS8yDvu2evs3EHfjvBFiM5X2fot9nNIRrTpg/GWYeHLnOel0teLQVf8+pCqAuu5uyBMjd+6lscSSXFI8US3qc55TJ43U2RvhNn6bH6tUsdZiR8OcgaQVZY0zjLprSFEm3RQG6MbFpTSt3+V4M8hMY/+MtOJ7/eIRvGMISIhtHIdsEs2NU8jAUzbkSduj3rYYgMPHN+mBI7dslcT5VQMAUGzGAkuWExjZNO03wMUyL96pToJxfTx4T9a1AL4KWRuayxFI0sWi5lq6CtZSlVU2n7BvVcqD98526R6eDvyIHqg+6JbXyCSxAxMh/ogNQ0sfLsy+U3gmyYJtII0RH4FlkawXmC6fz8B5B0mcJyzrAQd2R9Nl4tFUppgDa04KUshC0551DUpmi0qN529my8P5FBF8Nxb2n54t6nHSanXy7nFj2OkMK4YN9nzVX4cDYwWTSFIkxW55WptMixAIGmEbZWRqr8DdhfAPYZRrp2d7ievV+hpsq8LDXU2gppaxRWgSlUxU1B9dVK/0btXwJ0OrTTrr+S7P2IcJs93vwC+MQP8A==', 'source': 'donut_slide.pptx'}),\n",
       " Document(page_content='Conclusions\\n\\nThe proposed method, Donut, directly maps an input document image into a desired structured output.\\n\\nUnlike conventional methods, Donut does not depend on OCR and can easily be trained in an end-to-end fashion.\\n\\nWe also propose a synthetic document image generator, SynthDoG.\\n\\nThrough extensive experiments and analyses, we show the high performance and cost-effectiveness of Donut.', metadata={'filename': 'donut_slide.pptx', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'last_modified': '2024-05-10T05:19:09', 'page_number': 72, 'orig_elements': 'eJztlFFr3DAMx7+KyfPdzckll+ReWxiDwWDr2EMph2zLF7PENrHS9ij77lPSFtqywe15fYot/6VI+lm+fsiwxwE9HZzJ9iIrm6ps0LYyb6DVEipUbWNKXZfG2FrLbCWyAQkMELD+IdNAeAzj6WAwUscmyQrrevQw4BzRBD/RIfXO4CZGus+ezukUl3OIsXccxQX/4dabTYjo74fehnEASutgrdNogp7mLDdxxMTfRT70r7Zz4B78cYIjJo58naE/ZjeLNdFhCMZZh0uVhSzKtazWubyS1T5v97KdvSN7Hvw0KBxZVRe/2EZ4T7PLRfC6nxL/J83S5+yvHPWYsfBtI3caJOQSS7CqLWXT6NJCA021k21eFdV/1cjFNP7DLXvZ+asORRxDDAmN4JZ1wazE5dyMlTBuRE39SQwQkwAvnI8TiecqhRs4EzZSECAMJpYbkWicNE3zMkzE+s1Lop9dok+Ew5+g2kaWpcHa5rZqc6PKWhm9q1qjc6W0ad+hngv1u+/dTxQ6+FuOwPlB/8Q2PcFliJiED7xALppZefHl4itDNkIzaYTkGLxCQSM4zzCdn28AS9cU1rOHhdRx6LPxYlOpxuQadWG323Zb5O1WSbQ6L4wst+94z8b7AwX0KTzPLQ9fOnnqkJx+O5xH9DgChXElvs2ay/DxbGA17nI5v6lWVW2zK2wOtd6qXaWNVLZW78DOf2THMB07wVv0yd0iryKObi4jLTMHPKKnhDyedyhSF+4E8xSdYycWLg3wGh/HMyRao7X8NHMkjymJYB+n+i9ob34D3tDPew==', 'source': 'donut_slide.pptx'}),\n",
       " Document(page_content='Through extensive experiments and analyses, \\x0bwe show the high performance and cost-effectiveness of Donut.', metadata={'filename': 'donut_slide.pptx', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'last_modified': '2024-05-10T05:19:09', 'page_number': 72, 'orig_elements': 'eJzNUctu5CAQ/BXEOTPBNn4wxyjn1R7mlo2sHmhsFBsQ4GRGUf59wcpqs8oP7AFBN1Wl6uqnd4oLrmjTaBQ9ESo470XTg+AtF8DqRg5NV/FLXfdt1dWa3hG6YgIFCTL+nUpIOLlwGxX6NOcWywhtFrSwYlFUzm5pjItRePQ+Xennf7r5/R+8X0xWMc7ev1p1dB7tdV20CyukeHBaG4nKya24PPqAMd87fF3+KYvwAnbaYMKYlZ8o2ok+792YxtUpow3uU9as5gfWHip2Zu2pEicmCttn5mi39YIho/p6b4W/4fCh5QNqwaoBhGTQ4kUMisueK6V7yehHZiS8pgI+z8Ft00xyiTaaV8wvj8GUMSIBq/KB5RYx3pFfG2Ps8oYkzu6NpBnJbDI1w/cYrMSdIF1MB9QaZcp6FmMkTpPHEvCx+P8T6Q8IAQrkXLxkU9+3XHMuoZO66nWHlRagGtEy1WiBnZDftvw/LfRryF+n/plhDwHhhX48/waPY/Hi', 'source': 'donut_slide.pptx'}),\n",
       " Document(page_content='Conclusions\\n\\nThe proposed method, Donut, directly maps an input document image into a desired structured output.\\n\\nUnlike conventional methods, Donut does not depend on OCR and can easily be trained in an end-to-end fashion.\\n\\nWe also propose a synthetic document image generator, SynthDoG.\\n\\nThrough extensive experiments and analyses, we show the high performance and cost-effectiveness of Donut.\\n\\nWe believe our work can easily be extended to other domains/tasks regarding document understanding.', metadata={'filename': 'donut_slide.pptx', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'last_modified': '2024-05-10T05:19:09', 'page_number': 73, 'orig_elements': 'eJztlcFq3DAQhl9F+JzdyLa8tnJNoBQKhTalhxCWsTSyRWzJSHKSJfTdO3YSmoYWtufuyfL4n/FoPv/yzVOGA47o0t7q7IJlDVeoBVd5U/NKlnVuciiELGSV81IBZmcsGzGBhgSkf8oUJOx8OOw1TqmnECeFsQM6GHGpqL2b0z4OVuN2mtJj9vI8Hab1OUzTYKmK9e783umtn9A9joPxYYQUN94YSy15NS9dbqeAka6rfBx+u10KD+C6GTqMVPkmQ9dlt2s0pv3otTUW110WvBAbXm1yfs2ri1xecLlkT5S5d/PYYiBVXf6gWMLHtKRceqeGOdJ74iJ97f7apgEzEr4fpBYFiB1yISQN0TQib3dQoVZ1nnOp+H81yDUU/uErezv56x7ZFPzkI2pGI+u9PmNXyzDOmLYBVRoObIQpMnDMumlO7HWXzI7UCQWTZ8A0RpJrFlOYVZqXpZ8T6bdviX6yMX1MOP4JKhbG5EaUOymxqssmB9EYXpVtXVcASp2gHgv1mxvsHTLl3T1VoP5geGEbX+ASRIzMeVogbZpYOfb58gtB1kwRaYRoCXyLLAWwjmBat3wBJN0kv1kyDMSeSh+NV7RcFbnWdcvzQrSVbgqsi2onsNm1ZVue8B6L9zsyGKJ/9S2ZLx5c6jFZ9d6cHToMkHw4Y18XzZX/cDQwU/CmMVDusCkrKSRwoYwRRM9AoxFOwI4/ZIOfu57RLbpo75FWEwa7bCOungOy6CEi2fMBWez9AyOerLeURMJ1AE7hsz19TBs0ho5mquQwRubNs6uPRlthyWvZFq1RRSt3dSERZZkLoN8n188TOKE90ostDhaJqZ8De/Dh7t0BukLXdILSb9IT1UAeHelMjecJ4l1kATsI2rrul3dn0oeYiDaF/wL19icQuWQt', 'source': 'donut_slide.pptx'}),\n",
       " Document(page_content='We believe our work can easily be extended to other domains/tasks regarding document understanding.\\n\\nContact: gwkim.rsrch@gmail.com\\n\\nIf you have any questions please feel free to contact me :)\\x0bThank you!', metadata={'filename': 'donut_slide.pptx', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'last_modified': '2024-05-10T05:19:09', 'page_number': 73, 'orig_elements': 'eJzlVMGO0zAQ/RWTGxLt2kkcJz0hOHFBHCpxWFbVxB6nVh072E53qxX/jlNYbVlW7BXBKcrMm/GbN6N3fV+gxRFd2hlVbEgBsq1VKbhuqp6VHbS0Yl0DXCqqpURRvCHFiAkUJMj4+0JCwsGH007hlPY5RDNCG4sORlw6Ku/mtIvWKFxPU7orfubTaTrnYZqsyV2Md1dHp9Z+Qnc3Wu3DCCmuvNZGovJyXliup4Axf8/w0f7yuzS24IYZBoy583WBbihuztGYdqNXRhs8T1nSsl5RvmJ0S/mGdRvaLdVTrty5eewxZJSozqHwKE5LM5OaStYKyrtKMM2grLuy44xWErD4lisS3qUF/BlJj9bgEYmfA7n14UAkOIIQjT3lHMlAdAoVSZ74tMdAlB/BuHiVIB4iCThAUMYN5GF8Mmd8iAncEl4vnB9k/AghZBmOuF3ez0Sebrary7qW0EjNhG6Q6Q5U1XGqKt1h08nfNvs3LfFS2MupP2XYu4BweG5i1bRCNEpXdVsCNLxlXLSib0DJClkPL98y+2duub5U8L3Pz8i0IcPtwYzrEIPcvx3y6dm19OOlvluTLD6nbY2s5NiXSjes7hlre9kLzlFp0TSspf+TT9RPfeLFw7tcxgdNTn4me8hGAe5Evs4YF5qRTDZ7BRKNaIkOiItPyB+rIyOSzesvM6W03+7BHZYer/7gBzffAa4K8wU=', 'source': 'donut_slide.pptx'}),\n",
       " Document(page_content='# Donut 🍩 : Document Understanding Transformer\\n\\n[![Paper](https://img.shields.io/badge/Paper-arxiv.2111.15664-red)](https://arxiv.org/abs/2111.15664)\\n[![Conference](https://img.shields.io/badge/ECCV-2022-blue)](#how-to-cite)\\n[![Demo](https://img.shields.io/badge/Demo-Gradio-brightgreen)](#demo)\\n[![Demo](https://img.shields.io/badge/Demo-Colab-orange)](#demo)\\n[![PyPI](https://img.shields.io/pypi/v/donut-python?color=green&label=pip%20install%20donut-python)](https://pypi.org/project/donut-python)', metadata={'filename': 'donut_readme.md', 'filetype': 'text/markdown', 'last_modified': '2024-05-10T05:19:09', 'page_number': 1, 'orig_elements': 'eJyVVG1P2zAQ/itZ0aohkaQJpG9ShbrCoJOAsgJfWhQ58SXx8EtmO20D47/PSYGWbYjtSxT5nnvuufPjmz00gAIDrkOCG32rEXTjTteHTtfDEXhxK95vdwK/53cDiJOo1WvsWQ0GGmGkkcE/NBJCgSMGVTIWvNChBIQZOAxX2Cqsy7wOa1hplyF5h8WSV0GKeFqgFJSJzhrA08Ztfap0yAQmCYFak9/yD+xWYHutq1bQ93r9tYzcZIa8YBFIg/Ie99YVqowd66iSYs0L3N2PzTdp96y+OYyLqlfrmmOQSiOOCU+tK4m4SoRkIOd8zmcfZhOUg7z9lGmdq77rEpY6KiNAsXKIcCOEU3BrjI3kiiwc3/M8xwva7QNbAt7dZK7DQqYuipS7ge3WZUaCJyCBx/BOrePR6MY2Y/DtiBZg+HcysbS1sGOiYc11BEy8w1JB7BOJMBF2JEma6VQC8IoOm9D/8YwERZEtzOhSeM0wKSfjNxnyMifuwq2dYuelzgQ/jAUVclBraRpSoIOc5B/9FuHmiig1f9vwrelWZPVwcym+Q6xf0T61Y6xGBcJqk2VINYmdHPLS0RBnrrlHJTii5B6wvW7xlT4TJwIPtDBqmgUnWg0I1yC54anyQlUqDaxJIdHhSzNlsx7x08HWvNe4yqqDF3XbTW10/b2rOb9IEhITRK0xy9evt1ZiieTJ98bY1rTkOjsSJ9ZP63c/v+nKCjulBG/5EYtYOakQKQUnFsyIAvVc0MWuly72hx19sMqXy3Ocr27CEk7vjy/OShtWX0fD+/aQDse9iyCZdl3ARB8WKh+oDEnz8up6E2GGJ/+9IPOS7udoyILFahL/KDkPz6IvCbsc0tPL8bfgpOPb2eX1l6mv7v4sWO2N53V0jqQ0rAu4qrbG4+0vZXTBkg==', 'source': 'donut_readme.md'}),\n",
       " Document(page_content='[![Downloads](https://static.pepy.tech/personalized-badge/donut-python?period=total&units=international_system&left_color=grey&right_color=brightgreen&left_text=Downloads)](https://pepy.tech/project/donut-python)', metadata={'filename': 'donut_readme.md', 'filetype': 'text/markdown', 'last_modified': '2024-05-10T05:19:09', 'page_number': 1, 'orig_elements': 'eJyVVG1P2zAQ/itZ0aohkaQJpG9ShbrCoJOAsgJfWhQ58SXx8EtmO20D47/PSYGWbYjtSxT5nnvuufPjmz00gAIDrkOCG32rEXTjTteHTtfDEXhxK95vdwK/53cDiJOo1WvsWQ0GGmGkkcE/NBJCgSMGVTIWvNChBIQZOAxX2Cqsy7wOa1hplyF5h8WSV0GKeFqgFJSJzhrA08Ztfap0yAQmCYFak9/yD+xWYHutq1bQ93r9tYzcZIa8YBFIg/Ie99YVqowd66iSYs0L3N2PzTdp96y+OYyLqlfrmmOQSiOOCU+tK4m4SoRkIOd8zmcfZhOUg7z9lGmdq77rEpY6KiNAsXKIcCOEU3BrjI3kiiwc3/M8xwva7QNbAt7dZK7DQqYuipS7ge3WZUaCJyCBx/BOrePR6MY2Y/DtiBZg+HcysbS1sGOiYc11BEy8w1JB7BOJMBF2JEma6VQC8IoOm9D/8YwERZEtzOhSeM0wKSfjNxnyMifuwq2dYuelzgQ/jAUVclBraRpSoIOc5B/9FuHmiig1f9vwrelWZPVwcym+Q6xf0T61Y6xGBcJqk2VINYmdHPLS0RBnrrlHJTii5B6wvW7xlT4TJwIPtDBqmgUnWg0I1yC54anyQlUqDaxJIdHhSzNlsx7x08HWvNe4yqqDF3XbTW10/b2rOb9IEhITRK0xy9evt1ZiieTJ98bY1rTkOjsSJ9ZP63c/v+nKCjulBG/5EYtYOakQKQUnFsyIAvVc0MWuly72hx19sMqXy3Ocr27CEk7vjy/OShtWX0fD+/aQDse9iyCZdl3ARB8WKh+oDEnz8up6E2GGJ/+9IPOS7udoyILFahL/KDkPz6IvCbsc0tPL8bfgpOPb2eX1l6mv7v4sWO2N53V0jqQ0rAu4qrbG4+0vZXTBkg==', 'is_continuation': True, 'source': 'donut_readme.md'}),\n",
       " Document(page_content='Official Implementation of Donut and SynthDoG | [Paper](https://arxiv.org/abs/2111.15664) | [Slide](https://docs.google.com/presentation/d/1gv3A7t4xpwwNdpxV_yeHzEOMy-exJCAz6AlAI9O5fS8/edit?usp=sharing) | [Poster](https://docs.google.com/presentation/d/1m1f8BbAm5vxPcqynn_MbFfmQAlHQIR5G72-hQUFS2sk/edit?usp=sharing)', metadata={'filename': 'donut_readme.md', 'filetype': 'text/markdown', 'last_modified': '2024-05-10T05:19:09', 'page_number': 1, 'orig_elements': 'eJyVVG1P2zAQ/itZ0aohkaQJpG9ShbrCoJOAsgJfWhQ58SXx8EtmO20D47/PSYGWbYjtSxT5nnvuufPjmz00gAIDrkOCG32rEXTjTteHTtfDEXhxK95vdwK/53cDiJOo1WvsWQ0GGmGkkcE/NBJCgSMGVTIWvNChBIQZOAxX2Cqsy7wOa1hplyF5h8WSV0GKeFqgFJSJzhrA08Ztfap0yAQmCYFak9/yD+xWYHutq1bQ93r9tYzcZIa8YBFIg/Ie99YVqowd66iSYs0L3N2PzTdp96y+OYyLqlfrmmOQSiOOCU+tK4m4SoRkIOd8zmcfZhOUg7z9lGmdq77rEpY6KiNAsXKIcCOEU3BrjI3kiiwc3/M8xwva7QNbAt7dZK7DQqYuipS7ge3WZUaCJyCBx/BOrePR6MY2Y/DtiBZg+HcysbS1sGOiYc11BEy8w1JB7BOJMBF2JEma6VQC8IoOm9D/8YwERZEtzOhSeM0wKSfjNxnyMifuwq2dYuelzgQ/jAUVclBraRpSoIOc5B/9FuHmiig1f9vwrelWZPVwcym+Q6xf0T61Y6xGBcJqk2VINYmdHPLS0RBnrrlHJTii5B6wvW7xlT4TJwIPtDBqmgUnWg0I1yC54anyQlUqDaxJIdHhSzNlsx7x08HWvNe4yqqDF3XbTW10/b2rOb9IEhITRK0xy9evt1ZiieTJ98bY1rTkOjsSJ9ZP63c/v+nKCjulBG/5EYtYOakQKQUnFsyIAvVc0MWuly72hx19sMqXy3Ocr27CEk7vjy/OShtWX0fD+/aQDse9iyCZdl3ARB8WKh+oDEnz8up6E2GGJ/+9IPOS7udoyILFahL/KDkPz6IvCbsc0tPL8bfgpOPb2eX1l6mv7v4sWO2N53V0jqQ0rAu4qrbG4+0vZXTBkg==', 'is_continuation': True, 'source': 'donut_readme.md'}),\n",
       " Document(page_content='Introduction', metadata={'filename': 'donut_readme.md', 'filetype': 'text/markdown', 'last_modified': '2024-05-10T05:19:09', 'page_number': 1, 'orig_elements': 'eJw1jsFuwjAQRH8F7RmKYxpR5w+4c4uiyMmOI6uxjcJGtEL8e2NXXOfN00z7JMwIiNJ7pmZHg3Zu1Kr+OquTg6s1a0AZNw58OhvDtN9RgFi2Yrf+k5yfEW1AljnFVfoFlgM+QulmLL+3ggU/cgx2+eb0iBnONk6rnXDfaEuIE3UlvUsfEnvnUT5ppT8Pqj5U6qrqpjKNMtm+bWYf1zBg2VrVa/+/kI1LlCXxOopPZej94OplBr26P5UhTPA=', 'source': 'donut_readme.md'}),\n",
       " Document(page_content='Donut 🍩, Document understanding transformer, is a new method of document understanding that utilizes an OCR-free end-to-end Transformer model. Donut does not require off-the-shelf OCR engines/APIs, yet it shows state-of-the-art performances on various visual document understanding tasks, such as visual document classification or information extraction (a.k.a. document parsing).', metadata={'filename': 'donut_readme.md', 'filetype': 'text/markdown', 'last_modified': '2024-05-10T05:19:09', 'page_number': 1, 'orig_elements': 'eJx1UsFu2zAM/RXCpw2wXcVOmji3YQGKXrZhy60tAtqiYiG2lEly0qzov4+y0aXA0Issk3yPT498eEmoo55M2GmZrCGpb+uVWIqiwlWxWOJc1HK+wlLQklaiKqskhaSngBIDcv1LonRHBnuKYGnNEHaOUPaU9zLWxnS4HMd0oOdw06M7SHs2Mdmh2Q+4J8/Zh4TMPnkaoz7seiu10jRqKkQxz8Qim4mtWKxn1VqMMo6M3Jmhr8lx1WyMuHcvKZRqCrFYLUWpSC0KWRCJSjW1LJdVJZPXdJIUizdROTwOclU2fKrbKoWNbYboDAxGkvMBjdRmD8Gh8cq6nlwK2gOCoTOwJ62VYBXID2AtcijoTv8hBhn4/vVnphwRkJFZsBl/YHvlBnaAuhwmZdIyyNgAjn4P2hE3UlloKfMtdSpyMc1eG/I3X37c+xQuFEAH8K09e2ARgTI7IdAFOJKLXdA0TGsNnNBpO3g4aT9g9+ET0B+Y2g9NC/h/ccOD8zy0BoNmTutAm7HL+MtGO2zG6yfMDznmVyTPzXODzzk8mnsDKKWOhSmcCY6OfKz5dTGh3di7aUiSz6a8TacwBd1cx3VHhhwGy+MZTWeHjp6vNFkaGTPWos34Jgs1geroWdcdvffi33LysCRr7Rnh87h4b/v8DR330Sfaxi16ffoLbzoibA==', 'source': 'donut_readme.md'}),\n",
       " Document(page_content='In addition, we present SynthDoG 🐶, Synthetic Document Generator, that helps the model pre-training to be flexible on various languages and domains.', metadata={'filename': 'donut_readme.md', 'filetype': 'text/markdown', 'last_modified': '2024-05-10T05:19:09', 'page_number': 1, 'orig_elements': 'eJx1UsFu2zAM/RXCpw2wXcVOmji3YQGKXrZhy60tAtqiYiG2lEly0qzov4+y0aXA0Issk3yPT498eEmoo55M2GmZrCGpb+uVWIqiwlWxWOJc1HK+wlLQklaiKqskhaSngBIDcv1LonRHBnuKYGnNEHaOUPaU9zLWxnS4HMd0oOdw06M7SHs2Mdmh2Q+4J8/Zh4TMPnkaoz7seiu10jRqKkQxz8Qim4mtWKxn1VqMMo6M3Jmhr8lx1WyMuHcvKZRqCrFYLUWpSC0KWRCJSjW1LJdVJZPXdJIUizdROTwOclU2fKrbKoWNbYboDAxGkvMBjdRmD8Gh8cq6nlwK2gOCoTOwJ62VYBXID2AtcijoTv8hBhn4/vVnphwRkJFZsBl/YHvlBnaAuhwmZdIyyNgAjn4P2hE3UlloKfMtdSpyMc1eG/I3X37c+xQuFEAH8K09e2ARgTI7IdAFOJKLXdA0TGsNnNBpO3g4aT9g9+ET0B+Y2g9NC/h/ccOD8zy0BoNmTutAm7HL+MtGO2zG6yfMDznmVyTPzXODzzk8mnsDKKWOhSmcCY6OfKz5dTGh3di7aUiSz6a8TacwBd1cx3VHhhwGy+MZTWeHjp6vNFkaGTPWos34Jgs1geroWdcdvffi33LysCRr7Rnh87h4b/v8DR330Sfaxi16ffoLbzoibA==', 'is_continuation': True, 'source': 'donut_readme.md'}),\n",
       " Document(page_content='Our academic paper, which describes our method in detail and provides full experimental results and analyses, can be found here:\\n\\nOCR-free Document Understanding Transformer.\\n\\nGeewook Kim,\\n\\nTeakgyu Hong,\\n\\nMoonbin Yim,\\n\\nJeongYeon Nam,\\n\\nJinyoung Park,\\n\\nJinyeong Yim,\\n\\nWonseok Hwang,\\n\\nSangdoo Yun,\\n\\nDongyoon Han,\\n\\nSeunghyun Park. In ECCV 2022.', metadata={'filename': 'donut_readme.md', 'filetype': 'text/markdown', 'last_modified': '2024-05-10T05:19:09', 'page_number': 1, 'orig_elements': 'eJzdlttuGzcQhl+F0LXs8nzwrVPUSdG0aJUERhAYQ85QWniXK+whjhrk3cNVUcQJUhRVbwxhgb3gDMnhx39m+PbjilrqqEx3Da6u2MqGiEZlUI6S99lZQ1GFQFnHoJJWqzVbdTQBwgTV/+MqNy0V6GiZjH2Zp7uBADu67HDxXczTYX80T/Rh+qGD4R77h7IYWyjbGbY0VuvbFZXt6t1xdJzuuh6b3NAxJsmlvuDmQvANN1ciXPGwzN7XmXdl7iIN1UscR4YvJ4ky5yS58Y6rTNlIlEQ85BRRuRBw9Wn9V0iL86/zwCABUtcktoc9DWv2sGvSjiGNaWgijayvPvXoux5ZU+r4BE3LoCDbD/37pvqxPLctow91drMghZYNNM7tNB7doEB7GGlcswSFRWK5n+vwjga6Ws7zN6aXMAwwNe9pswRXo/z2jtBYjDpF57SN3iPE5GVQQnqlA3L9dO/oK+TXv1/kgYg969O8nI69KkjDOFVWTdmyzQBlzP3Q0XD5GM+mmVr6HhbFiWRAEBF1pJR9/WQ2PtqgnHHy6WL5Vrr/esGPOf5E9ND39+znpls/5vSqJJho2w/Nn4T/JCWjl8UrJKWcVdI7yynmSssAGIf2TJltCO63h5nd9GX736GpEEV0XAbONRihotEuRrRSSIdBhzOF9kvfl1gr3+0pQkOFwVmKOiS0iQcSZHndVEUEJ+y5Cu0FVYXd1h97CSdQUwqyNwakCZayDEEoScCNVDIjhCdc6f8ftaYcam/cst9qeCcUtZqFCgUnRzmImqj1E1xkaX0UEPCMqS1yOy1BCUEHQVz6islawrqr5FFZKxLX+lyhvenLSLV73jzAKa1AahG5kFZZ71D7unwMserOZacC0rkm6B81Eux7djuXE5SWvTQyq8jra1wrLbJzJoeQlOIRApwps2c1NQ+1hbIbOAGaUcZaHoVWZLSMyH3tAZQpxKx8TPzpQvtKN1Sr+u4wl2Nlv2TPC/vx+vo1qyvJ7z3y330GpLl7vg==', 'source': 'donut_readme.md'}),\n",
       " Document(page_content='Pre-trained Models and Web Demos\\n\\nGradio web demos are available!  \\n|:--:|\\n||\\n- You can run the demo with ./app.py file.\\n- Sample images are available at ./misc and more receipt images are available at CORD dataset link.\\n- Web demos are available from the links in the following table.\\n- Note: We have updated the Google Colab demo (as of June 15, 2023) to ensure its proper working.', metadata={'filename': 'donut_readme.md', 'filetype': 'text/markdown', 'last_modified': '2024-05-10T05:19:09', 'page_number': 1, 'orig_elements': 'eJzNUktr3DAQ/itTn1pYO7Z3V619TSBQaFrahVDSsIw9Y69YPYwsrxuS/PdKXgppaQ+99SAEmu8x843uHhNWrNn4vaSkhgQbaqkpShZtzuvNO7GhhsTbLqftpmpJJCtINHsk9Bjwj0knFRvUHMlkzeT3jpE0Z5oiNpb9w7CUPX/3FxrdkexsYlGh6SfseQzVu4RNn9wvr6Pfa0uyk7z0VOblJs23aZHv8m1dVHVeRfYQmHsz6YZdQBXPq7NDZHxynHqH0jDBB0usRkBDcMsNXLG2Y6T/7GonveIksH+PYl0WosxFK0oqmgKp2VRNK6pCiLKgMs//3yiWF/cPS32Z3bVDkhbmkBXFrCAoAZ5QKmwUvwL4Zp7qNK2fwh1OCl/tBC0acJMBf+CFBbP0B8gucBiy4QHi6FnEfkE9KAap46i/KgP6QNBybJddaRuKjluWg/8r/vLj5yuI+Y/sQUlzXExu/9w6dM7qpcOIHEGe2+2sUnaWpgcfUYvCjfVcBx044IlhGoJF+EkRfW1tH6QubZA8T/oaR7AdvJ8MQ7FdQVjR+g14C2zGKfhLP8Lg7MAOZuuOwSh7+f1u0Dn08sS7mP/z/Q9fUSXw', 'source': 'donut_readme.md'}),\n",
       " Document(page_content='Task Sec/Img Score Trained Model Demo CORD (Document Parsing) 0.7 / 0.7 / 1.2 91.3 / 91.1 / 90.9 donut-base-finetuned-cord-v2 (1280) / donut-base-finetuned-cord-v1 (1280) / donut-base-finetuned-cord-v1-2560 gradio space web demo , google colab demo (updated at 23.06.15) Train Ticket (Document Parsing) 0.6 98.7 donut-base-finetuned-zhtrainticket google colab demo (updated at 23.06.15) RVL-CDIP (Document Classification) 0.75 95.3 donut-base-finetuned-rvlcdip gradio space web demo , google colab', metadata={'last_modified': '2024-05-10T05:19:09', 'text_as_html': '<table><tr><td>Task</td><td>Sec/Img</td><td>Score</td><td>Trained Model</td><td>Demo</td></tr><tr><td>CORD (Document Parsing)</td><td>0.7 / 0.7 / 1.2</td><td>91.3 / 91.1 / 90.9</td><td>donut-base-finetuned-cord-v2 (1280) / donut-base-finetuned-cord-v1 (1280) / donut-base-finetuned-cord-v1-2560</td><td>gradio space web demo , google colab demo (updated at 23.06.15)</td></tr><tr><td>Train Ticket (Document Parsing)</td><td>0.6</td><td>98.7</td><td>donut-base-finetuned-zhtrainticket</td><td>google c', 'page_number': 1, 'filetype': 'text/markdown', 'filename': 'donut_readme.md', 'orig_elements': 'eJy1VV1v2jAU/StXeSpSk9ghXyBUaYKXSpvWdagvVYUc+yaNSGKWGLqt6n+vHVAoK0ursT5AuJ8+Ppyj3D5aWGCJlVrkwhqDlaYiwdD3KQnTCEngx9SLKBlGbDQMvBCtc7BKVEwwxXT/o5XmBVasRDMsZLVWixqZKNEphek1ZfVr1ZYV/lRuyeqlkA+VKRasytYsw0ZXby2sMuuuzTZqUUqRpzm2mDzi+TYJbErmJBjT0ZiMzPRKTy6qdZlgrbtom6n3N2GJ4CKhHoac4NCPQ18kIoxSIgJ/xEVoVhhEC9Ys7lVZmJmJYkmBFxNV64+4mLNmOXH1DxN8R+5eltk+5rLGLprXLK9QwBcpsOiyMyzlNnDbldu106/XMzibSb42xMMVq5u8ygbdFHEicGH7TR2vy4+oM9Qp/aDmQZxRV2qZtxPWoJ1qHGqtsdgaoLA3HpxRLyYDPdLTRd/XZXtBSLpTs5qJXEKzYhzhARMQ+r5wDpmUWYHAZcF2ubP1SitG88MUeEOHhA4NBq+ZaVmEec6XqPoZCvesxE7Uz8Pve2X2qnbtHvy/gry++WxPZ5dXLwBOtWYbrVfOVC6rg38y2AMNnGE/0HpTcJGvPo5fDfjm2ycwuqYv4OvcAea4C8LICfoxC8k3P9gHQHa3XrSedj41/jTAYWdEaA0IB8YDYzj4i7/gD1/Baz/1af//+whOJAvesIu+agjGHsehHNji3We+rX7DcABG7cfP3an85Nv3iNlAiMGI9ziErWhPRdC+QXbvtrkRq/V09wzqSFc5', 'source': 'donut_readme.md'}),\n",
       " Document(page_content='demo (updated at 23.06.15) DocVQA Task1 (Document VQA) 0.78 67.5 donut-base-finetuned-docvqa gradio space web demo , google colab demo (updated at 23.06.15)', metadata={'last_modified': '2024-05-10T05:19:09', 'text_as_html': 'olab demo (updated at 23.06.15)</td></tr><tr><td>RVL-CDIP (Document Classification)</td><td>0.75</td><td>95.3</td><td>donut-base-finetuned-rvlcdip</td><td>gradio space web demo , google colab demo (updated at 23.06.15)</td></tr><tr><td>DocVQA Task1 (Document VQA)</td><td>0.78</td><td>67.5</td><td>donut-base-finetuned-docvqa</td><td>gradio space web demo , google colab demo (updated at 23.06.15)</td></tr></table>', 'page_number': 1, 'filetype': 'text/markdown', 'filename': 'donut_readme.md', 'orig_elements': 'eJy1VV1v2jAU/StXeSpSk9ghXyBUaYKXSpvWdagvVYUc+yaNSGKWGLqt6n+vHVAoK0ursT5AuJ8+Ppyj3D5aWGCJlVrkwhqDlaYiwdD3KQnTCEngx9SLKBlGbDQMvBCtc7BKVEwwxXT/o5XmBVasRDMsZLVWixqZKNEphek1ZfVr1ZYV/lRuyeqlkA+VKRasytYsw0ZXby2sMuuuzTZqUUqRpzm2mDzi+TYJbErmJBjT0ZiMzPRKTy6qdZlgrbtom6n3N2GJ4CKhHoac4NCPQ18kIoxSIgJ/xEVoVhhEC9Ys7lVZmJmJYkmBFxNV64+4mLNmOXH1DxN8R+5eltk+5rLGLprXLK9QwBcpsOiyMyzlNnDbldu106/XMzibSb42xMMVq5u8ygbdFHEicGH7TR2vy4+oM9Qp/aDmQZxRV2qZtxPWoJ1qHGqtsdgaoLA3HpxRLyYDPdLTRd/XZXtBSLpTs5qJXEKzYhzhARMQ+r5wDpmUWYHAZcF2ubP1SitG88MUeEOHhA4NBq+ZaVmEec6XqPoZCvesxE7Uz8Pve2X2qnbtHvy/gry++WxPZ5dXLwBOtWYbrVfOVC6rg38y2AMNnGE/0HpTcJGvPo5fDfjm2ycwuqYv4OvcAea4C8LICfoxC8k3P9gHQHa3XrSedj41/jTAYWdEaA0IB8YDYzj4i7/gD1/Baz/1af//+whOJAvesIu+agjGHsehHNji3We+rX7DcABG7cfP3an85Nv3iNlAiMGI9ziErWhPRdC+QXbvtrkRq/V09wzqSFc5', 'is_continuation': True, 'source': 'donut_readme.md'}),\n",
       " Document(page_content='The links to the pre-trained backbones are here:\\n- donut-base: trained with 64 A100 GPUs (~2.5 days), number of layers (encoder: {2,2,14,2}, decoder: 4), input size 2560x1920, swin window size 10, IIT-CDIP (11M) and SynthDoG (English, Chinese, Japanese, Korean, 0.5M x 4).\\n- donut-proto: (preliminary model) trained with 8 V100 GPUs (~5 days), number of layers (encoder: {2,2,18,2}, decoder: 4), input size 2048x1536, swin window size 8, and SynthDoG (English, Japanese, Korean, 0.4M x 3).', metadata={'filename': 'donut_readme.md', 'filetype': 'text/markdown', 'last_modified': '2024-05-10T05:19:09', 'page_number': 1, 'orig_elements': 'eJyNklFr2zAUhf/KxU8JyKlky67tt9GOko2OwrK9dCVI1nUsYktGVpZkpfvtk5OWddCNvVn3nGud7+reP0bYYY/Gr7WKKogESp6nPJOXEtNC8qTheZnVTVNQlpYsiwhEPXqhhBfB/xg1ukMjepyalTU7v3YoVI+LXk3eSfbH4SR7PPiLXritsnsziZ0wm53Y4BjU+wjNJno4VUe/7q3SjcZTpoQmPKZZzOiKZhUrK1pO3UPoXJtdL9EFFztV3CsSqWolWYJ5TTHlRc6VVPllQ1XGy1rl0RM5R5rMqxah02Y7grfgw2FwGHsntEEFUtRbaQ2OEP4PLTqsvpkYTrSxFCNW8GLda99CzuEdoxRu7r6MMPuZLDJQ4jjOCZzTgm2gE0d0QUVTW4UuTDIhCWGcJCGWwuciDz3aDDsPo/6BkGQ5PbAyoQTGvTbhNhNGedZYKC6Xq/jqenkHM8Zu5yCMgs9H49trewOz92bT6bElcNWGqCMS+CAGcf76aMOjGQJ0kd3CIVy7+A04OOttBbMwkU732gh3hPA62M3/pC7g6yvo/0cu/o1MeXFgWZq/gVyQvyG+RcYnsnS+mFbnZSM/CeeE199xNe3B08MvnST1dQ==', 'source': 'donut_readme.md'}),\n",
       " Document(page_content='Please see our paper for more details.\\n\\nSynthDoG datasets\\n\\nThe links to the SynthDoG-generated datasets are here:\\n\\nsynthdog-en: English, 0.5M.\\n\\nsynthdog-zh: Chinese, 0.5M.\\n\\nsynthdog-ja: Japanese, 0.5M.\\n\\nsynthdog-ko: Korean, 0.5M.\\n\\nTo generate synthetic datasets with our SynthDoG, please see ./synthdog/README.md and our paper for details.', metadata={'filename': 'donut_readme.md', 'filetype': 'text/markdown', 'last_modified': '2024-05-10T05:19:09', 'page_number': 1, 'orig_elements': 'eJzdlktv2zAMgP+K4HOTSrIelm/DWgx7dBi23IoioCQq8WrLga2s64b998kZ0nZFgS07dbkYkEiK5GdK5OX3AlvsMKZl44uaFKFEhgjeUVMaBlhqaSVVXkmms4QXJ6ToMIGHBFn/exGaFiN0OBn7Pm7TcsjmHc47P+lO4nS72YkTfk2nHQzXvr+Jk7CFuNrCCscsvSwwroqr3e6Yll3vm9DgLiZOuZhROWN0QWXNTE3NZL3Jlsu47SwOWYvtdob7TMB65y3jqBzFUlRKeOuVDtRLYZxXxY+TXyFNyh9ahBHJiEj67UA2sMGBhH4gXT8g8Tnhph3nk9d9Mu9hGCA1X3AxHZHPekxSeu2lC8FY7r2WAL6SloErbcihcPV8ST4E8+k2pvVZ/4pMYY6YxocMFk1q8anctXdloExSqYwGoFTagFXQstI2OFk939wfV9Ef/+JDWIs1kraJ1yNJPUl5sac3W2HEXC7o7ziS7IWsccD6oKpiOnBmchU7IVRpcjSopTfcGWRCmf/ofh5EdpxA+n41w1iT87hqm3F9QuhcXvx2Kd81Y3qdsHuKXFnRoJjXlWDM5rqU3DjJ0XKqPQQUx07u27omL9dNxBEPJOeUAe4Yyw6kq5S3xhrHBAcphQjmWG/zHbnPUJM3sIF/QKf11G1KzZnKX6ZEfgrBlkFYVSoe5LGju+5r8ja3UIgHghOOOsry+1YFJ1TGxwWzzlf55eMBDDtScIue7FsF2UHE1Lj7nnHTpPVuQNk3lhOyuZ9c5qd77qcfz1+cXZzn3AlE/2ik+Ytp5uonICAdhg==', 'source': 'donut_readme.md'}),\n",
       " Document(page_content='Updates\\n\\n2023-06-15 We have updated all Google Colab demos to ensure its proper working.\\n\\n2022-11-14 New version 1.0.9 is released (\\n\\npip install donut-python --upgrade). See\\n\\n1.0.9 Release Notes.\\n\\n2022-08-12 Donut 🍩 is also available at\\n\\nhuggingface/transformers 🤗 (contributed by\\n\\n@NielsRogge).\\n\\ndonut-python loads the pre-trained weights from the\\n\\nofficial branch of the model repositories. See\\n\\n1.0.5 Release Notes.\\n\\n2022-08-05 A well-executed hands-on tutorial on donut 🍩 is published at', metadata={'filename': 'donut_readme.md', 'filetype': 'text/markdown', 'last_modified': '2024-05-10T05:19:09', 'page_number': 1, 'orig_elements': 'eJzdl0tv3DYQx78KsacUiDakKL58atECvfmQuughDYwhOdQSkURBDztG0O/e0bqHrWPU2Z42vggCHxrOj/956MOXHXbY47Dc5ri7YjtjUaugFA8alFI2eA+Q6qaRVkvTwO4t2/W4QIQFaP2XXcodDtDjtjmWYV1uJ4TY476P29ptenkYj9MLfl7e9TB9iuV+2CY7GNoVWpxp9sMOh3b38Tg6L7d9iTllPJ6p5nVTcVUJfsPVlXBX3G27R9p5O6y9x4lWib/ePlrYdvw+0vnos9vQP8Zv8tLhjhZ95TFYk3xDXqeEXGqItXGh1lY7Yby7YI+PI9MZd3eKiEzIiutKKPYHsgPcIVuP1CKDrmO/ltJ2yH4uHXgWsS8zWwrDYV4nZHmZ2TiVESd2X6ZPeWj3p6ivYZpgyXd4s9l6BjkGVaMTwTSYtNFeJwGEPllrYwpavF7kdSVEJRp2jffsDqc5l4GJPd87lmc2ESWY6QLenEUzOvBCY2Oss4311kkhhZBWIkcdXXqlNMc8sjzMy6bWoxfV+LAciGdVrWM7QcQf9uw3xLNYemGDAOm1SZZSgtRaNdzU6LUWtZTyclmeonlU1PtHObHrQqlw/025sKmdrkk9wFMKLpFlsYWnSSkpgcFervtPpfTiRX4VmNxWoma/bE6wP9doZaBn0sfIhG4uDO4gUzakrAjLWaLiZBoAGkp6SSoRpHPciMYLr7xqmvhKqR7WtqXSkCDgu2WCYU5l6innPdJFekZh2JtQhmXKft0qj384C2xI3DnDfbTB1Y1yluo1esdDoyWgCZcL9pTTj9cZu/l9aVvKWN8UpoGTlFBFVElJTiyFtsaoIJRTSfLvSFAv3uApqH9l+a5ApJ7kgNSJYEX6ygMJ6B5ze6DuJE2l3ybPLKRo0Cjq/3wA0E0Uto7eJGeiCSC/o7bkLKwlpRwydMxTkIYDK+mIlU6BHfUkY5nzUqZMNeTscgq1cNHwWAfntAupBtFIEZT1QnIvLrg1eVpO1f8ppyKmOjROYtANCQoawZNBWUtOzUwyF+z+U0G9eJHPlVOu2E8UkV1X4WcMxwR/gCHOFYXvsm6aItHRe3y25o6r7/J82P5H/qvcfvwb2XKD+g==', 'source': 'donut_readme.md'}),\n",
       " Document(page_content='Towards Data Science (written by\\n\\n@estaudere).\\n\\n2022-07-20 First Commit, We release our code, model weights, synthetic data and generator.', metadata={'filename': 'donut_readme.md', 'filetype': 'text/markdown', 'last_modified': '2024-05-10T05:19:09', 'page_number': 1, 'orig_elements': 'eJzVUk1v1DAQ/Ssjn6iULY7z4WRPSCCOXFiJQ1WtJvYktRo7lTNhWVX979hBSMCp3OD6PvTePPvuWdBMngKfnRVHEINRqEut+pFID7IfqrE3dWW1JK1kY0UBwhOjRcakfxajmymgp2y2S9j4HAmtp1u/azPN16edZvrGbz3GR7tcQiZnDNOGE62JvRMUJnG/oyuf/WLd6GjvpKSqD7I5lPIkm2PZH2Wf3U/JeQ6bHygmVflS/EjIjtNywWhX+JBawmfjKBiCN5fomCnAcM32n61OjmcSyf3nFEoNplfY6aqsKlu2vW5aWzdjZ3Qnlaz/jyne0cq4WYp0c/uqs1vZ2rG2ujGVxLYqWyRD2HVG5v/QDf/u2TsS/+IBf90pRaiD1Acl4aOLK8P7xXvHBXwhiGkhXAmWLYJZLBWQytEMF3LTA68FrNfAD8TOQF4FMFiYKFBEXuJvq3/CmED3lU459uX+Oz8QE0U=', 'source': 'donut_readme.md'}),\n",
       " Document(page_content='Software installation\\n\\nbash\\npip install donut-python\\n\\nor clone this repository and install the dependencies:\\nbash\\ngit clone https://github.com/clovaai/donut.git\\ncd donut/\\nconda create -n donut_official python=3.7\\nconda activate donut_official\\npip install .\\n\\nWe tested donut-python == 1.0.1 with:\\n- torch == 1.11.0+cu113 \\n- torchvision == 0.12.0+cu113\\n- pytorch-lightning == 1.6.4\\n- transformers == 4.11.3\\n- timm == 0.5.4', metadata={'filename': 'donut_readme.md', 'filetype': 'text/markdown', 'last_modified': '2024-05-10T05:19:09', 'page_number': 1, 'orig_elements': 'eJzdU8uOmzAU/RWLbQewzSMQKb/QTSN1MRlFF/s6WAUbGSdpNOq/14Zm1KhVpdlVswKdx7XvOfD8muCAIxp/1DLZkkRUBZVVhwqQNpKyTjaNYHTDy5YqVlTJE0lG9CDBQ9C/JkoPaGDEaJbWnP3RIcgRs1FGbaT9bVpoj999PoL7Ju3VRHIAczrDCefAPidoTsnLgs7+OFqplcblTpzyMqVVyuieVlvWbmkb3VNwHs157NAFFfvxtJ4QHV+s8ldwSLSZPQwDeG2XE+9X2Ws/YBIsf+yvRLGpKVe14GHxpi2pLIuKolKy6QT/f/dfEPeOJn8PrIO5P5hJT/fEyLJLOt18/5jcZ3AuxHnBfbT+JcF6QzcoGecUOaoWWiF5DU0hqyo+4IMmaB0RgzVIfK9n4nCys/bW3QgY+Raq75FInNBINELjvD2YNfmT9r/svffTvM3zgPTnLhN2zANxAdD5Ek4WiIMRci0oD6/WSCAiZOaRpGbFj1YpLTQMZG1wV2SbuxREqC+KH5WP9Wfv6pzyRnZNjVXRUlEJJqGWDNpWcbEpsVUftPOvoW2cPcqHv4XsdoRlNGPkGjoMFackfAiiX3EWqE/izFhB3piLnvXqCy5+5yMdJkZBOuhT7402p3VInZWL2YGZlXUjujkSZZy++Lwex3VelZX/qPLlJ/2s5Do=', 'source': 'donut_readme.md'}),\n",
       " Document(page_content='Note: From several reported issues, we have noticed increased challenges in configuring the testing environment for donut-python due to recent updates in key dependency libraries. While we are actively working on a solution, we have updated the Google Colab demo (as of June 15, 2023) to ensure its proper working. For assistance, we encourage you to refer to the following demo links: CORD Colab Demo, Train Ticket Colab Demo, RVL-CDIP Colab Demo, DocVQA Colab Demo.\\n\\nGetting Started', metadata={'filename': 'donut_readme.md', 'filetype': 'text/markdown', 'last_modified': '2024-05-10T05:19:09', 'page_number': 1, 'orig_elements': 'eJzNUt1u2jAUfpWjXG0SsCQQmnA3gVptmrqtQ91FVaET+xgsEjuyHRiq+u49DqtK9wS7s87f9+Pv4SmhhloyYaNlsoCkyjOZzxQqVWGG5RSpLPOqlFk+F3U+zZIRJC0FlBiQ558SpRsy2FJcltb0YeMIZUuTVsbZ2A6nbmgH+hM+tej20h5NbDZotj1uyXP3ISGzTR6Hqg+b1kqtNA2c8jSfjdNinKXrtFhk1SKt4nbHmxvTtzU5nsqGintTIoppKouaFFJayjSrZVmKLL3KZ1WqsmmRPI/OlOLwrQ20gGtnW/B0IIcNOOqsCyRBe9+TH8GRYIcHAmODFrFuBEv1/BI7bNgFFsJFENYove2dNlsIO4JAPsQ3mYN21kSzQVkHg1vj7hR21oDsedAyqIjtvmN/z9f2dAJJHRlJRpyg0bVDp8lP4PeOvY2sWDWgCPpAzQmO1u0jGt9E8Lbpg7bmjfz5shyI3Vi75QtL22DNGK2FD+jBKvjaG4KsGAE7P/0YeZHxPaPo4KFztiP3ijOBa5aC3msf0AgakJip7R1/D5xsf5aleIUfEVbZprHHyHHAbLTZ+wUsv9+t/lJZcXkEa4csf63FnsK7xt39t/Fy9eXHu+LKivufny9KkxiR1+TdonMYDVrH/+aP/zf2hSpEhrkSM5XPOTAo59VVLuqiIszEPP1/Y3+Z4hsKQ9J+BYzJvXRgrUNDyfPjCxj8Spc=', 'source': 'donut_readme.md'}),\n",
       " Document(page_content='Data\\n\\nThis repository assumes the following structure of dataset:\\n```bash\\n\\ntree dataset_name\\ndataset_name\\n├── test\\n│   ├── metadata.jsonl\\n│   ├── {image_path0}\\n│   ├── {image_path1}\\n│             .\\n│             .\\n├── train\\n│   ├── metadata.jsonl\\n│   ├── {image_path0}\\n│   ├── {image_path1}\\n│             .\\n│             .\\n└── validation\\n    ├── metadata.jsonl\\n    ├── {image_path0}\\n    ├── {image_path1}\\n              .\\n              .', metadata={'filename': 'donut_readme.md', 'filetype': 'text/markdown', 'last_modified': '2024-05-10T05:19:09', 'page_number': 1, 'orig_elements': 'eJzdk0Fv2zAMhf+KoHObSbZlxznvvFNuTeHQFp1os6RAotsVQf/7rKTFkqDLuttaHwSI76PJ9wzf7TkOaNFRYzRfMJ4r1bV1n8lCQd1KWZYdziFDkDnWWVnxG8YtEmggmPg9782ADiymZu3dSE1A0BZnVic2yfS0O8iEP+mLhfBD+0eXxAHcZoQNxkm94+g2/P5QjdRYr01v8LBTJrLiVqhbKZZCLWS9EHXq3k2djRtti2Gi5PPNcULq+Jq2S/eXyUtDA/KJuLRbdwBlIRRmLbRaKw19Xeq+rSqZtaLU/6/dQyX8w4c7zWe5NZEF3PloyIcnBjGOFiOjLbLeD4N/NG7DIoWxozEg8z1LCUSkxcqt1+sW4vY0328QApB5wGUa8EbOlc6VLnKp50oLrKCeQ1lplUsltOpb/KQ5U0B8Ta5Jdlbu/LYaMyW7dApxPBlhpKMgMsbYG8RrULPv0bvhOrs3NvnZAW3F87tReYr+fmbXqpc2Ahj3IX0UZ297gMFMSxo/mWHv8/EH7MLC3yn5Qp2teFG48hfe/wIFx8Hw', 'source': 'donut_readme.md'}),\n",
       " Document(page_content='cat dataset_name/test/metadata.jsonl\\n{\"file_name\": {image_path0}, \"ground_truth\": \"{\\\\\"gt_parse\\\\\": {ground_truth_parse}, ... {other_metadata_not_used} ... }\"}\\n{\"file_name\": {image_path1}, \"ground_truth\": \"{\\\\\"gt_parse\\\\\": {ground_truth_parse}, ... {other_metadata_not_used} ... }\"}\\n     .\\n     .\\n```\\n\\nThe structure of metadata.jsonl file is in JSON Lines text format, i.e., .jsonl. Each line consists of\\n\\nfile_name : relative path to the image file.', metadata={'filename': 'donut_readme.md', 'filetype': 'text/markdown', 'last_modified': '2024-05-10T05:19:09', 'page_number': 1, 'orig_elements': 'eJzdU01znDAM/SsanzcEzMfC3ntoJ9Meuj2FDGuwYN1ie8cW/drZ/16bJE3ayfTYQ7jA6D0JPT3p9sxwRo2GOiXZDliFKHMs+r6utnkxZJKXvKxSLnmP6Xbs2QaYRhJSkAj8MxvVjEZojMnSmoU6h0JqTLSM3AjTj9MKE36nay3cF2m/mQjOwkyLmNAH9JahmdjdGvXUaSvVqHDtiae8uErLqyzdp+Uua3ZpE7NPIbMzi+7RBVa2RtyTkrwsh74ZeVaUoumzrKoGrAVHkeXY8GrLLpv7liJ5EARRkkfqopprQh96fRCafPbWzK05t6ueldFG9UrHHk6Cjmmo1rLJ2cXIjtxCx0ho2bltQ5gCx3mM3yHrOeseCMlJksDZ0hFd9/jbzljqFo/ysqKXll3+2UP2v3qA+CRP78PhEA159PmTCePEyTr1E+U+TjjU/nvRRN3wot42dV2LLK2k4GHd0r4SveBV3b/WRdsfEXyY+kCLQ7Aj/LljEGWA8qAMvPv44T3cKIMeYjaM1mlBG1AJJsGrlZ/AGzEcYQ4sGKzxypMPVZ+7cRNibwn1SyZUeV6E2w7X3gxlmQ8oUfIcsR7HIh+G12rC7wOCHTicBamvCPGCgCyE9Yf1plYvkpdHefcLRYWrOg==', 'source': 'donut_readme.md'}),\n",
       " Document(page_content='ground_truth : string format (json dumped), the dictionary contains either gt_parse or gt_parses. Other fields (metadata) can be added to the dictionary but will not be used.\\n\\ndonut interprets all tasks as a JSON prediction problem. As a result, all donut model training share a same pipeline. For training and inference, the only thing to do is preparing gt_parse or gt_parses for the task in format described below.', metadata={'filename': 'donut_readme.md', 'filetype': 'text/markdown', 'last_modified': '2024-05-10T05:19:09', 'page_number': 1, 'orig_elements': 'eJzdUk1r3DAQ/SuDTwlsNv5Ye+299VJoKc2hvYVgJM/Yq8SWjDRmu4T+94683QZC+wcKOgx6b4b3Zt7ja0IjTWS5NZgcIFHFrs9T6nCf5qruNDbUlQ3V9b7CKu/SZAPJRKxQsRL+a9KbkayaKDajswu3nhROtJ0wciPM53mFmX7w/aT8C7qTjeCo7LCogYKgjwnZIXlafwO3k0PTG1o15Wm+u0vLuyz9npaHrDmkTeyepbO1y6TJCytbf/ybk6IsO930ebYrVaOzrKo6qlVOKiuoyat98nNzkRTJg3eLxZb9wkc4QGBv7AC985NiuHkOzgIu00x4uwE+EqDp2Dir/Bk6Z1kZG4CMIB4GbkVHIHBvddjCwwqKpRED3FxXeAudsqAJFCIhsHs/XS8MJzOOYB1H3hIIt9H9dalfTOBPTFO08/6YZZEVfVHQPkUsMauISq3KnvJMU1U39X96zFU5GMvkZ08cQMn+WIUXqeTB528PX0GQ32uW0mnZ2xY+RNRTWEberE2XSSKfZICXK8dUhKMoE2KQRcFsZhqNpS18lHv/4SiLIqAnsdDRJTLOjmcpIipnRgcmRBFiM379NTUxgGtvFC/zroFECp03WgKjaXSnf+Th6Rc3KkLV', 'source': 'donut_readme.md'}),\n",
       " Document(page_content='For Document Classification\\n\\nThe gt_parse follows the format of {\"class\" : {class_name}}, for example, {\"class\" : \"scientific_report\"} or {\"class\" : \"presentation\"}.\\n- Google colab demo is available here.\\n- Gradio web demo is available here.', metadata={'filename': 'donut_readme.md', 'filetype': 'text/markdown', 'last_modified': '2024-05-10T05:19:09', 'page_number': 1, 'orig_elements': 'eJzNkU1PwzAMhv+KlfMG6dqtdFcQ3DjtRlHlJm4XkTRVmjHQtP+OU4TEhzhw45bYz+uP1w8nQZYcDbExWmxBqGyzxrJrCfNKZkUrr2ReZOuiVDkWqtNiAcJRRI0RmT+Jzlga0FESaz8cYhMItaMLN7MpHV/HOR3pJV46DE/aH4eUtDj0B+xp4uyDoKEXj3N0io3z2nSG5plWclUs5XqZyZ1cb7NqK6ukHlnZDAfXUmAqOy/eOyTFrQ9w49UhLQbXXHDiYgqj8XPfj4F2JloSLPzhAm02paqq1apoS1SkrrqyK3TeUp7JYqP+rwtzJPzhnp9t2+0J+thwgYmg89b64wRxn97BYQTfwakWKvlZC+C952eTFj9zHaaAXtCNlhZfwVpMyvBQ6QrszOhDrMUZmP+GjYEm5uZLMXFRD0u48763BMpbbEGT82AmwGc0/Of4ngK9cwG18XCkX6nPt7/HELjNM+3S8ufHNwus+O4=', 'source': 'donut_readme.md'}),\n",
       " Document(page_content='For Document Information Extraction\\n\\nThe gt_parse is a JSON object that contains full information of the document image, for example, the JSON object for a receipt may look like {\"menu\" : [{\"nm\": \"ICE BLACKCOFFEE\", \"cnt\": \"2\", ...}, ...], ...}.\\n- More examples are available at CORD dataset.\\n- Google colab demo is available here.\\n- Gradio web demo is available here.\\n\\nFor Document Visual Question Answering', metadata={'filename': 'donut_readme.md', 'filetype': 'text/markdown', 'last_modified': '2024-05-10T05:19:09', 'page_number': 1, 'orig_elements': 'eJzVU02P2jAQ/SujnIHmi5Bw27JQbT92tS3qZYPQxJ6AS2wjx1lYof3vtYOoUD8OvbUXa8bvzfj5efx0CqghScquBQ+mEGS8mEziMGGYRwULJzzJsZhQFibhuKgwDAYQSLLI0aLjn4JaNKRQki/mWnV2bQi5pJHknuth+7LvYUtH+0ai2XF9UB5sUG063FDr0KeA1CZY9butXUvNRS2o1xSHcToMx8MoXIbjaVRMw8JX713lWnWyIuNY0evgfIKvWGgDt5p1/mJwp2ptJFqhFcyP1iDzoe9wEbYUtqHANfjZjSjEKkvSusqR5RUr0qxAPk7TuMhZznn677rR75i/eNdr+5Zbgo1duwYtgWgB4f2Xh3vQ1TdiFuwWLTCtLArVQt01DYgri3XtGAT8Yr+QTtkAHAHoiHLfuMQTrlt6EMEQI7G3IPEFGq130Igdwal0FquuDMDZ4hIlS6exDO5mc3j78Wb2YfawWMznpTOhDJiyZzT2+Wg0eu3X1TkelWoIn7ShixJ3NZfgM4oGq8ZFFmYPn2/Bv2dLtue/03rjIKYdBThJ3Tvyo2RLhs48g1xoONAfWdcjd4/GOL+eaek9/83osbTOGSU8Z6zmVVpPqjjNGMuqmpI8i/7Hj/hVtB028NhR20/KjWoPZIQ77de/uPoOselz1A==', 'source': 'donut_readme.md'}),\n",
       " Document(page_content='The gt_parses follows the format of [{\"question\" : {question_sentence}, \"answer\" : {answer_candidate_1}}, {\"question\" : {question_sentence}, \"answer\" : {answer_candidate_2}}, ...], for example, [{\"question\" : \"what is the model name?\", \"answer\" : \"donut\"}, {\"question\" : \"what is the model name?\", \"answer\" : \"document understanding transformer\"}].\\n- DocVQA Task1 has multiple answers, hence gt_parses should be a list of dictionary that contains a pair of question and answer.', metadata={'filename': 'donut_readme.md', 'filetype': 'text/markdown', 'last_modified': '2024-05-10T05:19:09', 'page_number': 1, 'orig_elements': 'eJytUk2L2zAQ/StC5yS1Ha/r5LIUCr0VCqGXOJixNLbF6iOV5M0uIf+9I7uhSz8Ohd6kmTdv3puZ45WjRoM2tkryPeN9BwLKAna17IusqjuRl1hWeV1VEmW25SvGDUaQEIHwV94rjRYMpmLp7BRbjyANboxM2JSOr+c5HfElvjPgn6S72JTUYIcJBgyUPXK0Az/N0RBb46TqFc6aiqwo19nDOs8O2cM+3+2zXao+U2VrJ9OhJ1Q+R/xPJ6Lsa4FbWQvRy67s33dFWQlRdT1u6yrP+G21SErgw4hsiC0RBAysd1q7S2CRor3zBiJzPTteG/5twhCVsw1nZP7+awO1RSuQKBsONlzQL4jl3QqwUtHIsM1vhPkfREUi2mw2NDGSyPAFzFnj6neVDb+MZEAtdmiuqFla2GPDf2nSLAts+B8k/huLmNJJsclK9CEmzXZg0RMqjTMBb6dNY9fsoxNfv3xgBwhPORshMDPpqMgIWxjDio1pHm+WE0Y3ack6gjCtwrwaqURSCv6V5JFM4WwEZQNBzqB8gtzdELH8QT4r+OTcQO2E09AxicYlj/AMiv4UH9HjgvMglWMX/Csq3eT91D+D9xDVMx7Sgd1O3wH5DCbB', 'source': 'donut_readme.md'}),\n",
       " Document(page_content='- Google colab demo is available here.\\n- Gradio web demo is available here.', metadata={'filename': 'donut_readme.md', 'filetype': 'text/markdown', 'last_modified': '2024-05-10T05:19:09', 'page_number': 1, 'orig_elements': 'eJytUk2L2zAQ/StC5yS1Ha/r5LIUCr0VCqGXOJixNLbF6iOV5M0uIf+9I7uhSz8Ohd6kmTdv3puZ45WjRoM2tkryPeN9BwLKAna17IusqjuRl1hWeV1VEmW25SvGDUaQEIHwV94rjRYMpmLp7BRbjyANboxM2JSOr+c5HfElvjPgn6S72JTUYIcJBgyUPXK0Az/N0RBb46TqFc6aiqwo19nDOs8O2cM+3+2zXao+U2VrJ9OhJ1Q+R/xPJ6Lsa4FbWQvRy67s33dFWQlRdT1u6yrP+G21SErgw4hsiC0RBAysd1q7S2CRor3zBiJzPTteG/5twhCVsw1nZP7+awO1RSuQKBsONlzQL4jl3QqwUtHIsM1vhPkfREUi2mw2NDGSyPAFzFnj6neVDb+MZEAtdmiuqFla2GPDf2nSLAts+B8k/huLmNJJsclK9CEmzXZg0RMqjTMBb6dNY9fsoxNfv3xgBwhPORshMDPpqMgIWxjDio1pHm+WE0Y3ack6gjCtwrwaqURSCv6V5JFM4WwEZQNBzqB8gtzdELH8QT4r+OTcQO2E09AxicYlj/AMiv4UH9HjgvMglWMX/Csq3eT91D+D9xDVMx7Sgd1O3wH5DCbB', 'is_continuation': True, 'source': 'donut_readme.md'}),\n",
       " Document(page_content='For (Pseudo) Text Reading Task\\n\\nThe gt_parse looks like {\"text_sequence\" : \"word1 word2 word3 ... \"}\\n- This task is also a pre-training task of Donut model.\\n- You can use our SynthDoG 🐶 to generate synthetic images for the text reading task with proper gt_parse. See ./synthdog/README.md for details.\\n\\nTraining\\n\\nThis is the configuration of Donut model training on CORD dataset used in our experiment. \\nWe ran this with a single NVIDIA A100 GPU.', metadata={'filename': 'donut_readme.md', 'filetype': 'text/markdown', 'last_modified': '2024-05-10T05:19:09', 'page_number': 1, 'orig_elements': 'eJzNk0tP3DAQx7/KKKdWYoPjPHazN9SliEMpgm2rikUrJx5nrU3sreMU0Irv3nFKEaCqjxsXW5qHZ/yf31ztI2yxQ+PXWkZziOqSV1meFWlWKsZTxqYMy1ql02mZJVmqogOIOvRCCi8ofh8p3aIRHYZkac3g1w6F7DDuZIgNbn+3G90eb/1hJ9xW2hsTnK0wzSAa7Ml7FaFpouvR2vt1Z6VWGseeOOPZhOWThC1ZPk/KOStD9o4y12boKnQUldwf/KwQMt5bB2/OexykfQtLMsIFNaVNA0vRb0Pyr56W2rcYUe5LIcosU3w2KzgrlKhyhlLKaVXmeVmohKf16xVitLj/GOlT5ZYbhMav6YEeobV220Ortwj71Riy7vHbgKbGVQRzWEU31skEwsnHM4U4jsl+vzITWG50D54EB7pF21sQsHM48U5oE4Yx+qyCRZAL6KfYxiHxqx2gFgYG6sEODi7vjN8s7AmsBjlLJZ11WoC30KBBJzxCHyLQ6xp0F3QERQSQBULT4B6GP9a70X5DbdgdusefxnCJCPHh+Iy0zeHF8dHiwzGNbnxI0px128dPwTkTjirr7xjw+h1AvJplRZ7jTFSYTZlQU15IxJQnaaayLH+9AD3j4WFW/7QzqiqTokaeT7lUQlZZWcik5jXnlRK0Sq/3yy935q/De74zxHdAnXirrVG6GQIa1rxgGx7BJ9e7jxcLCEL06APoErQZYcdbIlMHVWmRzBcER5vgQ4mRXAE9vdAinH0+XZwewVHCGJycf/oTnNc/AHg62+Q=', 'source': 'donut_readme.md'}),\n",
       " Document(page_content='bash\\npython train.py --config config/train_cord.yaml \\\\\\n                --pretrained_model_name_or_path \"naver-clova-ix/donut-base\" \\\\\\n                --dataset_name_or_paths \\'[\"naver-clova-ix/cord-v2\"]\\' \\\\\\n                --exp_version \"test_experiment\"    \\n  .\\n  .', metadata={'filename': 'donut_readme.md', 'filetype': 'text/markdown', 'last_modified': '2024-05-10T05:19:09', 'page_number': 1, 'orig_elements': 'eJztVltvmzAU/isW0tZNCmBuSYjaSF3Xrg9VVa15CxFy4JBYBcOMSRO1/e+zSXOjyrJWkzZVQwJ8Lt8XnwvOGT5okEIGTIQ01npII20nseOOG3cxBt+1SJLYVmIliZP4CY6w1kJaBoLERBDp/6AlNAVGMlDgOGeVCDmQOAMji5WvMotFUZsFzIWZEX4X5/dMGVPCJhWZQCmtQw3YRBvV2lKEWR7ThEK9Jxvbro493cID7PUsv4d9hS4kMmRVNgYuvaxawzeR2OOu2/Y86JIxuB1Mko7djgEc23LcxHU97am13JJyHpNyGrBiIaY5Q4ITyoxigXQ9yllCJ2j5MmtDGOU8NhYkS1EQBAw1Ll0vONSOEKsoIA1VesKchwURUxRojMyA61Gaz4hO52adNF1uAAJtD6NKdglih6hER8MXXGpr+swOtNHRHiqYF6GElFTGGcjwZaqlCjhVPSA3IC8FM+pHE/5ur4DdcIhpJGRaeui4DGU2qr58s6x/BZnM1QAI+nT1+disddISMdG3lKgWUi44jaBvewbGWKmXslqtuEQuSLpevABsaxW/7Mil4OCVy0anHKby64GluCbZVq5J+wE7ZeU98HcZ2nXOM4jR+dcekmy/quRlld6hQV7Idkc3JLqT58ehoC2MXxn1FmJv2Fs+e+N+QznfT3y/X9NvlDCBbn9UNN6NdY5eRvu9MJDjNwIuw7IaP7OdGReU0XJK2QTp6KwSG84Nww4YCvMZ+8W4LWi0QFcwg1Siz+eCy39WdJkL1ISZqx/dwE+Ni5TM8opL6C1JBfqIbkCV8tAWVlxbhZPypnhS2C7Ubg4axmdVo/LNFtil2NsFys073AjKzbLedoDtr/6/U/xXsKx74H8LHDwUHL+9c7t/e4ZQU8R5kUdTZPtyHMX4w2NQ2V63+6eej8jG2JQ3GmKr5/rHGPcwbiFkGV2bCrNsoTQvyxOVb8futJCa9dTIeLKZW5fK5wHwpDH+jdRUvRrWrwnnRNAZDNSI/DT6CTdpxXY=', 'source': 'donut_readme.md'}),\n",
       " Document(page_content='Prediction: <s_menu><s_nm>Lemon Tea (L)</s_nm><s_cnt>1</s_cnt><s_price>25.000</s_price></s_menu><s_total><s_total_price>25.000</s_total_price><s_cashprice>30.000</s_cashprice><s_changeprice>5.000</s_changeprice></s_total>\\nAnswer: <s_menu><s_nm>Lemon Tea (L)</s_nm><s_cnt>1</s_cnt><s_price>25.000</s_price></s_menu><s_total><s_total_price>25.000</s_total_price><s_cashprice>30.000</s_cashprice><s_changeprice>5.000</s_changeprice></s_total>\\nNormed ED: 0.0', metadata={'filename': 'donut_readme.md', 'filetype': 'text/markdown', 'last_modified': '2024-05-10T05:19:09', 'page_number': 1, 'orig_elements': 'eJztVltvmzAU/isW0tZNCmBuSYjaSF3Xrg9VVa15CxFy4JBYBcOMSRO1/e+zSXOjyrJWkzZVQwJ8Lt8XnwvOGT5okEIGTIQ01npII20nseOOG3cxBt+1SJLYVmIliZP4CY6w1kJaBoLERBDp/6AlNAVGMlDgOGeVCDmQOAMji5WvMotFUZsFzIWZEX4X5/dMGVPCJhWZQCmtQw3YRBvV2lKEWR7ThEK9Jxvbro493cID7PUsv4d9hS4kMmRVNgYuvaxawzeR2OOu2/Y86JIxuB1Mko7djgEc23LcxHU97am13JJyHpNyGrBiIaY5Q4ITyoxigXQ9yllCJ2j5MmtDGOU8NhYkS1EQBAw1Ll0vONSOEKsoIA1VesKchwURUxRojMyA61Gaz4hO52adNF1uAAJtD6NKdglih6hER8MXXGpr+swOtNHRHiqYF6GElFTGGcjwZaqlCjhVPSA3IC8FM+pHE/5ur4DdcIhpJGRaeui4DGU2qr58s6x/BZnM1QAI+nT1+disddISMdG3lKgWUi44jaBvewbGWKmXslqtuEQuSLpevABsaxW/7Mil4OCVy0anHKby64GluCbZVq5J+wE7ZeU98HcZ2nXOM4jR+dcekmy/quRlld6hQV7Idkc3JLqT58ehoC2MXxn1FmJv2Fs+e+N+QznfT3y/X9NvlDCBbn9UNN6NdY5eRvu9MJDjNwIuw7IaP7OdGReU0XJK2QTp6KwSG84Nww4YCvMZ+8W4LWi0QFcwg1Siz+eCy39WdJkL1ISZqx/dwE+Ni5TM8opL6C1JBfqIbkCV8tAWVlxbhZPypnhS2C7Ubg4axmdVo/LNFtil2NsFys073AjKzbLedoDtr/6/U/xXsKx74H8LHDwUHL+9c7t/e4ZQU8R5kUdTZPtyHMX4w2NQ2V63+6eej8jG2JQ3GmKr5/rHGPcwbiFkGV2bCrNsoTQvyxOVb8futJCa9dTIeLKZW5fK5wHwpDH+jdRUvRrWrwnnRNAZDNSI/DT6CTdpxXY=', 'is_continuation': True, 'source': 'donut_readme.md'}),\n",
       " Document(page_content='Prediction: <s_menu><s_nm>Hulk Topper Package</s_nm><s_cnt>1</s_cnt><s_price>100.000</s_price></s_menu><s_total><s_total_price>100.000</s_total_price><s_cashprice>100.000</s_cashprice><s_changeprice>0</s_changeprice></s_total>\\nAnswer: <s_menu><s_nm>Hulk Topper Package</s_nm><s_cnt>1</s_cnt><s_price>100.000</s_price></s_menu><s_total><s_total_price>100.000</s_total_price><s_cashprice>100.000</s_cashprice><s_changeprice>0</s_changeprice></s_total>\\nNormed ED: 0.0', metadata={'filename': 'donut_readme.md', 'filetype': 'text/markdown', 'last_modified': '2024-05-10T05:19:09', 'page_number': 1, 'orig_elements': 'eJztVltvmzAU/isW0tZNCmBuSYjaSF3Xrg9VVa15CxFy4JBYBcOMSRO1/e+zSXOjyrJWkzZVQwJ8Lt8XnwvOGT5okEIGTIQ01npII20nseOOG3cxBt+1SJLYVmIliZP4CY6w1kJaBoLERBDp/6AlNAVGMlDgOGeVCDmQOAMji5WvMotFUZsFzIWZEX4X5/dMGVPCJhWZQCmtQw3YRBvV2lKEWR7ThEK9Jxvbro493cID7PUsv4d9hS4kMmRVNgYuvaxawzeR2OOu2/Y86JIxuB1Mko7djgEc23LcxHU97am13JJyHpNyGrBiIaY5Q4ITyoxigXQ9yllCJ2j5MmtDGOU8NhYkS1EQBAw1Ll0vONSOEKsoIA1VesKchwURUxRojMyA61Gaz4hO52adNF1uAAJtD6NKdglih6hER8MXXGpr+swOtNHRHiqYF6GElFTGGcjwZaqlCjhVPSA3IC8FM+pHE/5ur4DdcIhpJGRaeui4DGU2qr58s6x/BZnM1QAI+nT1+disddISMdG3lKgWUi44jaBvewbGWKmXslqtuEQuSLpevABsaxW/7Mil4OCVy0anHKby64GluCbZVq5J+wE7ZeU98HcZ2nXOM4jR+dcekmy/quRlld6hQV7Idkc3JLqT58ehoC2MXxn1FmJv2Fs+e+N+QznfT3y/X9NvlDCBbn9UNN6NdY5eRvu9MJDjNwIuw7IaP7OdGReU0XJK2QTp6KwSG84Nww4YCvMZ+8W4LWi0QFcwg1Siz+eCy39WdJkL1ISZqx/dwE+Ni5TM8opL6C1JBfqIbkCV8tAWVlxbhZPypnhS2C7Ubg4axmdVo/LNFtil2NsFys073AjKzbLedoDtr/6/U/xXsKx74H8LHDwUHL+9c7t/e4ZQU8R5kUdTZPtyHMX4w2NQ2V63+6eej8jG2JQ3GmKr5/rHGPcwbiFkGV2bCrNsoTQvyxOVb8futJCa9dTIeLKZW5fK5wHwpDH+jdRUvRrWrwnnRNAZDNSI/DT6CTdpxXY=', 'is_continuation': True, 'source': 'donut_readme.md'}),\n",
       " Document(page_content='Prediction: <s_menu><s_nm>Giant Squid</s_nm><s_cnt>x 1</s_cnt><s_price>Rp. 39.000</s_price><s_sub><s_nm>C.Finishing - Cut</s_nm><s_price>Rp. 0</s_price><sep/><s_nm>B.Spicy Level - Extreme Hot Rp. 0</s_price></s_sub><sep/><s_nm>A.Flavour - Salt & Pepper</s_nm><s_price>Rp. 0</s_price></s_sub></s_menu><s_sub_total><s_subtotal_price>Rp. 39.000</s_subtotal_price></s_sub_total><s_total><s_total_price>Rp. 39.000</s_total_price><s_cashprice>Rp. 50.000</s_cashprice><s_changeprice>Rp.', metadata={'filename': 'donut_readme.md', 'filetype': 'text/markdown', 'last_modified': '2024-05-10T05:19:09', 'page_number': 1, 'orig_elements': 'eJztVltvmzAU/isW0tZNCmBuSYjaSF3Xrg9VVa15CxFy4JBYBcOMSRO1/e+zSXOjyrJWkzZVQwJ8Lt8XnwvOGT5okEIGTIQ01npII20nseOOG3cxBt+1SJLYVmIliZP4CY6w1kJaBoLERBDp/6AlNAVGMlDgOGeVCDmQOAMji5WvMotFUZsFzIWZEX4X5/dMGVPCJhWZQCmtQw3YRBvV2lKEWR7ThEK9Jxvbro493cID7PUsv4d9hS4kMmRVNgYuvaxawzeR2OOu2/Y86JIxuB1Mko7djgEc23LcxHU97am13JJyHpNyGrBiIaY5Q4ITyoxigXQ9yllCJ2j5MmtDGOU8NhYkS1EQBAw1Ll0vONSOEKsoIA1VesKchwURUxRojMyA61Gaz4hO52adNF1uAAJtD6NKdglih6hER8MXXGpr+swOtNHRHiqYF6GElFTGGcjwZaqlCjhVPSA3IC8FM+pHE/5ur4DdcIhpJGRaeui4DGU2qr58s6x/BZnM1QAI+nT1+disddISMdG3lKgWUi44jaBvewbGWKmXslqtuEQuSLpevABsaxW/7Mil4OCVy0anHKby64GluCbZVq5J+wE7ZeU98HcZ2nXOM4jR+dcekmy/quRlld6hQV7Idkc3JLqT58ehoC2MXxn1FmJv2Fs+e+N+QznfT3y/X9NvlDCBbn9UNN6NdY5eRvu9MJDjNwIuw7IaP7OdGReU0XJK2QTp6KwSG84Nww4YCvMZ+8W4LWi0QFcwg1Siz+eCy39WdJkL1ISZqx/dwE+Ni5TM8opL6C1JBfqIbkCV8tAWVlxbhZPypnhS2C7Ubg4axmdVo/LNFtil2NsFys073AjKzbLedoDtr/6/U/xXsKx74H8LHDwUHL+9c7t/e4ZQU8R5kUdTZPtyHMX4w2NQ2V63+6eej8jG2JQ3GmKr5/rHGPcwbiFkGV2bCrNsoTQvyxOVb8futJCa9dTIeLKZW5fK5wHwpDH+jdRUvRrWrwnnRNAZDNSI/DT6CTdpxXY=', 'is_continuation': True, 'source': 'donut_readme.md'}),\n",
       " Document(page_content='11.000</s_changeprice></s_total>', metadata={'filename': 'donut_readme.md', 'filetype': 'text/markdown', 'last_modified': '2024-05-10T05:19:09', 'page_number': 1, 'orig_elements': 'eJztVltvmzAU/isW0tZNCmBuSYjaSF3Xrg9VVa15CxFy4JBYBcOMSRO1/e+zSXOjyrJWkzZVQwJ8Lt8XnwvOGT5okEIGTIQ01npII20nseOOG3cxBt+1SJLYVmIliZP4CY6w1kJaBoLERBDp/6AlNAVGMlDgOGeVCDmQOAMji5WvMotFUZsFzIWZEX4X5/dMGVPCJhWZQCmtQw3YRBvV2lKEWR7ThEK9Jxvbro493cID7PUsv4d9hS4kMmRVNgYuvaxawzeR2OOu2/Y86JIxuB1Mko7djgEc23LcxHU97am13JJyHpNyGrBiIaY5Q4ITyoxigXQ9yllCJ2j5MmtDGOU8NhYkS1EQBAw1Ll0vONSOEKsoIA1VesKchwURUxRojMyA61Gaz4hO52adNF1uAAJtD6NKdglih6hER8MXXGpr+swOtNHRHiqYF6GElFTGGcjwZaqlCjhVPSA3IC8FM+pHE/5ur4DdcIhpJGRaeui4DGU2qr58s6x/BZnM1QAI+nT1+disddISMdG3lKgWUi44jaBvewbGWKmXslqtuEQuSLpevABsaxW/7Mil4OCVy0anHKby64GluCbZVq5J+wE7ZeU98HcZ2nXOM4jR+dcekmy/quRlld6hQV7Idkc3JLqT58ehoC2MXxn1FmJv2Fs+e+N+QznfT3y/X9NvlDCBbn9UNN6NdY5eRvu9MJDjNwIuw7IaP7OdGReU0XJK2QTp6KwSG84Nww4YCvMZ+8W4LWi0QFcwg1Siz+eCy39WdJkL1ISZqx/dwE+Ni5TM8opL6C1JBfqIbkCV8tAWVlxbhZPypnhS2C7Ubg4axmdVo/LNFtil2NsFys073AjKzbLedoDtr/6/U/xXsKx74H8LHDwUHL+9c7t/e4ZQU8R5kUdTZPtyHMX4w2NQ2V63+6eej8jG2JQ3GmKr5/rHGPcwbiFkGV2bCrNsoTQvyxOVb8futJCa9dTIeLKZW5fK5wHwpDH+jdRUvRrWrwnnRNAZDNSI/DT6CTdpxXY=', 'is_continuation': True, 'source': 'donut_readme.md'}),\n",
       " Document(page_content='Answer: <s_menu><s_nm>Giant Squid</s_nm><s_cnt>x1</s_cnt><s_price>Rp. 39.000</s_price><s_sub><s_nm>C.Finishing - Cut</s_nm><s_price>Rp. 0</s_price><sep/><s_nm>B.Spicy Level - Extreme Hot</s_nm><s_price>Rp. 0</s_price><sep/><s_nm>A.Flavour- Salt & Pepper</s_nm><s_price>Rp. 0</s_price></s_sub></s_menu><s_sub_total><s_subtotal_price>Rp. 39.000</s_subtotal_price></s_sub_total><s_total><s_total_price>Rp. 39.000</s_total_price><s_cashprice>Rp. 50.000</s_cashprice><s_changeprice>Rp.', metadata={'filename': 'donut_readme.md', 'filetype': 'text/markdown', 'last_modified': '2024-05-10T05:19:09', 'page_number': 1, 'orig_elements': 'eJztVltvmzAU/isW0tZNCmBuSYjaSF3Xrg9VVa15CxFy4JBYBcOMSRO1/e+zSXOjyrJWkzZVQwJ8Lt8XnwvOGT5okEIGTIQ01npII20nseOOG3cxBt+1SJLYVmIliZP4CY6w1kJaBoLERBDp/6AlNAVGMlDgOGeVCDmQOAMji5WvMotFUZsFzIWZEX4X5/dMGVPCJhWZQCmtQw3YRBvV2lKEWR7ThEK9Jxvbro493cID7PUsv4d9hS4kMmRVNgYuvaxawzeR2OOu2/Y86JIxuB1Mko7djgEc23LcxHU97am13JJyHpNyGrBiIaY5Q4ITyoxigXQ9yllCJ2j5MmtDGOU8NhYkS1EQBAw1Ll0vONSOEKsoIA1VesKchwURUxRojMyA61Gaz4hO52adNF1uAAJtD6NKdglih6hER8MXXGpr+swOtNHRHiqYF6GElFTGGcjwZaqlCjhVPSA3IC8FM+pHE/5ur4DdcIhpJGRaeui4DGU2qr58s6x/BZnM1QAI+nT1+disddISMdG3lKgWUi44jaBvewbGWKmXslqtuEQuSLpevABsaxW/7Mil4OCVy0anHKby64GluCbZVq5J+wE7ZeU98HcZ2nXOM4jR+dcekmy/quRlld6hQV7Idkc3JLqT58ehoC2MXxn1FmJv2Fs+e+N+QznfT3y/X9NvlDCBbn9UNN6NdY5eRvu9MJDjNwIuw7IaP7OdGReU0XJK2QTp6KwSG84Nww4YCvMZ+8W4LWi0QFcwg1Siz+eCy39WdJkL1ISZqx/dwE+Ni5TM8opL6C1JBfqIbkCV8tAWVlxbhZPypnhS2C7Ubg4axmdVo/LNFtil2NsFys073AjKzbLedoDtr/6/U/xXsKx74H8LHDwUHL+9c7t/e4ZQU8R5kUdTZPtyHMX4w2NQ2V63+6eej8jG2JQ3GmKr5/rHGPcwbiFkGV2bCrNsoTQvyxOVb8futJCa9dTIeLKZW5fK5wHwpDH+jdRUvRrWrwnnRNAZDNSI/DT6CTdpxXY=', 'is_continuation': True, 'source': 'donut_readme.md'}),\n",
       " Document(page_content='11.000</s_changeprice></s_total>\\nNormed ED: 0.039603960396039604                                                                                                                                  \\nEpoch 29: 100%|█████████████| 200/200 [01:49<00:00,  1.82it/s, loss=0.00327, exp_name=train_cord, exp_version=test_experiment]', metadata={'filename': 'donut_readme.md', 'filetype': 'text/markdown', 'last_modified': '2024-05-10T05:19:09', 'page_number': 1, 'orig_elements': 'eJztVltvmzAU/isW0tZNCmBuSYjaSF3Xrg9VVa15CxFy4JBYBcOMSRO1/e+zSXOjyrJWkzZVQwJ8Lt8XnwvOGT5okEIGTIQ01npII20nseOOG3cxBt+1SJLYVmIliZP4CY6w1kJaBoLERBDp/6AlNAVGMlDgOGeVCDmQOAMji5WvMotFUZsFzIWZEX4X5/dMGVPCJhWZQCmtQw3YRBvV2lKEWR7ThEK9Jxvbro493cID7PUsv4d9hS4kMmRVNgYuvaxawzeR2OOu2/Y86JIxuB1Mko7djgEc23LcxHU97am13JJyHpNyGrBiIaY5Q4ITyoxigXQ9yllCJ2j5MmtDGOU8NhYkS1EQBAw1Ll0vONSOEKsoIA1VesKchwURUxRojMyA61Gaz4hO52adNF1uAAJtD6NKdglih6hER8MXXGpr+swOtNHRHiqYF6GElFTGGcjwZaqlCjhVPSA3IC8FM+pHE/5ur4DdcIhpJGRaeui4DGU2qr58s6x/BZnM1QAI+nT1+disddISMdG3lKgWUi44jaBvewbGWKmXslqtuEQuSLpevABsaxW/7Mil4OCVy0anHKby64GluCbZVq5J+wE7ZeU98HcZ2nXOM4jR+dcekmy/quRlld6hQV7Idkc3JLqT58ehoC2MXxn1FmJv2Fs+e+N+QznfT3y/X9NvlDCBbn9UNN6NdY5eRvu9MJDjNwIuw7IaP7OdGReU0XJK2QTp6KwSG84Nww4YCvMZ+8W4LWi0QFcwg1Siz+eCy39WdJkL1ISZqx/dwE+Ni5TM8opL6C1JBfqIbkCV8tAWVlxbhZPypnhS2C7Ubg4axmdVo/LNFtil2NsFys073AjKzbLedoDtr/6/U/xXsKx74H8LHDwUHL+9c7t/e4ZQU8R5kUdTZPtyHMX4w2NQ2V63+6eej8jG2JQ3GmKr5/rHGPcwbiFkGV2bCrNsoTQvyxOVb8futJCa9dTIeLKZW5fK5wHwpDH+jdRUvRrWrwnnRNAZDNSI/DT6CTdpxXY=', 'is_continuation': True, 'source': 'donut_readme.md'}),\n",
       " Document(page_content='Some important arguments:\\n\\n--config : config file path for model training.\\n\\n--pretrained_model_name_or_path : string format, model name in Hugging Face modelhub or local path.\\n\\n--dataset_name_or_paths : string format (json dumped), list of dataset names in Hugging Face datasets or local paths.\\n\\n--result_path : file path to save model outputs/artifacts.\\n\\n--exp_version : used for experiment versioning. The output files are saved at {result_path}/{exp_version}/*', metadata={'filename': 'donut_readme.md', 'filetype': 'text/markdown', 'last_modified': '2024-05-10T05:19:09', 'page_number': 1, 'orig_elements': 'eJzdlctu3CAUhl8FedVWMxmML9h+gKqVumt2UWQd4OCh9U2A01SjvHvBcdTJZZHuqtlZ/Oe3D99/wDenBHsccPStUUlDkhqrQtBC10oLqSuJIteFFILLGrTikOxIMqAHBR5C/SnRpscRBoxmNY2Lby2CGvBqULE2yv73vMoe7/1hAPtTTb/GKPYwdgt06IJ6k+DYJbfrqvPtMCmjDa49McryPS32Kb2mRZPWDa2jew7OdlwGgTZUpQ+7xy9Ex/dpQGKGebIeRk/Adkvco2ui76mda+N7TILtJYOiyFLOMoQsY5wXTHAhRVHVac1SKJH9vwzWFfsPaZ5D2+/lNGrTkYZsD7FxMoM/Ej1ZEtrBnngLZjRjd3WO8ptx/qvH4S2aJVCeakxlzkrByyrjigWOOktLXVNaXSzN2eIKC1W7omvjptrJtivRhjhvA8eIdgC/2/DGGmJG8mXpuqh+BomP0nERJKTQTxL6NZR3J5BSpLpUkrGCa9C0lhwqzFieCZBlVlxsAnFDDv0z8O4lefLhh5tGopZhRvVxR/pAkkyabOY1Efcqkk11zyNx785EpynD0DUvNUqWSZFpmVEoKuBIVVZfbCYW3dL7pzPw94bxE3Fwt806mRY/L94dwHqjQfr3g61kRaEqNc/LgLUKky9KXqa5KHKUgl3usOP93N6hdSYMc0MWh2q9tcMyWhMBkU2Nlze5PuIGec3AhX8krgEoEo7E6Sylh8Pp7NUPh09vJ3H7B7T/ouM=', 'source': 'donut_readme.md'}),\n",
       " Document(page_content='Test\\n\\nWith the trained model, test images and ground truth parses, you can get inference results and accuracy scores.\\n\\nbash\\npython test.py --dataset_name_or_path naver-clova-ix/cord-v2 --pretrained_model_name_or_path ./result/train_cord/test_experiment --save_path ./result/output.json\\n100%|█████████████| 100/100 [00:35<00:00,  2.80it/s]\\nTotal number of samples: 100, Tree Edit Distance (TED) based accuracy score: 0.9129639764131697, F1 accuracy score: 0.8406020841373987', metadata={'filename': 'donut_readme.md', 'filetype': 'text/markdown', 'last_modified': '2024-05-10T05:19:09', 'page_number': 1, 'orig_elements': 'eJzdkk1r3DAQhv/KYCi04A9ZXn8tPSY99mToIRvM2BrvqrVlI8nbLEn/eyVvC2kIhUIPpQfLoHlezcc7d48BjTSRsq0UwR6CVAyYcd6hYMOO8oGwo2woWVdXlBXVLgghmMiiQIuOfwwGOZLCibxYzGq1rSYUE8WT8KwP28uyhS092GRC/UXMX5UPjqiOKx7JuOhdQOoY3G+3xrbTLOQgaauJM76LWB6lrGH5Pq33rPbqxSlbtU4daUel38JrBq9oyFiP/MzcSDtS4IiX7XYV61jPi67oK5HWJHhe5Oi+sqwLLrp/t93tRv+Bcc/n80naE9gTgdUoFQlw+WkMwbq5gZx8jYBKwFHPq/tZvTrepTNkQrjMK/So4EiOVQO5InoCTWYd7VWGfb9q7C9g+tndx8+t+Ihao5Vnanwtr1jCBO/LosoFR1F2LCv6YRCM93WR5V22o//Ukg7N6aCWiz3NavMhXi4QRb5LQ7b1Dbazbhd0Tig8k476cT5jJB8SN2QRnbmjF00/HG03R3+VxcnVpGRjWi9LfKaWHhbS0nvg3jDu8Rf8vNpltfFnM6uDShl783RYeV5Vf+t8Avdo4j64Y2yf5e/dyVgIwOOKSZuY+4NqZosjXIcN8wAGp2Uks/fSEBpNBLdCWriRxqJfyLfN7c07cGOllwu5BxbXKXcLVZfFLs3Soi5D+JC+glU7VjDOKkeVWV2Vv9nk+++5wasb', 'source': 'donut_readme.md'}),\n",
       " Document(page_content='Some important arguments:\\n\\n--dataset_name_or_path : string format, the target dataset name in Hugging Face datasets or local path.\\n\\n--pretrained_model_name_or_path : string format, the model name in Hugging Face modelhub or local path.\\n\\n--save_path: file path to save predictions and scores.', metadata={'filename': 'donut_readme.md', 'filetype': 'text/markdown', 'last_modified': '2024-05-10T05:19:09', 'page_number': 1, 'orig_elements': 'eJzdk81q3TAQhV9l8PreVLLlH/kBSgvdNbsQzNga+4pa0kUa94fQd6/lNhBKCk1XJds551gz58N3DwWt5MjzYE3RQ0FlJ6aKmlELlCgnLcdu7Jq2q8pOlUoVJygcMRpk3P0PxWxX8ugoh03wGw+R0Di6cSZ7s8zfrofM9JXfOIyfTPjis7iiXzZcKO3qXUF+Ke6PaeLBBWNnS8dOpSjVWdRnKW5F3UvdC53T1z05+M2NFHeX/H76+UJOfAyOwLpriIyeAeOy5RtTn3OP69xaXqnYY793gFVTt6pSs5yrRpflrI2SnRAlCWpIyP+3g2MSX0DzaWnncz4oEQ/5liHE4Yp8gR4SR+sXmEN0yCfgCwHvlRLDrwDkAFgP77Zlyda3ONGjmCBEWMOEK+Tv3TxF8MEmfs/knqPQalV1ErtRVajbGuvZ0NhU3US6mWp6vRSukTii9WTy27T+BY7D9zyFQ7ps4z9SUGbUSrf79g02LYl5lPtfIVrUQkvZvl4KCT/T0XgPeeejNeAAeQ47ImMntsEnQG8gTSFS+kOp9z8AstCtNA==', 'source': 'donut_readme.md'}),\n",
       " Document(page_content='How to Cite\\n\\nIf you find this work useful to you, please cite:\\nbibtex\\n@inproceedings{kim2022donut,\\n  title     = {OCR-Free Document Understanding Transformer},\\n  author    = {Kim, Geewook and Hong, Teakgyu and Yim, Moonbin and Nam, JeongYeon and Park, Jinyoung and Yim, Jinyeong and Hwang, Wonseok and Yun, Sangdoo and Han, Dongyoon and Park, Seunghyun},\\n  booktitle = {European Conference on Computer Vision (ECCV)},\\n  year      = {2022}\\n}\\n\\nLicense\\n\\n```\\nMIT license', metadata={'filename': 'donut_readme.md', 'filetype': 'text/markdown', 'last_modified': '2024-05-10T05:19:09', 'page_number': 1, 'orig_elements': 'eJzVU+9v0zAQ/VesfAIpA+d3UwkJqRtswACxbmhap86xL5nV5lw5jkqp+r9zTgcqCCT4NvIhUt69d773zrnZBrCEFtDNtQrGLBhFRVmlWV1mqqqjMh0lQlaK5zlkIwk8CUIWtOCEEk4QfxvUegkoWvBiZbB3cwtCtfCsVZ7ry26zGsoOvrjnrbALZdboi0uBTS8a6Kh6EwA2we2Adm7eGqVrDcNMMY/TI54dRXzKs3FUjnnp1StSzrFvK7DEinbh/gSvODVr5gybaAee+X2AqXZLCIj4q2teQSyrVKkiSnOleFGkXI5kkvKoVGnJH6/rAbH/sL/DmM5qtjE9qzUq5u51x9bGLljfQd0vfX5UDNlqCaIDJinL8QwrXZF6hi81rqyRAEpj020XuqV54yGKcIaMOR81888Ltv0w+XT0ygKwYyN7Hzu7RAW2cwK9nE2twK42tgW7G9Sid/fGPqjf6jZkrwHWxiwYKdipwSZkUxCLZtMPyLWnnBtD4+EAvBcEvAEiXtNrgD7SCgjTSK7ozB8yj3jivvVa+N6fDXbwcNp1jyG7IFgZs+cIAo5JsTE/tb4A6nu/6XHvoaJx9ymQh5PemhUIZBODNdC+JDDjv9pV78CyK91p+n5yMplcPd3rNyCGBIYMfLi7Ge4Ob/MlSuGgMVZ/BTX1K/3NzS5FpYo6jxNZFbLI4ywuorJQZSJGeZIW0eO92YcX9Z2WQBv5q3+5zlUuSs6jIivIaDFKoiyXaVXWqeAcsv/D8d3d3QzPz6Zs+Ufrt98AsirPcQ==', 'source': 'donut_readme.md'}),\n",
       " Document(page_content='Copyright (c) 2022-present NAVER Corp.\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:', metadata={'filename': 'donut_readme.md', 'filetype': 'text/markdown', 'last_modified': '2024-05-10T05:19:09', 'page_number': 1, 'orig_elements': 'eJzNUk1v2zAM/SuETy3gtM6H7SS3odi1GLZgl6YIZImytdmSIcnzgqL/faSWoN1uu+0kgOR7fO9RTy8Z9jigjSejsj1km22j1kWtVutyWem60LWUldzUVaVFsS5llkM2YBRKREHzL5k2PVoxIIOVs1M8eRRqwLtB8Sy343lM7Yg/4/0g/HflZsvNXth2Ei0G6j5laNvsOVVDPA1OGW0waVoVq82iKBfL4lCU++VuX+wYPRLyZKehQU9Ty9f89wZGPLjx7E3bRbiRt0D41WL0GMgmPH74+vEzPDg/3jHJVdvBxB4z4vg7kGa3EWWzbTRW2/VOyV1R1dtiqYUqy7qU6v8NJFX8P5z2fYKf0A8mBOMsmAAdemzO0HphI6octEcEp0F2wreYQ3Qg7BlG9IEAronCWGNbECDpEkdLo7EjnuB0nEkTTSsQIThpBBGCcnLizEXkhRxRgJvYIRyzLxfIMbtNexSK/kjkwO1rE2YTOzdFoCNHbyTT5GCs7CfFOq7t3gzmsoTh6YuEoyXaKZANFptDSppfTN7GqelN6HJQhrmbKVIxcFGiZRR5uXceAvYkjCgMiU+G3/SlIVY/cqzxElTgyty54U8vhhTpyVtaigmkHAWXdn5DGbnC89r1vZvZnXRWGTYV9u+/9KPwnrz+wAPf9PX5F1S3TTQ=', 'source': 'donut_readme.md'}),\n",
       " Document(page_content='The above copyright notice and this permission notice shall be included in\\nall copies or substantial portions of the Software.', metadata={'filename': 'donut_readme.md', 'filetype': 'text/markdown', 'last_modified': '2024-05-10T05:19:09', 'page_number': 1, 'orig_elements': 'eJxFkM1uwyAQhF8FcW5S/O/kIXppbmlkrWGJUQ1YsE4aRXn3QqqoNzTfzGqG453jjBYdDUbxPeOy1H1XgmzqXYVd20I39oVuZVv20Iu64m+MWyRQQJD8d67NjA4s5rDybqUhICiLW6uyN2O6LU9M+EPvFsK38leX4QzuvMIZY6JHju7MT0810mC9Mtrgs1Mpynojmk0hDqLZF7u92OX0kpKDW+2IIbmKpxL+l9T9qCrRqbJqilZ3QndStrJOmzSIqpH88fZXKZsPEzIY/QWZ9MstmPNEzHkyMslOMZpMZAsGa2I03r1QnGCe2YjMODmvClV6fLmspSsGI/OBxXWMBI4MzGzxgVI86TqdRPbpNV1T522e8/qlDwgByFzwkLs9Tr/mb4de', 'source': 'donut_readme.md'}),\n",
       " Document(page_content='THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\\nTHE SOFTWARE.\\n```', metadata={'filename': 'donut_readme.md', 'filetype': 'text/markdown', 'last_modified': '2024-05-10T05:19:09', 'page_number': 1, 'orig_elements': 'eJxVUtuOmzAQ/ZURz2xKLuT25oAJVsFGxtk0WlaJAyaKGsgqS3rRav+946TdtpJfZuZc5th+enPMyTSm7bbHypmD41V7U030rF+PJr4upyPfn5amrqbjmR6agXFccBrT6Up3GvFvTn08mVY3xpKrc3vtthejq8b0mspi7bj7+XIbd+ZH96nRl6/V+XtrhyfdHq76YF5x+uSY9uA837qv3bY5V8f6aG47DbzB6MHzH/qe8vx5fzb3Zpb9gsxte2325oKo/q1z+ZtkNN1XQ29SDYZ+f1xPvHpSluNyNBmPa+0N/dJ5d+8rWbCKKeQiUmsiKbAcMikeWUhDKBySY6NAwzVTsVgpQIwkXG1ARED4Bj4zHrpAv2SS5jkIWbQszRJGscl4kKxCxpewQCIXChKWMoW6SoD1/K3FaG7VUiqDGEuyYAlTG7doI6a4VY2EBAIZkYoFq4RIyFYyEznFBULU5YxHEm1oSrnqAfpiE+gjVpDHJEmsWdGSFSaQdkcIRLaRbBkriEUSUmwuKC5HFgm9m2GwICEsdSEkKVnSG0ugDOazuPuGsI6p7VlHgidQTHAbJRBcSSxdTCrVB3fNcuoCkSy3lxJJkWJIe6lIETcVJHJ6l7EXDv+9DEJsvcrphyKElCQohq/Ei/ZfdK9od7ud/Sl/PiDXl4vujt+Mss/+/vwLX5bZww==', 'source': 'donut_readme.md'})]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d0dcc9a3-a437-43a3-b62f-01f5763ee537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "vstore = Chroma.from_documents(documents,embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7103577d-fefe-421b-ab25-476a51bb7b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 6}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "03853e80-2173-4161-9e3a-71dca8708ff0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x3089484d0>, search_kwargs={'k': 6})"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cc70ce84-3b51-479a-83fc-0ab77598625e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain_openai import OpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain, LLMChain\n",
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "10defdac-4c49-41fa-98b3-a64de3ac441a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['context', 'question'], template='You are an AI assistant for answering questions about the Donut document understanding model.\\nYou are given the following extracted parts of a long document and a question. Provide a conversational answer.\\nIf you don\\'t know the answer, just say \"Hmm, I\\'m not sure.\" Don\\'t try to make up an answer.\\nIf the question is not about Donut, politely inform them that you are tuned to only answer questions about Donut.\\nQuestion: {question}\\n=========\\n{context}\\n=========\\nAnswer in Markdown:')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"\"\"You are an AI assistant for answering questions about the Donut document understanding model.\n",
    "You are given the following extracted parts of a long document and a question. Provide a conversational answer.\n",
    "If you don't know the answer, just say \"Hmm, I'm not sure.\" Don't try to make up an answer.\n",
    "If the question is not about Donut, politely inform them that you are tuned to only answer questions about Donut.\n",
    "Question: {question}\n",
    "=========\n",
    "{context}\n",
    "=========\n",
    "Answer in Markdown:\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\", \"context\"])\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b4d3b9ad-d692-453b-a0a8-2e1c728888b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "doc_chain = load_qa_with_sources_chain(llm, chain_type=\"map_reduce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ce6d5a92-e084-40d9-b020-632c4ad59171",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapReduceDocumentsChain(llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['context', 'question'], template='Use the following portion of a long document to see if any of the text is relevant to answer the question. \\nReturn any relevant text verbatim.\\n{context}\\nQuestion: {question}\\nRelevant text, if any:'), llm=OpenAI(client=<openai.resources.completions.Completions object at 0x308d2c110>, async_client=<openai.resources.completions.AsyncCompletions object at 0x30783e8d0>, temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='')), reduce_documents_chain=ReduceDocumentsChain(combine_documents_chain=StuffDocumentsChain(llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['question', 'summaries'], template='Given the following extracted parts of a long document and a question, create a final answer with references (\"SOURCES\"). \\nIf you don\\'t know the answer, just say that you don\\'t know. Don\\'t try to make up an answer.\\nALWAYS return a \"SOURCES\" part in your answer.\\n\\nQUESTION: Which state/country\\'s law governs the interpretation of the contract?\\n=========\\nContent: This Agreement is governed by English law and the parties submit to the exclusive jurisdiction of the English courts in  relation to any dispute (contractual or non-contractual) concerning this Agreement save that either party may apply to any court for an  injunction or other relief to protect its Intellectual Property Rights.\\nSource: 28-pl\\nContent: No Waiver. Failure or delay in exercising any right or remedy under this Agreement shall not constitute a waiver of such (or any other)  right or remedy.\\n\\n11.7 Severability. The invalidity, illegality or unenforceability of any term (or part of a term) of this Agreement shall not affect the continuation  in force of the remainder of the term (if any) and this Agreement.\\n\\n11.8 No Agency. Except as expressly stated otherwise, nothing in this Agreement shall create an agency, partnership or joint venture of any  kind between the parties.\\n\\n11.9 No Third-Party Beneficiaries.\\nSource: 30-pl\\nContent: (b) if Google believes, in good faith, that the Distributor has violated or caused Google to violate any Anti-Bribery Laws (as  defined in Clause 8.5) or that such a violation is reasonably likely to occur,\\nSource: 4-pl\\n=========\\nFINAL ANSWER: This Agreement is governed by English law.\\nSOURCES: 28-pl\\n\\nQUESTION: What did the president say about Michael Jackson?\\n=========\\nContent: Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.  \\n\\nLast year COVID-19 kept us apart. This year we are finally together again. \\n\\nTonight, we meet as Democrats Republicans and Independents. But most importantly as Americans. \\n\\nWith a duty to one another to the American people to the Constitution. \\n\\nAnd with an unwavering resolve that freedom will always triumph over tyranny. \\n\\nSix days ago, Russia’s Vladimir Putin sought to shake the foundations of the free world thinking he could make it bend to his menacing ways. But he badly miscalculated. \\n\\nHe thought he could roll into Ukraine and the world would roll over. Instead he met a wall of strength he never imagined. \\n\\nHe met the Ukrainian people. \\n\\nFrom President Zelenskyy to every Ukrainian, their fearlessness, their courage, their determination, inspires the world. \\n\\nGroups of citizens blocking tanks with their bodies. Everyone from students to retirees teachers turned soldiers defending their homeland.\\nSource: 0-pl\\nContent: And we won’t stop. \\n\\nWe have lost so much to COVID-19. Time with one another. And worst of all, so much loss of life. \\n\\nLet’s use this moment to reset. Let’s stop looking at COVID-19 as a partisan dividing line and see it for what it is: A God-awful disease.  \\n\\nLet’s stop seeing each other as enemies, and start seeing each other for who we really are: Fellow Americans.  \\n\\nWe can’t change how divided we’ve been. But we can change how we move forward—on COVID-19 and other issues we must face together. \\n\\nI recently visited the New York City Police Department days after the funerals of Officer Wilbert Mora and his partner, Officer Jason Rivera. \\n\\nThey were responding to a 9-1-1 call when a man shot and killed them with a stolen gun. \\n\\nOfficer Mora was 27 years old. \\n\\nOfficer Rivera was 22. \\n\\nBoth Dominican Americans who’d grown up on the same streets they later chose to patrol as police officers. \\n\\nI spoke with their families and told them that we are forever in debt for their sacrifice, and we will carry on their mission to restore the trust and safety every community deserves.\\nSource: 24-pl\\nContent: And a proud Ukrainian people, who have known 30 years  of independence, have repeatedly shown that they will not tolerate anyone who tries to take their country backwards.  \\n\\nTo all Americans, I will be honest with you, as I’ve always promised. A Russian dictator, invading a foreign country, has costs around the world. \\n\\nAnd I’m taking robust action to make sure the pain of our sanctions  is targeted at Russia’s economy. And I will use every tool at our disposal to protect American businesses and consumers. \\n\\nTonight, I can announce that the United States has worked with 30 other countries to release 60 Million barrels of oil from reserves around the world.  \\n\\nAmerica will lead that effort, releasing 30 Million barrels from our own Strategic Petroleum Reserve. And we stand ready to do more if necessary, unified with our allies.  \\n\\nThese steps will help blunt gas prices here at home. And I know the news about what’s happening can seem alarming. \\n\\nBut I want you to know that we are going to be okay.\\nSource: 5-pl\\nContent: More support for patients and families. \\n\\nTo get there, I call on Congress to fund ARPA-H, the Advanced Research Projects Agency for Health. \\n\\nIt’s based on DARPA—the Defense Department project that led to the Internet, GPS, and so much more.  \\n\\nARPA-H will have a singular purpose—to drive breakthroughs in cancer, Alzheimer’s, diabetes, and more. \\n\\nA unity agenda for the nation. \\n\\nWe can do this. \\n\\nMy fellow Americans—tonight , we have gathered in a sacred space—the citadel of our democracy. \\n\\nIn this Capitol, generation after generation, Americans have debated great questions amid great strife, and have done great things. \\n\\nWe have fought for freedom, expanded liberty, defeated totalitarianism and terror. \\n\\nAnd built the strongest, freest, and most prosperous nation the world has ever known. \\n\\nNow is the hour. \\n\\nOur moment of responsibility. \\n\\nOur test of resolve and conscience, of history itself. \\n\\nIt is in this moment that our character is formed. Our purpose is found. Our future is forged. \\n\\nWell I know this nation.\\nSource: 34-pl\\n=========\\nFINAL ANSWER: The president did not mention Michael Jackson.\\nSOURCES:\\n\\nQUESTION: {question}\\n=========\\n{summaries}\\n=========\\nFINAL ANSWER:'), llm=OpenAI(client=<openai.resources.completions.Completions object at 0x308d2c110>, async_client=<openai.resources.completions.AsyncCompletions object at 0x30783e8d0>, temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='')), document_prompt=PromptTemplate(input_variables=['page_content', 'source'], template='Content: {page_content}\\nSource: {source}'), document_variable_name='summaries')), document_variable_name='context')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2bd29610-d931-4023-93c9-07c576faa728",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_generator_chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "85c5d922-7a38-4683-b69b-afac634e1e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = ConversationalRetrievalChain(\n",
    "    retriever=retriever,\n",
    "    question_generator=question_generator_chain,\n",
    "    combine_docs_chain=doc_chain,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "59466b50-59b2-426c-924f-4b5c91c2bd4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Document prompt requires documents to have metadata variables: ['source']. Received document with missing metadata: ['source'].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m qa_chain\u001b[38;5;241m.\u001b[39minvoke({\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHow does Donut compare to other document understanding models?\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchat_history\u001b[39m\u001b[38;5;124m\"\u001b[39m: []\n\u001b[1;32m      4\u001b[0m })[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/langchain/chains/base.py:163\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    162\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 163\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    164\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/langchain/chains/base.py:153\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[1;32m    152\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 153\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs, run_manager\u001b[38;5;241m=\u001b[39mrun_manager)\n\u001b[1;32m    154\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    156\u001b[0m     )\n\u001b[1;32m    158\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    159\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[1;32m    160\u001b[0m     )\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/langchain/chains/conversational_retrieval/base.py:167\u001b[0m, in \u001b[0;36mBaseConversationalRetrievalChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    165\u001b[0m         new_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m new_question\n\u001b[1;32m    166\u001b[0m     new_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchat_history\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m chat_history_str\n\u001b[0;32m--> 167\u001b[0m     answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcombine_docs_chain\u001b[38;5;241m.\u001b[39mrun(\n\u001b[1;32m    168\u001b[0m         input_documents\u001b[38;5;241m=\u001b[39mdocs, callbacks\u001b[38;5;241m=\u001b[39m_run_manager\u001b[38;5;241m.\u001b[39mget_child(), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mnew_inputs\n\u001b[1;32m    169\u001b[0m     )\n\u001b[1;32m    170\u001b[0m     output[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key] \u001b[38;5;241m=\u001b[39m answer\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_source_documents:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:148\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    146\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    147\u001b[0m     emit_warning()\n\u001b[0;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/langchain/chains/base.py:600\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[1;32m    595\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(args[\u001b[38;5;241m0\u001b[39m], callbacks\u001b[38;5;241m=\u001b[39mcallbacks, tags\u001b[38;5;241m=\u001b[39mtags, metadata\u001b[38;5;241m=\u001b[39mmetadata)[\n\u001b[1;32m    596\u001b[0m         _output_key\n\u001b[1;32m    597\u001b[0m     ]\n\u001b[1;32m    599\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[0;32m--> 600\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(kwargs, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, tags\u001b[38;5;241m=\u001b[39mtags, metadata\u001b[38;5;241m=\u001b[39mmetadata)[\n\u001b[1;32m    601\u001b[0m         _output_key\n\u001b[1;32m    602\u001b[0m     ]\n\u001b[1;32m    604\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[1;32m    605\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    606\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`run` supported with either positional arguments or keyword arguments,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    607\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but none were provided.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    608\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:148\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    146\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    147\u001b[0m     emit_warning()\n\u001b[0;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/langchain/chains/base.py:378\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[1;32m    347\u001b[0m \n\u001b[1;32m    348\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;124;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    371\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    372\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks,\n\u001b[1;32m    373\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m: tags,\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_name,\n\u001b[1;32m    376\u001b[0m }\n\u001b[0;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[1;32m    379\u001b[0m     inputs,\n\u001b[1;32m    380\u001b[0m     cast(RunnableConfig, {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m}),\n\u001b[1;32m    381\u001b[0m     return_only_outputs\u001b[38;5;241m=\u001b[39mreturn_only_outputs,\n\u001b[1;32m    382\u001b[0m     include_run_info\u001b[38;5;241m=\u001b[39minclude_run_info,\n\u001b[1;32m    383\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/langchain/chains/base.py:163\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    162\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 163\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    164\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/langchain/chains/base.py:153\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[1;32m    152\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 153\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs, run_manager\u001b[38;5;241m=\u001b[39mrun_manager)\n\u001b[1;32m    154\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    156\u001b[0m     )\n\u001b[1;32m    158\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    159\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[1;32m    160\u001b[0m     )\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/langchain/chains/combine_documents/base.py:137\u001b[0m, in \u001b[0;36mBaseCombineDocumentsChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;66;03m# Other keys are assumed to be needed for LLM prediction\u001b[39;00m\n\u001b[1;32m    136\u001b[0m other_keys \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_key}\n\u001b[0;32m--> 137\u001b[0m output, extra_return_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcombine_docs(\n\u001b[1;32m    138\u001b[0m     docs, callbacks\u001b[38;5;241m=\u001b[39m_run_manager\u001b[38;5;241m.\u001b[39mget_child(), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mother_keys\n\u001b[1;32m    139\u001b[0m )\n\u001b[1;32m    140\u001b[0m extra_return_dict[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key] \u001b[38;5;241m=\u001b[39m output\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m extra_return_dict\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/langchain/chains/combine_documents/map_reduce.py:237\u001b[0m, in \u001b[0;36mMapReduceDocumentsChain.combine_docs\u001b[0;34m(self, docs, token_max, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    231\u001b[0m question_result_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_chain\u001b[38;5;241m.\u001b[39moutput_key\n\u001b[1;32m    232\u001b[0m result_docs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    233\u001b[0m     Document(page_content\u001b[38;5;241m=\u001b[39mr[question_result_key], metadata\u001b[38;5;241m=\u001b[39mdocs[i]\u001b[38;5;241m.\u001b[39mmetadata)\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;66;03m# This uses metadata from the docs, and the textual results from `results`\u001b[39;00m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, r \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(map_results)\n\u001b[1;32m    236\u001b[0m ]\n\u001b[0;32m--> 237\u001b[0m result, extra_return_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduce_documents_chain\u001b[38;5;241m.\u001b[39mcombine_docs(\n\u001b[1;32m    238\u001b[0m     result_docs, token_max\u001b[38;5;241m=\u001b[39mtoken_max, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    239\u001b[0m )\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_intermediate_steps:\n\u001b[1;32m    241\u001b[0m     intermediate_steps \u001b[38;5;241m=\u001b[39m [r[question_result_key] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m map_results]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/langchain/chains/combine_documents/reduce.py:243\u001b[0m, in \u001b[0;36mReduceDocumentsChain.combine_docs\u001b[0;34m(self, docs, token_max, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcombine_docs\u001b[39m(\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    223\u001b[0m     docs: List[Document],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    227\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mdict\u001b[39m]:\n\u001b[1;32m    228\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Combine multiple documents recursively.\u001b[39;00m\n\u001b[1;32m    229\u001b[0m \n\u001b[1;32m    230\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;124;03m        element returned is a dictionary of other keys to return.\u001b[39;00m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 243\u001b[0m     result_docs, extra_return_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_collapse(\n\u001b[1;32m    244\u001b[0m         docs, token_max\u001b[38;5;241m=\u001b[39mtoken_max, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    245\u001b[0m     )\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcombine_documents_chain\u001b[38;5;241m.\u001b[39mcombine_docs(\n\u001b[1;32m    247\u001b[0m         docs\u001b[38;5;241m=\u001b[39mresult_docs, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    248\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/langchain/chains/combine_documents/reduce.py:288\u001b[0m, in \u001b[0;36mReduceDocumentsChain._collapse\u001b[0;34m(self, docs, token_max, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    286\u001b[0m result_docs \u001b[38;5;241m=\u001b[39m docs\n\u001b[1;32m    287\u001b[0m length_func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcombine_documents_chain\u001b[38;5;241m.\u001b[39mprompt_length\n\u001b[0;32m--> 288\u001b[0m num_tokens \u001b[38;5;241m=\u001b[39m length_func(result_docs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_collapse_docs_func\u001b[39m(docs: List[Document], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_collapse_chain\u001b[38;5;241m.\u001b[39mrun(\n\u001b[1;32m    292\u001b[0m         input_documents\u001b[38;5;241m=\u001b[39mdocs, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    293\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/langchain/chains/combine_documents/stuff.py:224\u001b[0m, in \u001b[0;36mStuffDocumentsChain.prompt_length\u001b[0;34m(self, docs, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprompt_length\u001b[39m(\u001b[38;5;28mself\u001b[39m, docs: List[Document], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mint\u001b[39m]:\n\u001b[1;32m    209\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the prompt length given the documents passed in.\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \n\u001b[1;32m    211\u001b[0m \u001b[38;5;124;03m    This can be used by a caller to determine whether passing in a list\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;124;03m        otherwise the length of the prompt in tokens.\u001b[39;00m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 224\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_inputs(docs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    225\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_chain\u001b[38;5;241m.\u001b[39mprompt\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_chain\u001b[38;5;241m.\u001b[39m_get_num_tokens(prompt)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/langchain/chains/combine_documents/stuff.py:198\u001b[0m, in \u001b[0;36mStuffDocumentsChain._get_inputs\u001b[0;34m(self, docs, **kwargs)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Construct inputs from kwargs and docs.\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \n\u001b[1;32m    185\u001b[0m \u001b[38;5;124;03mFormat and then join all the documents together into one input with name\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;124;03m    dictionary of inputs to LLMChain\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# Format each document according to the prompt\u001b[39;00m\n\u001b[0;32m--> 198\u001b[0m doc_strings \u001b[38;5;241m=\u001b[39m [format_document(doc, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdocument_prompt) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs]\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# Join the documents together to put them in the prompt.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    201\u001b[0m     k: v\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_chain\u001b[38;5;241m.\u001b[39mprompt\u001b[38;5;241m.\u001b[39minput_variables\n\u001b[1;32m    204\u001b[0m }\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/langchain/chains/combine_documents/stuff.py:198\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Construct inputs from kwargs and docs.\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \n\u001b[1;32m    185\u001b[0m \u001b[38;5;124;03mFormat and then join all the documents together into one input with name\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;124;03m    dictionary of inputs to LLMChain\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# Format each document according to the prompt\u001b[39;00m\n\u001b[0;32m--> 198\u001b[0m doc_strings \u001b[38;5;241m=\u001b[39m [format_document(doc, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdocument_prompt) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs]\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# Join the documents together to put them in the prompt.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    201\u001b[0m     k: v\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_chain\u001b[38;5;241m.\u001b[39mprompt\u001b[38;5;241m.\u001b[39minput_variables\n\u001b[1;32m    204\u001b[0m }\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/langchain_core/prompts/base.py:332\u001b[0m, in \u001b[0;36mformat_document\u001b[0;34m(doc, prompt)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mformat_document\u001b[39m(doc: Document, prompt: BasePromptTemplate[\u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    299\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Format a document into a string based on a prompt template.\u001b[39;00m\n\u001b[1;32m    300\u001b[0m \n\u001b[1;32m    301\u001b[0m \u001b[38;5;124;03m    First, this pulls information from the document from two sources:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;124;03m            >>> \"Page 1: This is a joke\"\u001b[39;00m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 332\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m prompt\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_get_document_info(doc, prompt))\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/langchain_core/prompts/base.py:290\u001b[0m, in \u001b[0;36m_get_document_info\u001b[0;34m(doc, prompt)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_metadata) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    287\u001b[0m     required_metadata \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    288\u001b[0m         iv \u001b[38;5;28;01mfor\u001b[39;00m iv \u001b[38;5;129;01min\u001b[39;00m prompt\u001b[38;5;241m.\u001b[39minput_variables \u001b[38;5;28;01mif\u001b[39;00m iv \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpage_content\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    289\u001b[0m     ]\n\u001b[0;32m--> 290\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    291\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDocument prompt requires documents to have metadata variables: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    292\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrequired_metadata\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Received document with missing metadata: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    293\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(missing_metadata)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    294\u001b[0m     )\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {k: base_info[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m prompt\u001b[38;5;241m.\u001b[39minput_variables}\n",
      "\u001b[0;31mValueError\u001b[0m: Document prompt requires documents to have metadata variables: ['source']. Received document with missing metadata: ['source']."
     ]
    }
   ],
   "source": [
    "qa_chain.invoke({\n",
    "    \"question\": \"How does Donut compare to other document understanding models?\",\n",
    "    \"chat_history\": []\n",
    "})[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5926ae1-939a-4888-abbb-1f098a7e5de2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69dee57-faa8-4b6b-914f-472c856b320f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
