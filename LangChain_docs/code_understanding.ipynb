{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85877b93-375a-476c-8884-6500cecbc8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade --quiet langchain-openai tiktoken chromadb langchain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "695f8bbe-9900-47e2-9eb1-61041e42f1bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement git (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for git\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05a1ec32-7d5a-46b6-aed9-38012988fae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from git import Repo\n",
    "from langchain_community.document_loaders.generic import GenericLoader\n",
    "from langchain_community.document_loaders.parsers import LanguageParser\n",
    "from langchain_text_splitters import Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05f4465a-b5df-4dbc-8278-304408001597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone\n",
    "repo_path = \"/Users/praveenreddy/GenAI/GenAI_Practice/LangChain_docs/test_repo\"\n",
    "repo = Repo.clone_from(\"https://github.com/langchain-ai/langchain\", to_path=repo_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75d88283-c162-4b16-a4b3-2aaabb02d822",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = GenericLoader.from_filesystem(\n",
    "    repo_path + \"/libs/core/langchain_core\",\n",
    "    glob=\"**/*\",\n",
    "    suffixes=[\".py\"],\n",
    "    exclude=[\"**/non-utf8-encoding.py\"],\n",
    "    parser = LanguageParser(language=Language.PYTHON, parser_threshold=500)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fdc3ded4-e7ff-4486-961b-cb1932e215a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1a15938-a5da-42d4-95ca-4b048d7bd838",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "308"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8a23524-b71c-4509-a610-41550fdddfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON, chunk_size=2000, chunk_overlap=200\n",
    ")\n",
    "texts = python_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12c20d8f-00f0-4611-b240-cee5efb46e1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "941"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ace49bd-494c-42c7-8367-714e74fba310",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='\"\"\"**sys_info** prints information about the system and langchain packages\\nfor debugging purposes.\\n\"\"\"\\n\\nfrom typing import Sequence', metadata={'source': '/Users/praveenreddy/GenAI/GenAI_Practice/LangChain_docs/test_repo/libs/core/langchain_core/sys_info.py', 'language': <Language.PYTHON: 'python'>})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc2ac082-0796-4ff2-b769-95cda687d720",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9e91447a-edd2-4e31-89ee-4b487d1ea752",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "db = Chroma.from_documents(texts, OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY,disallowed_special=()))\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"mmr\",  # Also test \"similarity\"\n",
    "    search_kwargs={\"k\": 8},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b9c683e7-4f75-4cdd-a7ac-67aa97ebdda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(openai_api_key=OPENAI_API_KEY,model=\"gpt-4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e2a304f8-7ef5-413b-b8bc-82c848bd41d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"placeholder\", \"{chat_history}\"),\n",
    "        (\"user\", \"{input}\"),\n",
    "        (\n",
    "            \"user\",\n",
    "            \"Given the above conversation, generate a search query to look up to get information relevant to the conversation\",\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "19fd5285-740e-4803-b354-c0e920caf239",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_chain = create_history_aware_retriever(llm, retriever, prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f525e048-deab-43bd-906d-e24f5b7883b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Answer the user's questions based on the below context:\\n\\n{context}\",\n",
    "        ),\n",
    "        (\"placeholder\", \"{chat_history}\"),\n",
    "        (\"user\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "87011326-02ce-41bb-b04e-d094d1e5ae3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = create_retrieval_chain(retriever_chain, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "84e9c24a-da27-47cc-9526-545c842139da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A RunnableBinding is a class in LangChain that allows arguments to be bound to a Runnable, returning a new Runnable. This is especially useful when a runnable in a chain requires an argument that is not in the output of the previous runnable or included in the user input. The bound arguments are provided in the form of keyword arguments (kwargs). An example usage of RunnableBinding is provided in the context, where it is used to bind a stop parameter to a ChatOllama model in a chain.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What is a RunnableBinding?\"\n",
    "result = qa.invoke({\"input\": question})\n",
    "result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "08f39ecd-285f-47c6-b28f-78f410a6999d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'What is a RunnableBinding?', 'context': [Document(page_content='# from langchain_core.runnables.base import RunnableBinding\\n\\n\\n# class RunnableLearnable(RunnableBinding):\\n#     def __init__(self, *args, **kwargs):\\n#         super().__init__(*args, **kwargs)\\n#         self.parameters = []\\n\\n#     def backward(self):\\n#         for param in self.parameters:\\n#             param.backward()\\n\\n#     def update(self, optimizer):\\n#         for param in self.parameters:\\n#             optimizer.update(param)', metadata={'language': 'python', 'source': '/Users/praveenreddy/GenAI/GenAI_Practice/LangChain_docs/test_repo/libs/core/langchain_core/runnables/learnable.py'}), Document(page_content='async for chunk in input:\\n            if not got_first_val:\\n                final = adapt_first_streaming_chunk(chunk)  # type: ignore\\n                got_first_val = True\\n            else:\\n                # Make a best effort to gather, for any type that supports `+`\\n                # This method should throw an error if gathering fails.\\n                try:\\n                    final = final + chunk  # type: ignore[operator]\\n                except TypeError:\\n                    raise TypeError(\\n                        f\"Failed while trying to add together \"\\n                        f\"type {type(final)} and {type(chunk)}.\"\\n                        f\"These types should be addable for atransform to work.\"\\n                    )\\n\\n        if got_first_val:\\n            async for output in self.astream(final, config, **kwargs):\\n                yield output\\n\\n    def bind(self, **kwargs: Any) -> Runnable[Input, Output]:\\n        \"\"\"\\n        Bind arguments to a Runnable, returning a new Runnable.\\n\\n        Useful when a runnable in a chain requires an argument that is not\\n        in the output of the previous runnable or included in the user input.\\n\\n        Example:\\n\\n        .. code-block:: python\\n\\n            from langchain_community.chat_models import ChatOllama\\n            from langchain_core.output_parsers import StrOutputParser\\n\\n            llm = ChatOllama(model=\\'llama2\\')\\n\\n            # Without bind.\\n            chain = (\\n                llm\\n                | StrOutputParser()\\n            )\\n\\n            chain.invoke(\"Repeat quoted words exactly: \\'One two three four five.\\'\")\\n            # Output is \\'One two three four five.\\'\\n\\n            # With bind.\\n            chain = (\\n                llm.bind(stop=[\"three\"])\\n                | StrOutputParser()\\n            )\\n\\n            chain.invoke(\"Repeat quoted words exactly: \\'One two three four five.\\'\")\\n            # Output is \\'One two\\'\\n\\n        \"\"\"\\n        return RunnableBinding(bound=self, kwargs=kwargs, config={})', metadata={'content_type': 'functions_classes', 'language': 'python', 'source': '/Users/praveenreddy/GenAI/GenAI_Practice/LangChain_docs/test_repo/libs/core/langchain_core/runnables/base.py'}), Document(page_content='The type can be a pydantic model, or a type annotation (e.g., `List[str]`).\\n    \"\"\"\\n\\n    class Config:\\n        arbitrary_types_allowed = True\\n\\n    def __init__(\\n        self,\\n        *,\\n        bound: Runnable[Input, Output],\\n        kwargs: Optional[Mapping[str, Any]] = None,\\n        config: Optional[RunnableConfig] = None,\\n        config_factories: Optional[\\n            List[Callable[[RunnableConfig], RunnableConfig]]\\n        ] = None,\\n        custom_input_type: Optional[Union[Type[Input], BaseModel]] = None,\\n        custom_output_type: Optional[Union[Type[Output], BaseModel]] = None,\\n        **other_kwargs: Any,\\n    ) -> None:\\n        \"\"\"Create a RunnableBinding from a runnable and kwargs.', metadata={'content_type': 'functions_classes', 'language': 'python', 'source': '/Users/praveenreddy/GenAI/GenAI_Practice/LangChain_docs/test_repo/libs/core/langchain_core/runnables/base.py'}), Document(page_content='class RunnableRetry(RunnableBindingBase[Input, Output]):\\n    \"\"\"Retry a Runnable if it fails.\\n\\n    RunnableRetry can be used to add retry logic to any object\\n    that subclasses the base Runnable.\\n\\n    Such retries are especially useful for network calls that may fail\\n    due to transient errors.\\n\\n    The RunnableRetry is implemented as a RunnableBinding. The easiest\\n    way to use it is through the `.with_retry()` method on all Runnables.\\n\\n    Example:\\n\\n    Here\\'s an example that uses a RunnableLambda to raise an exception\\n\\n        .. code-block:: python\\n\\n            import time\\n\\n            def foo(input) -> None:\\n                \\'\\'\\'Fake function that raises an exception.\\'\\'\\'\\n                raise ValueError(f\"Invoking foo failed. At time {time.time()}\")\\n\\n            runnable = RunnableLambda(foo)\\n\\n            runnable_with_retries = runnable.with_retry(\\n                retry_if_exception_type=(ValueError,), # Retry only on ValueError\\n                wait_exponential_jitter=True, # Add jitter to the exponential backoff\\n                stop_after_attempt=2, # Try twice\\n            )\\n\\n            # The method invocation above is equivalent to the longer form below:\\n\\n            runnable_with_retries = RunnableRetry(\\n                bound=runnable,\\n                retry_exception_types=(ValueError,),\\n                max_attempt_number=2,\\n                wait_exponential_jitter=True\\n            )\\n\\n    This logic can be used to retry any Runnable, including a chain of Runnables,\\n    but in general it\\'s best practice to keep the scope of the retry as small as\\n    possible. For example, if you have a chain of Runnables, you should only retry\\n    the Runnable that is likely to fail, not the entire chain.\\n\\n    Example:\\n\\n        .. code-block:: python\\n\\n            from langchain_core.chat_models import ChatOpenAI\\n            from langchain_core.prompts import PromptTemplate', metadata={'language': 'python', 'source': '/Users/praveenreddy/GenAI/GenAI_Practice/LangChain_docs/test_repo/libs/core/langchain_core/runnables/retry.py'}), Document(page_content='def with_listeners(\\n        self,\\n        *,\\n        on_start: Optional[Listener] = None,\\n        on_end: Optional[Listener] = None,\\n        on_error: Optional[Listener] = None,\\n    ) -> RunnableEach[Input, Output]:\\n        \"\"\"\\n        Bind lifecycle listeners to a Runnable, returning a new Runnable.\\n\\n        on_start: Called before the runnable starts running, with the Run object.\\n        on_end: Called after the runnable finishes running, with the Run object.\\n        on_error: Called if the runnable throws an error, with the Run object.\\n\\n        The Run object contains information about the run, including its id,\\n        type, input, output, error, start_time, end_time, and any tags or metadata\\n        added to the run.\\n        \"\"\"\\n        return RunnableEach(\\n            bound=self.bound.with_listeners(\\n                on_start=on_start, on_end=on_end, on_error=on_error\\n            )\\n        )', metadata={'content_type': 'functions_classes', 'language': 'python', 'source': '/Users/praveenreddy/GenAI/GenAI_Practice/LangChain_docs/test_repo/libs/core/langchain_core/runnables/base.py'}), Document(page_content='class RunnableConfig(TypedDict, total=False):\\n    \"\"\"Configuration for a Runnable.\"\"\"\\n\\n    tags: List[str]\\n    \"\"\"\\n    Tags for this call and any sub-calls (eg. a Chain calling an LLM).\\n    You can use these to filter calls.\\n    \"\"\"\\n\\n    metadata: Dict[str, Any]\\n    \"\"\"\\n    Metadata for this call and any sub-calls (eg. a Chain calling an LLM).\\n    Keys should be strings, values should be JSON-serializable.\\n    \"\"\"\\n\\n    callbacks: Callbacks\\n    \"\"\"\\n    Callbacks for this call and any sub-calls (eg. a Chain calling an LLM).\\n    Tags are passed to all callbacks, metadata is passed to handle*Start callbacks.\\n    \"\"\"\\n\\n    run_name: str\\n    \"\"\"\\n    Name for the tracer run for this call. Defaults to the name of the class.\\n    \"\"\"\\n\\n    max_concurrency: Optional[int]\\n    \"\"\"\\n    Maximum number of parallel calls to make. If not provided, defaults to \\n    ThreadPoolExecutor\\'s default.\\n    \"\"\"\\n\\n    recursion_limit: int\\n    \"\"\"\\n    Maximum number of times a call can recurse. If not provided, defaults to 25.\\n    \"\"\"\\n\\n    configurable: Dict[str, Any]\\n    \"\"\"\\n    Runtime values for attributes previously made configurable on this Runnable,\\n    or sub-Runnables, through .configurable_fields() or .configurable_alternatives().\\n    Check .output_schema() for a description of the attributes that have been made \\n    configurable.\\n    \"\"\"\\n\\n    run_id: Optional[uuid.UUID]\\n    \"\"\"\\n    Unique identifier for the tracer run for this call. If not provided, a new UUID\\n        will be generated.\\n    \"\"\"', metadata={'content_type': 'functions_classes', 'language': 'python', 'source': '/Users/praveenreddy/GenAI/GenAI_Practice/LangChain_docs/test_repo/libs/core/langchain_core/runnables/config.py'}), Document(page_content='\"\"\"LangChain **Runnable** and the **LangChain Expression Language (LCEL)**.\\n\\nThe LangChain Expression Language (LCEL) offers a declarative method to build\\nproduction-grade programs that harness the power of LLMs.\\n\\nPrograms created using LCEL and LangChain Runnables inherently support\\nsynchronous, asynchronous, batch, and streaming operations.\\n\\nSupport for **async** allows servers hosting LCEL based programs to scale better\\nfor higher concurrent loads.\\n\\n**Batch** operations allow for processing multiple inputs in parallel.\\n\\n**Streaming** of intermediate outputs, as they\\'re being generated, allows for\\ncreating more responsive UX.\\n\\nThis module contains schema and implementation of LangChain Runnables primitives.\\n\"\"\"\\nfrom langchain_core.runnables.base import (\\n    Runnable,\\n    RunnableBinding,\\n    RunnableGenerator,\\n    RunnableLambda,\\n    RunnableMap,\\n    RunnableParallel,\\n    RunnableSequence,\\n    RunnableSerializable,\\n    chain,\\n)\\nfrom langchain_core.runnables.branch import RunnableBranch\\nfrom langchain_core.runnables.config import (\\n    RunnableConfig,\\n    ensure_config,\\n    get_config_list,\\n    patch_config,\\n    run_in_executor,\\n)\\nfrom langchain_core.runnables.fallbacks import RunnableWithFallbacks\\nfrom langchain_core.runnables.passthrough import (\\n    RunnableAssign,\\n    RunnablePassthrough,\\n    RunnablePick,\\n)\\nfrom langchain_core.runnables.router import RouterInput, RouterRunnable\\nfrom langchain_core.runnables.utils import (\\n    AddableDict,\\n    ConfigurableField,\\n    ConfigurableFieldMultiOption,\\n    ConfigurableFieldSingleOption,\\n    ConfigurableFieldSpec,\\n    aadd,\\n    add,\\n)', metadata={'language': 'python', 'source': '/Users/praveenreddy/GenAI/GenAI_Practice/LangChain_docs/test_repo/libs/core/langchain_core/runnables/__init__.py'}), Document(page_content='class RunnableConfigurableFields(DynamicRunnable[Input, Output]):\\n    \"\"\"Runnable that can be dynamically configured.\\n\\n    A RunnableConfigurableFields should be initiated using the\\n    `configurable_fields` method of a Runnable.\\n\\n    Here is an example of using a RunnableConfigurableFields with LLMs:\\n\\n        .. code-block:: python\\n\\n            from langchain_core.prompts import PromptTemplate\\n            from langchain_core.runnables import ConfigurableField\\n            from langchain_openai import ChatOpenAI\\n\\n            model = ChatOpenAI(temperature=0).configurable_fields(\\n                temperature=ConfigurableField(\\n                    id=\"temperature\",\\n                    name=\"LLM Temperature\",\\n                    description=\"The temperature of the LLM\",\\n                )\\n            )\\n            # This creates a RunnableConfigurableFields for a chat model.\\n\\n            # When invoking the created RunnableSequence, you can pass in the\\n            # value for your ConfigurableField\\'s id which in this case\\n            # will be change in temperature\\n\\n            prompt = PromptTemplate.from_template(\"Pick a random number above {x}\")\\n            chain = prompt | model\\n\\n            chain.invoke({\"x\": 0})\\n            chain.invoke({\"x\": 0}, config={\"configurable\": {\"temperature\": 0.9}})\\n\\n\\n    Here is an example of using a RunnableConfigurableFields with HubRunnables:\\n\\n        .. code-block:: python\\n\\n            from langchain_core.prompts import PromptTemplate\\n            from langchain_core.runnables import ConfigurableField\\n            from langchain_openai import ChatOpenAI\\n            from langchain.runnables.hub import HubRunnable\\n\\n            prompt = HubRunnable(\"rlm/rag-prompt\").configurable_fields(\\n                owner_repo_commit=ConfigurableField(\\n                    id=\"hub_commit\",\\n                    name=\"Hub Commit\",\\n                    description=\"The Hub commit to pull from\",\\n                )\\n            )', metadata={'content_type': 'functions_classes', 'language': 'python', 'source': '/Users/praveenreddy/GenAI/GenAI_Practice/LangChain_docs/test_repo/libs/core/langchain_core/runnables/configurable.py'})], 'answer': 'A RunnableBinding is a class in LangChain that allows arguments to be bound to a Runnable, returning a new Runnable. This is especially useful when a runnable in a chain requires an argument that is not in the output of the previous runnable or included in the user input. The bound arguments are provided in the form of keyword arguments (kwargs). An example usage of RunnableBinding is provided in the context, where it is used to bind a stop parameter to a ChatOllama model in a chain.'}\n"
     ]
    }
   ],
   "source": [
    "print(result) #.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f38060de-81ff-4c87-b5d2-b490e4ed5ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> **Question**: What classes are derived from the Runnable class? \n",
      "\n",
      "**Answer**: The classes derived from the Runnable class mentioned in the given context are:\n",
      "\n",
      "1. RunnableLambda\n",
      "2. RunnableBinding\n",
      "3. RunnableLearnable\n",
      "4. RunnableSerializable\n",
      "5. RunnableWithFallbacks (indirectly mentioned, not explicitly defined in the given context) \n",
      "\n",
      "-> **Question**: What one improvement do you propose in code in relation to the class hierarchy for the Runnable class? \n",
      "\n",
      "**Answer**: One potential improvement could be to create an abstract base class for the Runnable class to better organize the hierarchy and enforce the implementation of certain methods. This would ensure that all subclasses of Runnable would have to implement these methods, providing a consistent interface across all types of Runnables. This can be especially helpful in large codebases to ensure consistency and clarity in the design of the software. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    \"What classes are derived from the Runnable class?\",\n",
    "    \"What one improvement do you propose in code in relation to the class hierarchy for the Runnable class?\",\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    result = qa.invoke({\"input\": question})\n",
    "    print(f\"-> **Question**: {question} \\n\")\n",
    "    print(f\"**Answer**: {result['answer']} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a0ecac-d5bf-4234-85fc-666c3cf94c4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
