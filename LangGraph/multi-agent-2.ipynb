{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import (\n",
    "    BaseMessage,\n",
    "    HumanMessage,\n",
    "    ToolMessage,\n",
    ")\n",
    "import operator\n",
    "from typing import Annotated, Sequence, TypedDict\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from typing import Annotated\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.tools import tool\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "from langgraph.graph import END, StateGraph\n",
    "import functools\n",
    "from langchain_core.messages import AIMessage\n",
    "from typing import Literal\n",
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "from langchain_community.retrievers import WikipediaRetriever\n",
    "from langchain_openai import ChatOpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tavilee = TavilySearchResults()\n",
    "tool1 = [tavilee]\n",
    "\n",
    "@tool\n",
    "def wiki(query):\n",
    "    \"\"\" This is the method to do Wikipedia Search on the argument passed !! \"\"\"\n",
    "    wikipedia = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())\n",
    "    return wikipedia.run(query)\n",
    "    # retriever = WikipediaRetriever()\n",
    "    # retriever.invoke(\"HUNTER X HUNTER\")\n",
    "\n",
    "tool2 = [wiki]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "                \"system\",\n",
    "                \"You are a helpful AI assistant, collaborating with other assistants.\"\n",
    "                \" Use the provided tools to progress towards answering the question.\"\n",
    "                \" If you are unable to fully answer, that's OK, another assistant with different tools \"\n",
    "                \" will help where you left off. Execute what you can to make progress.\"\n",
    "                \" If you or any of the other assistants have the final answer or deliverable,\"\n",
    "                \" prefix your response with FINAL ANSWER so the team knows to stop.\"\n",
    "                \" You have access to the following tools: {tool_names}.\\n,\"\n",
    "                \"You should provide the exact name of the Chief Miniter Of Telangana , India \"\n",
    "        ),\n",
    "            MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "prompt1 = prompt1.partial(tool_names=', '.join(tool.name for tool in tool1))\n",
    "tavily_agent =  prompt1 |llm.bind_tools(tool1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt2 = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "                \"system\",\n",
    "                \"You are a helpful AI assistant, collaborating with other assistants.\"\n",
    "                \" Use the provided tools to progress towards answering the question.\"\n",
    "                \" If you are unable to fully answer, that's OK, another assistant with different tools \"\n",
    "                \" will help where you left off. Execute what you can to make progress.\"\n",
    "                \" If you or any of the other assistants have the final answer or deliverable,\"\n",
    "                \" prefix your response with FINAL ANSWER so the team knows to stop.\"\n",
    "                \" You have access to the following tools: {tool_names}.\\n,\"\n",
    "                \"You should do the summarize the response from the tool into 5 liners \"\n",
    "        ),\n",
    "            MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "prompt2 = prompt2.partial(tool_names=', '.join(tool.name for tool in tool2))\n",
    "wiki_agent =  prompt2 |llm.bind_tools(tool2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    messages:Annotated[Sequence[BaseMessage],operator.add]\n",
    "    sender:str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent_node(state,agent,name):\n",
    "    result = agent.invoke(state)\n",
    "    print(\"------>>>result  \",result)\n",
    "    if isinstance(result,ToolMessage):\n",
    "        pass\n",
    "    else:\n",
    "        result = AIMessage(**result.dict(exclude={\"type\",\"name\"}),name=name)\n",
    "    return {\n",
    "        \"messages\":[result],\n",
    "        \"sender\":name\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "tavily_node = functools.partial(agent_node,agent=tavily_agent,name='Search')\n",
    "\n",
    "wiki_node = functools.partial(agent_node,agent=wiki_agent,name='Wiki')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "tools = [tavilee, wiki]\n",
    "tool_node = ToolNode(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "functools.partial(<function agent_node at 0x10952bb00>, agent=ChatPromptTemplate(input_variables=['messages'], input_types={'messages': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, partial_variables={'tool_names': 'tavily_search_results_json'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['tool_names'], template=\"You are a helpful AI assistant, collaborating with other assistants. Use the provided tools to progress towards answering the question. If you are unable to fully answer, that's OK, another assistant with different tools  will help where you left off. Execute what you can to make progress. If you or any of the other assistants have the final answer or deliverable, prefix your response with FINAL ANSWER so the team knows to stop. You have access to the following tools: {tool_names}.\\n,You should provide the exact name of the Chief Miniter Of Telangana , India \")), MessagesPlaceholder(variable_name='messages')])\n",
       "| RunnableBinding(bound=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x108536d10>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x108542710>, openai_api_key=SecretStr('**********'), openai_proxy=''), kwargs={'tools': [{'type': 'function', 'function': {'name': 'tavily_search_results_json', 'description': 'A search engine optimized for comprehensive, accurate, and trusted results. Useful for when you need to answer questions about current events. Input should be a search query.', 'parameters': {'type': 'object', 'properties': {'query': {'description': 'search query to look up', 'type': 'string'}}, 'required': ['query']}}}]}), name='Search')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tavily_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def router(state) -> Literal[\"call_tool\", \"__end__\", \"continue\"]:\n",
    "    last_message = state['messages'][-1]\n",
    "    print(\"last_message \\n\\n\")\n",
    "    print(last_message)\n",
    "    print(\"last_message \\n\\n\")\n",
    "    \n",
    "    if last_message.tool_calls:\n",
    "        return \"call_tool\"\n",
    "    if \"FINAL ANSWER\" in last_message.content:\n",
    "        # Any agent decided the work is done\n",
    "        return \"__end__\"\n",
    "    return \"continue\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "workflow.add_node(\"Search\",tavily_node)\n",
    "workflow.add_node(\"Wiki\",wiki_node)\n",
    "workflow.add_node(\"call_tool\",tool_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.add_conditional_edges(\n",
    "    \"Search\",\n",
    "    router,\n",
    "    {\"continue\": \"Wiki\", \"call_tool\": \"call_tool\", \"__end__\": END},\n",
    ")\n",
    "workflow.add_conditional_edges(\n",
    "    \"Wiki\",\n",
    "    router,\n",
    "    {\"continue\": \"Search\", \"call_tool\": \"call_tool\", \"__end__\": END},\n",
    ")\n",
    "workflow.add_conditional_edges(\n",
    "    \"call_tool\",\n",
    "    lambda x: x[\"sender\"],\n",
    "    {\n",
    "        \"Search\": \"Search\",\n",
    "        \"Wiki\": \"Wiki\",\n",
    "    },\n",
    ") \n",
    "workflow.set_entry_point(\"Search\")\n",
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph(xray=True).draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------>>>result   content='' additional_kwargs={'tool_calls': [{'id': 'call_owHhuhx0L63LNHFLB3UZRHbu', 'function': {'arguments': '{\"query\":\"Current Chief Minister of Telangana, India\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}]} response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 221, 'total_tokens': 247}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'tool_calls', 'logprobs': None} id='run-5d23cf31-8db5-4e56-8553-d0a173abb801-0' tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'Current Chief Minister of Telangana, India'}, 'id': 'call_owHhuhx0L63LNHFLB3UZRHbu'}]\n",
      "last_message \n",
      "\n",
      "\n",
      "content='' additional_kwargs={'tool_calls': [{'id': 'call_owHhuhx0L63LNHFLB3UZRHbu', 'function': {'arguments': '{\"query\":\"Current Chief Minister of Telangana, India\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}]} response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 221, 'total_tokens': 247}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'tool_calls', 'logprobs': None} name='Search' id='run-5d23cf31-8db5-4e56-8553-d0a173abb801-0' tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'Current Chief Minister of Telangana, India'}, 'id': 'call_owHhuhx0L63LNHFLB3UZRHbu'}]\n",
      "last_message \n",
      "\n",
      "\n",
      "{'Search': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_owHhuhx0L63LNHFLB3UZRHbu', 'function': {'arguments': '{\"query\":\"Current Chief Minister of Telangana, India\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 221, 'total_tokens': 247}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'tool_calls', 'logprobs': None}, name='Search', id='run-5d23cf31-8db5-4e56-8553-d0a173abb801-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'Current Chief Minister of Telangana, India'}, 'id': 'call_owHhuhx0L63LNHFLB3UZRHbu'}])], 'sender': 'Search'}}\n",
      "----\n",
      "{'call_tool': {'messages': [ToolMessage(content='[{\"url\": \"https://www.thehindu.com/elections/telangana-assembly/revanth-reddy-is-the-new-chief-minister-of-telangana/article67608096.ece\", \"content\": \"To enjoy additional benefits\\\\nCONNECT WITH US\\\\nCongress announces Revanth Reddy as CM of Telangana\\\\nThe State party chief was named as Congress central leadership\\\\u2019s choice for the CLP chief on the basis of a report submitted by party observers; he will take oath as CM on December 7\\\\nDecember 05, 2023 07:52 pm | Updated December 06, 2023 04:06 pm IST\\\\nCOMMents\\\\nSHARE\\\\nREAD LATER\\\\nFile picture of Telangana Congress chief A. Revanth Reddy\\\\n| Photo Credit: Making the announcement at a press conference in New Delhi, All India Congress Committee general secretary K.C. Venugopal said: \\\\u201cGoing by the feedback provided by the Congress observers, including Karnataka Deputy Chief Minister D.K. Shivakumar, party in-charge for Telangana Manikrao Thakare, and other leaders, the Congress president decided to nominate Mr. Reddy as the CLP leader.\\\\u201d\\\\n COMMents\\\\nSHARE\\\\nTelangana\\\\n/\\\\nTelangana Assembly Elections 2023\\\\n/\\\\nIndian National Congress\\\\nTop News Today\\\\nBACK TO TOP\\\\nComments\\\\nComments have to be in English, and in full sentences. He reiterated the party\\\\u2019s commitment to the people of Telangana, and said that the new government\\\\u2019s top priority would be to fulfil their aspirations and the guarantees that the party has given them. The Telangana Pradesh Congress Committee president was elected with a thumping majority from the Kodangal Assembly constituency, though he \\\\u2014 along with outgoing Chief Minister K. Chandrashekhar Rao \\\\u2014 lost the Kamareddy seat to the BJP candidate Katipally Venkataramana Reddy.\\\\n\"}, {\"url\": \"https://en.wikipedia.org/wiki/List_of_chief_ministers_of_Telangana\", \"content\": \"Since the state\\'s creation on 2 June 2014,[2] Telangana has had two chief ministers, the first of which belongs to Bharat Rashtra Samithi party, its founder and former Minister of Labour and Employment of the Republic of India K. Chandrashekar Rao was the inaugural holder of the office who sworn in two times by winning the 2014 and 2018 assembly elections consecutively. Within the decade, Hyderabad would become the capital of the state of Telangana, and a new capital was selected for Andhra Pradesh.[4] Andhra Pradesh picked Amaravati as its capital and relocated its secretariat in 2016 and its legislature in 2017.[5][6]\\\\nChief Ministers of Erstwhile Hyderabad State[edit]\\\\nThe Hyderabad State included nine Telugu districts of Telangana, four Kannada districts in Gulbarga division and four Marathi districts in Aurangabad division.[8] Contents\\\\nList of chief ministers of Telangana\\\\nThe Chief Minister of Telangana is the chief executive of the Indian state of Telangana. Kalvakuntla Chandrashekar Rao was elected as the first chief minister of Telangana, following elections in which the Bharat Rashtra Samithi party secured a majority.[3] Hyderabad would remain as the joint capital of both Telangana and Andhra Pradesh for a period. Following elections to the Telangana Legislative Assembly, the state\\'s governor usually invites the party (or coalition) with a majority of seats to form the government.\"}, {\"url\": \"https://en.wikipedia.org/wiki/Revanth_Reddy\", \"content\": \"On 20 September 2018, he was appointed one of the three working presidents of Telangana Pradesh Congress Committee (TPCC).[15]\\\\nMember of Parliament[edit]\\\\nFollowing his defeat in the 2018 Telangana legislative assembly election, Reddy successfully contested the 2019 general election from the Malkajgiri Lok Sabha constituency as a Congress candidate, by a margin of 10,919 votes, constituting 38.63% of the total votes. He was elected as the floor leader of Telugu Desam Party\\\\n(TDP) in the Telangana Legislative Assembly.[3]\\\\nOn 25 October 2017, TDP removed him as the floor leader of the Telangana TDP after reports surfaced that he would consider joining the Congress party.[13] On 7 December 2023, he took oath as the Chief Minister of Telangana, making him the second person to hold the post.[23][24]\\\\nElectoral performance[edit]\\\\nOverseas engagements[edit]\\\\nIn June 2009, when Indian students were being attacked in Australia, then Telugu Desam leaders Reddy and Nama Nageswara Rao had visited Melbourne, Australia to meet with International students from India. On 31 May 2015, Revanth Reddy was arrested by the Anti-Corruption Bureau (ACB) during a sting operation after bribing nominated MLA Elvis Stephenson to vote in favour of the Telugu Desam Party (TDP) candidate in the Legislative Council election.[27][28][29][30][31] A criminal case under sections of Prevention of Corruption Act and sections 120-B (criminal conspiracy) \\\\u2013 read with section 34 (common intention) of Indian Penal Code was registered against him along with two others \\\\u2013 Bishop Sebastian Harry and Uday Simha. Early and personal life[edit]\\\\nRevanth Reddy was born on 8 November 1969 in Kondareddy Palli of Mahboobnagar district (in present-day Nagarkurnool district, Telangana), India.[3]\\\\n[4]\\\\nHe graduated with a Bachelor of Arts from\\\\nAndhra Vidyalaya College, Osmania University, Hyderabad.[5][6]\\\\nReddy married Geetha, former union minister Jaipal Reddy\\'s niece.[7][8]\"}, {\"url\": \"http://cm.telangana.gov.in/\", \"content\": \"More dump yards in Hyderabad\\\\nHon\\\\u2019ble Chief Minister Sri A. Revanth Reddy suggested to the officials to set up four Dump Yards on the four sides of the Hyderabad city, the dump yard will be established far away from the residential areas.\\\\n Social Media Updates\\\\nFollow us on social media\\\\nAwards & Recognition\\\\nBest tourism awards to Pemberthi and Chandlapur\\\\nChief Minister\\'s Profile\\\\nChief Minister Relief Fund\\\\nLast updated on\\\\nAccessibility Tools CM congratulated Police Officers on winning Best Police Station Award\\\\nHon\\\\u2019ble Chief Minister Sri Revanth Reddy congratulated the police officers on Rajendra Nagar Police Station being adjudged as the \\\\u201cBest Police Station\\\\u201d in the country.\\\\n Chief Minister\\\\nGovernment of Telangana\\\\nHon\\'ble Chief Minister Sri A. Revanth Reddy said that the State Government will unveil a \\\\u2018Mega Master policy\\\\u2019 aiming to achieve industrial growth in the entire Telangana State by 2050. 02.01.2024\\\\nChief Minister Sri A. Revanth Reddy directed the officials concerned to develop the Musi Riverfront in the next 36 months.\"}, {\"url\": \"https://www.msn.com/en-in/news/India/who-is-anumula-revanth-reddy-heres-all-about-the-new-chief-minister-of-telangana/ar-AA1l4hdS\", \"content\": \"The Congress Party on Tuesday announced Telangana Pradesh Congress Committee (TPCC) President Anumula Revanth Reddy as its Chief Minster face for Telangana. The announcement, which was widely ...\"}]', name='tavily_search_results_json', tool_call_id='call_owHhuhx0L63LNHFLB3UZRHbu')]}}\n",
      "----\n",
      "------>>>result   content='The current Chief Minister of Telangana, India is Anumula Revanth Reddy. He was appointed as the Chief Minister by the Congress party and took oath on December 7, 2023. Anumula Revanth Reddy is the President of the Telangana Pradesh Congress Committee (TPCC). He has been actively involved in politics and has a strong background in serving the people of Telangana. Anumula Revanth Reddy is known for his dedication to fulfilling the aspirations of the people and is focused on the development of the state.' response_metadata={'token_usage': {'completion_tokens': 115, 'prompt_tokens': 1899, 'total_tokens': 2014}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-af45301a-dd25-46d7-ab22-2487f4b7fdfa-0'\n",
      "last_message \n",
      "\n",
      "\n",
      "content='The current Chief Minister of Telangana, India is Anumula Revanth Reddy. He was appointed as the Chief Minister by the Congress party and took oath on December 7, 2023. Anumula Revanth Reddy is the President of the Telangana Pradesh Congress Committee (TPCC). He has been actively involved in politics and has a strong background in serving the people of Telangana. Anumula Revanth Reddy is known for his dedication to fulfilling the aspirations of the people and is focused on the development of the state.' response_metadata={'token_usage': {'completion_tokens': 115, 'prompt_tokens': 1899, 'total_tokens': 2014}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} name='Search' id='run-af45301a-dd25-46d7-ab22-2487f4b7fdfa-0'\n",
      "last_message \n",
      "\n",
      "\n",
      "{'Search': {'messages': [AIMessage(content='The current Chief Minister of Telangana, India is Anumula Revanth Reddy. He was appointed as the Chief Minister by the Congress party and took oath on December 7, 2023. Anumula Revanth Reddy is the President of the Telangana Pradesh Congress Committee (TPCC). He has been actively involved in politics and has a strong background in serving the people of Telangana. Anumula Revanth Reddy is known for his dedication to fulfilling the aspirations of the people and is focused on the development of the state.', response_metadata={'token_usage': {'completion_tokens': 115, 'prompt_tokens': 1899, 'total_tokens': 2014}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, name='Search', id='run-af45301a-dd25-46d7-ab22-2487f4b7fdfa-0')], 'sender': 'Search'}}\n",
      "----\n",
      "------>>>result   content='The current Chief Minister of Telangana, India is Anumula Revanth Reddy, appointed by the Congress party. He took office on December 7, 2023. Anumula Revanth Reddy is the President of the Telangana Pradesh Congress Committee (TPCC) and is dedicated to fulfilling the aspirations of the people of Telangana. He is known for his strong political background and commitment to the development of the state.' response_metadata={'token_usage': {'completion_tokens': 92, 'prompt_tokens': 1982, 'total_tokens': 2074}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-2fda295d-f148-4b8d-aede-8483ac3e4e5a-0'\n",
      "last_message \n",
      "\n",
      "\n",
      "content='The current Chief Minister of Telangana, India is Anumula Revanth Reddy, appointed by the Congress party. He took office on December 7, 2023. Anumula Revanth Reddy is the President of the Telangana Pradesh Congress Committee (TPCC) and is dedicated to fulfilling the aspirations of the people of Telangana. He is known for his strong political background and commitment to the development of the state.' response_metadata={'token_usage': {'completion_tokens': 92, 'prompt_tokens': 1982, 'total_tokens': 2074}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} name='Wiki' id='run-2fda295d-f148-4b8d-aede-8483ac3e4e5a-0'\n",
      "last_message \n",
      "\n",
      "\n",
      "{'Wiki': {'messages': [AIMessage(content='The current Chief Minister of Telangana, India is Anumula Revanth Reddy, appointed by the Congress party. He took office on December 7, 2023. Anumula Revanth Reddy is the President of the Telangana Pradesh Congress Committee (TPCC) and is dedicated to fulfilling the aspirations of the people of Telangana. He is known for his strong political background and commitment to the development of the state.', response_metadata={'token_usage': {'completion_tokens': 92, 'prompt_tokens': 1982, 'total_tokens': 2074}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, name='Wiki', id='run-2fda295d-f148-4b8d-aede-8483ac3e4e5a-0')], 'sender': 'Wiki'}}\n",
      "----\n",
      "------>>>result   content='Anumula Revanth Reddy is the current Chief Minister of Telangana, India. He was appointed by the Congress party and took office on December 7, 2023. Anumula Revanth Reddy is also the President of the Telangana Pradesh Congress Committee (TPCC) and is known for his dedication to fulfilling the aspirations of the people of Telangana. He has a strong political background and is focused on the development of the state.' response_metadata={'token_usage': {'completion_tokens': 96, 'prompt_tokens': 2116, 'total_tokens': 2212}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-fc3bde96-8c9b-4c7c-8dfa-fa4112373c12-0'\n",
      "last_message \n",
      "\n",
      "\n",
      "content='Anumula Revanth Reddy is the current Chief Minister of Telangana, India. He was appointed by the Congress party and took office on December 7, 2023. Anumula Revanth Reddy is also the President of the Telangana Pradesh Congress Committee (TPCC) and is known for his dedication to fulfilling the aspirations of the people of Telangana. He has a strong political background and is focused on the development of the state.' response_metadata={'token_usage': {'completion_tokens': 96, 'prompt_tokens': 2116, 'total_tokens': 2212}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} name='Search' id='run-fc3bde96-8c9b-4c7c-8dfa-fa4112373c12-0'\n",
      "last_message \n",
      "\n",
      "\n",
      "{'Search': {'messages': [AIMessage(content='Anumula Revanth Reddy is the current Chief Minister of Telangana, India. He was appointed by the Congress party and took office on December 7, 2023. Anumula Revanth Reddy is also the President of the Telangana Pradesh Congress Committee (TPCC) and is known for his dedication to fulfilling the aspirations of the people of Telangana. He has a strong political background and is focused on the development of the state.', response_metadata={'token_usage': {'completion_tokens': 96, 'prompt_tokens': 2116, 'total_tokens': 2212}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, name='Search', id='run-fc3bde96-8c9b-4c7c-8dfa-fa4112373c12-0')], 'sender': 'Search'}}\n",
      "----\n",
      "------>>>result   content=\"Anumula Revanth Reddy is the current Chief Minister of Telangana, India. He assumed office on December 7, 2023, after being appointed by the Congress party. Anumula Revanth Reddy is also the President of the Telangana Pradesh Congress Committee (TPCC). Known for his political background and commitment to the state's development, he aims to fulfill the aspirations of the people of Telangana.\" response_metadata={'token_usage': {'completion_tokens': 90, 'prompt_tokens': 2180, 'total_tokens': 2270}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-4a19cea2-b4db-4609-8c4e-f150f96cd43f-0'\n",
      "last_message \n",
      "\n",
      "\n",
      "content=\"Anumula Revanth Reddy is the current Chief Minister of Telangana, India. He assumed office on December 7, 2023, after being appointed by the Congress party. Anumula Revanth Reddy is also the President of the Telangana Pradesh Congress Committee (TPCC). Known for his political background and commitment to the state's development, he aims to fulfill the aspirations of the people of Telangana.\" response_metadata={'token_usage': {'completion_tokens': 90, 'prompt_tokens': 2180, 'total_tokens': 2270}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} name='Wiki' id='run-4a19cea2-b4db-4609-8c4e-f150f96cd43f-0'\n",
      "last_message \n",
      "\n",
      "\n",
      "{'Wiki': {'messages': [AIMessage(content=\"Anumula Revanth Reddy is the current Chief Minister of Telangana, India. He assumed office on December 7, 2023, after being appointed by the Congress party. Anumula Revanth Reddy is also the President of the Telangana Pradesh Congress Committee (TPCC). Known for his political background and commitment to the state's development, he aims to fulfill the aspirations of the people of Telangana.\", response_metadata={'token_usage': {'completion_tokens': 90, 'prompt_tokens': 2180, 'total_tokens': 2270}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, name='Wiki', id='run-4a19cea2-b4db-4609-8c4e-f150f96cd43f-0')], 'sender': 'Wiki'}}\n",
      "----\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 12\u001b[0m\n\u001b[1;32m      1\u001b[0m events \u001b[38;5;241m=\u001b[39m graph\u001b[38;5;241m.\u001b[39mstream(\n\u001b[1;32m      2\u001b[0m     {\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecursion_limit\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m20\u001b[39m},\n\u001b[1;32m     11\u001b[0m )\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m events:\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(s)\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m----\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/langgraph/pregel/__init__.py:963\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, stream_mode, output_keys, input_keys, interrupt_before, interrupt_after, debug)\u001b[0m\n\u001b[1;32m    960\u001b[0m         \u001b[38;5;28;01mdel\u001b[39;00m fut, task\n\u001b[1;32m    962\u001b[0m \u001b[38;5;66;03m# panic on failure or timeout\u001b[39;00m\n\u001b[0;32m--> 963\u001b[0m _panic_or_proceed(done, inflight, step)\n\u001b[1;32m    964\u001b[0m \u001b[38;5;66;03m# don't keep futures around in memory longer than needed\u001b[39;00m\n\u001b[1;32m    965\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m done, inflight, futures\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1489\u001b[0m, in \u001b[0;36m_panic_or_proceed\u001b[0;34m(done, inflight, step)\u001b[0m\n\u001b[1;32m   1487\u001b[0m             inflight\u001b[38;5;241m.\u001b[39mpop()\u001b[38;5;241m.\u001b[39mcancel()\n\u001b[1;32m   1488\u001b[0m         \u001b[38;5;66;03m# raise the exception\u001b[39;00m\n\u001b[0;32m-> 1489\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m   1491\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inflight:\n\u001b[1;32m   1492\u001b[0m     \u001b[38;5;66;03m# if we got here means we timed out\u001b[39;00m\n\u001b[1;32m   1493\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m inflight:\n\u001b[1;32m   1494\u001b[0m         \u001b[38;5;66;03m# cancel all pending tasks\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/langgraph/pregel/retry.py:66\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy)\u001b[0m\n\u001b[1;32m     64\u001b[0m task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m task\u001b[38;5;241m.\u001b[39mproc\u001b[38;5;241m.\u001b[39minvoke(task\u001b[38;5;241m.\u001b[39minput, task\u001b[38;5;241m.\u001b[39mconfig)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# if successful, end\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/langchain_core/runnables/base.py:2393\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   2391\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2392\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps):\n\u001b[0;32m-> 2393\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[1;32m   2394\u001b[0m             \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   2395\u001b[0m             \u001b[38;5;66;03m# mark each step as a child run\u001b[39;00m\n\u001b[1;32m   2396\u001b[0m             patch_config(\n\u001b[1;32m   2397\u001b[0m                 config, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq:step:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2398\u001b[0m             ),\n\u001b[1;32m   2399\u001b[0m         )\n\u001b[1;32m   2400\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   2401\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/langgraph/utils.py:95\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m accepts_config(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc):\n\u001b[1;32m     94\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config\n\u001b[0;32m---> 95\u001b[0m     ret \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc, \u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse:\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "Cell \u001b[0;32mIn[35], line 2\u001b[0m, in \u001b[0;36magent_node\u001b[0;34m(state, agent, name)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21magent_node\u001b[39m(state,agent,name):\n\u001b[0;32m----> 2\u001b[0m     result \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39minvoke(state)\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m------>>>result  \u001b[39m\u001b[38;5;124m\"\u001b[39m,result)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result,ToolMessage):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/langchain_core/runnables/base.py:2393\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   2391\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2392\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps):\n\u001b[0;32m-> 2393\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[1;32m   2394\u001b[0m             \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   2395\u001b[0m             \u001b[38;5;66;03m# mark each step as a child run\u001b[39;00m\n\u001b[1;32m   2396\u001b[0m             patch_config(\n\u001b[1;32m   2397\u001b[0m                 config, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq:step:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2398\u001b[0m             ),\n\u001b[1;32m   2399\u001b[0m         )\n\u001b[1;32m   2400\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   2401\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/langchain_core/runnables/base.py:4427\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   4421\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m   4422\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4423\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[1;32m   4424\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   4425\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   4426\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[0;32m-> 4427\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbound\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[1;32m   4428\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   4429\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_configs(config),\n\u001b[1;32m   4430\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs},\n\u001b[1;32m   4431\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:170\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    166\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    167\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    169\u001b[0m         ChatGeneration,\n\u001b[0;32m--> 170\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[1;32m    171\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[1;32m    172\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m    173\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    174\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    175\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    176\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    177\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    178\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    179\u001b[0m         )\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    180\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:599\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    592\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    593\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    597\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    598\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 599\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_messages, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:456\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    454\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    455\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 456\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    457\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    458\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[1;32m    459\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    460\u001b[0m ]\n\u001b[1;32m    461\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:446\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    445\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 446\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_with_cache(\n\u001b[1;32m    447\u001b[0m                 m,\n\u001b[1;32m    448\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m    449\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[i] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    450\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    451\u001b[0m             )\n\u001b[1;32m    452\u001b[0m         )\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    454\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:671\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    670\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 671\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[1;32m    672\u001b[0m             messages, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    673\u001b[0m         )\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    675\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/langchain_openai/chat_models/base.py:522\u001b[0m, in \u001b[0;36mBaseChatOpenAI._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    520\u001b[0m message_dicts, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_message_dicts(messages, stop)\n\u001b[1;32m    521\u001b[0m params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[0;32m--> 522\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mcreate(messages\u001b[38;5;241m=\u001b[39mmessage_dicts, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_result(response)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/openai/_utils/_utils.py:277\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 277\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/openai/resources/chat/completions.py:579\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    549\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    550\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    577\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    578\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 579\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[1;32m    580\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    581\u001b[0m         body\u001b[38;5;241m=\u001b[39mmaybe_transform(\n\u001b[1;32m    582\u001b[0m             {\n\u001b[1;32m    583\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[1;32m    584\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[1;32m    585\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[1;32m    586\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[1;32m    587\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[1;32m    588\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[1;32m    589\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[1;32m    590\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[1;32m    591\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[1;32m    592\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[1;32m    593\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[1;32m    594\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[1;32m    595\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[1;32m    596\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[1;32m    597\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[1;32m    598\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[1;32m    599\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[1;32m    600\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[1;32m    601\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[1;32m    602\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[1;32m    603\u001b[0m             },\n\u001b[1;32m    604\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[1;32m    605\u001b[0m         ),\n\u001b[1;32m    606\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[1;32m    607\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m    608\u001b[0m         ),\n\u001b[1;32m    609\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[1;32m    610\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    611\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mStream[ChatCompletionChunk],\n\u001b[1;32m    612\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:1240\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1226\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1227\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1228\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1235\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1236\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1237\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1238\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1239\u001b[0m     )\n\u001b[0;32m-> 1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls))\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:921\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    913\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    914\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    919\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    920\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 921\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m    922\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m    923\u001b[0m         options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m    924\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    925\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m    926\u001b[0m         remaining_retries\u001b[38;5;241m=\u001b[39mremaining_retries,\n\u001b[1;32m    927\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:1005\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1003\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m   1004\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1005\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[1;32m   1006\u001b[0m         options,\n\u001b[1;32m   1007\u001b[0m         cast_to,\n\u001b[1;32m   1008\u001b[0m         retries,\n\u001b[1;32m   1009\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m   1010\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1011\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1012\u001b[0m     )\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:1053\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1050\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1051\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1053\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m   1054\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   1055\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1056\u001b[0m     remaining_retries\u001b[38;5;241m=\u001b[39mremaining,\n\u001b[1;32m   1057\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1058\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1059\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:1005\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1003\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m   1004\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1005\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[1;32m   1006\u001b[0m         options,\n\u001b[1;32m   1007\u001b[0m         cast_to,\n\u001b[1;32m   1008\u001b[0m         retries,\n\u001b[1;32m   1009\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m   1010\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1011\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1012\u001b[0m     )\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:1053\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1050\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1051\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1053\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m   1054\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   1055\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1056\u001b[0m     remaining_retries\u001b[38;5;241m=\u001b[39mremaining,\n\u001b[1;32m   1057\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1058\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1059\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:1020\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1017\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1019\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1020\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1022\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1023\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1024\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1027\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1028\u001b[0m )\n",
      "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "source": [
    "events = graph.stream(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content=\"Find out the Current Chief Minister of Telangana, India and write a 5 liner about him.\"\n",
    "            )\n",
    "        ],\n",
    "    },\n",
    "    # Maximum number of steps to take in the graph\n",
    "    {\"recursion_limit\": 20},\n",
    ")\n",
    "for s in events:\n",
    "    print(s)\n",
    "    print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m llm\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHi\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:170\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    166\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    167\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    169\u001b[0m         ChatGeneration,\n\u001b[0;32m--> 170\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[1;32m    171\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[1;32m    172\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m    173\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    174\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    175\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    176\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    177\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    178\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    179\u001b[0m         )\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    180\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:599\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    592\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    593\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    597\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    598\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 599\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_messages, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:456\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    454\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    455\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 456\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    457\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    458\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[1;32m    459\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    460\u001b[0m ]\n\u001b[1;32m    461\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:446\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    445\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 446\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_with_cache(\n\u001b[1;32m    447\u001b[0m                 m,\n\u001b[1;32m    448\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m    449\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[i] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    450\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    451\u001b[0m             )\n\u001b[1;32m    452\u001b[0m         )\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    454\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:671\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    670\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 671\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[1;32m    672\u001b[0m             messages, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    673\u001b[0m         )\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    675\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/langchain_openai/chat_models/base.py:522\u001b[0m, in \u001b[0;36mBaseChatOpenAI._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    520\u001b[0m message_dicts, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_message_dicts(messages, stop)\n\u001b[1;32m    521\u001b[0m params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[0;32m--> 522\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mcreate(messages\u001b[38;5;241m=\u001b[39mmessage_dicts, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_result(response)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/openai/_utils/_utils.py:277\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 277\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/openai/resources/chat/completions.py:579\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    549\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    550\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    577\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    578\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 579\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[1;32m    580\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    581\u001b[0m         body\u001b[38;5;241m=\u001b[39mmaybe_transform(\n\u001b[1;32m    582\u001b[0m             {\n\u001b[1;32m    583\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[1;32m    584\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[1;32m    585\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[1;32m    586\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[1;32m    587\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[1;32m    588\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[1;32m    589\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[1;32m    590\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[1;32m    591\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[1;32m    592\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[1;32m    593\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[1;32m    594\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[1;32m    595\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[1;32m    596\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[1;32m    597\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[1;32m    598\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[1;32m    599\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[1;32m    600\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[1;32m    601\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[1;32m    602\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[1;32m    603\u001b[0m             },\n\u001b[1;32m    604\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[1;32m    605\u001b[0m         ),\n\u001b[1;32m    606\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[1;32m    607\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m    608\u001b[0m         ),\n\u001b[1;32m    609\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[1;32m    610\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    611\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mStream[ChatCompletionChunk],\n\u001b[1;32m    612\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:1240\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1226\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1227\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1228\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1235\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1236\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1237\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1238\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1239\u001b[0m     )\n\u001b[0;32m-> 1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls))\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:921\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    913\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    914\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    919\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    920\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 921\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m    922\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m    923\u001b[0m         options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m    924\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    925\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m    926\u001b[0m         remaining_retries\u001b[38;5;241m=\u001b[39mremaining_retries,\n\u001b[1;32m    927\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:1005\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1003\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m   1004\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1005\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[1;32m   1006\u001b[0m         options,\n\u001b[1;32m   1007\u001b[0m         cast_to,\n\u001b[1;32m   1008\u001b[0m         retries,\n\u001b[1;32m   1009\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m   1010\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1011\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1012\u001b[0m     )\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:1053\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1050\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1051\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1053\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m   1054\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   1055\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1056\u001b[0m     remaining_retries\u001b[38;5;241m=\u001b[39mremaining,\n\u001b[1;32m   1057\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1058\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1059\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:1005\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1003\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m   1004\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1005\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[1;32m   1006\u001b[0m         options,\n\u001b[1;32m   1007\u001b[0m         cast_to,\n\u001b[1;32m   1008\u001b[0m         retries,\n\u001b[1;32m   1009\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m   1010\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1011\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1012\u001b[0m     )\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:1053\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1050\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1051\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1053\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m   1054\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   1055\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1056\u001b[0m     remaining_retries\u001b[38;5;241m=\u001b[39mremaining,\n\u001b[1;32m   1057\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1058\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1059\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:1020\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1017\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1019\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1020\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1022\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1023\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1024\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1027\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1028\u001b[0m )\n",
      "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "source": [
    "llm.invoke(\"Hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
