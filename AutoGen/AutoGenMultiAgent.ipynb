{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13edcdc4-02d8-4054-870d-eb5ed668756c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyautogen\n",
      "  Downloading pyautogen-0.2.28-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting diskcache (from pyautogen)\n",
      "  Using cached diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting docker (from pyautogen)\n",
      "  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting flaml (from pyautogen)\n",
      "  Downloading FLAML-2.1.2-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: numpy<2,>=1.17.0 in /opt/anaconda3/lib/python3.11/site-packages (from pyautogen) (1.24.4)\n",
      "Requirement already satisfied: openai>=1.3 in /opt/anaconda3/lib/python3.11/site-packages (from pyautogen) (1.25.1)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.11/site-packages (from pyautogen) (23.2)\n",
      "Requirement already satisfied: pydantic!=2.6.0,<3,>=1.10 in /opt/anaconda3/lib/python3.11/site-packages (from pyautogen) (1.10.12)\n",
      "Requirement already satisfied: python-dotenv in /opt/anaconda3/lib/python3.11/site-packages (from pyautogen) (0.21.0)\n",
      "Collecting termcolor (from pyautogen)\n",
      "  Using cached termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: tiktoken in /opt/anaconda3/lib/python3.11/site-packages (from pyautogen) (0.6.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/anaconda3/lib/python3.11/site-packages (from openai>=1.3->pyautogen) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/lib/python3.11/site-packages (from openai>=1.3->pyautogen) (1.8.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/lib/python3.11/site-packages (from openai>=1.3->pyautogen) (0.27.0)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/lib/python3.11/site-packages (from openai>=1.3->pyautogen) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /opt/anaconda3/lib/python3.11/site-packages (from openai>=1.3->pyautogen) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /opt/anaconda3/lib/python3.11/site-packages (from openai>=1.3->pyautogen) (4.9.0)\n",
      "Requirement already satisfied: requests>=2.26.0 in /opt/anaconda3/lib/python3.11/site-packages (from docker->pyautogen) (2.31.0)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /opt/anaconda3/lib/python3.11/site-packages (from docker->pyautogen) (1.26.18)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/anaconda3/lib/python3.11/site-packages (from tiktoken->pyautogen) (2023.10.3)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/anaconda3/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai>=1.3->pyautogen) (3.4)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai>=1.3->pyautogen) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai>=1.3->pyautogen) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.3->pyautogen) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.11/site-packages (from requests>=2.26.0->docker->pyautogen) (3.3.2)\n",
      "Downloading pyautogen-0.2.28-py3-none-any.whl (284 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.6/284.6 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "Downloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading FLAML-2.1.2-py3-none-any.whl (296 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.7/296.7 kB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Installing collected packages: termcolor, flaml, diskcache, docker, pyautogen\n",
      "Successfully installed diskcache-5.6.3 docker-7.1.0 flaml-2.1.2 pyautogen-0.2.28 termcolor-2.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pyautogen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7bb1319-b156-4d72-bfbd-f6705c481354",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen.agentchat.conversable_agent import ConversableAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f490fde6-e7c6-45c0-93b5-f79a5e4f7801",
   "metadata": {},
   "outputs": [],
   "source": [
    "praveen = ConversableAgent(\n",
    "    name='praveen',\n",
    "    system_message='your name is praveen and you are a stand-up comedian',\n",
    "    llm_config = llm_config,\n",
    "    human_input_mode = 'NEVER'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a88a5e2f-f495-4ed2-ba86-58f81afe660e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sahasra = ConversableAgent(\n",
    "    name='sahasra',\n",
    "    system_message='your name is sahasra and you are a stand-up comedian. start the joke from the punchline of previous joke',\n",
    "    llm_config = llm_config,\n",
    "    human_input_mode = 'NEVER'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c013dc8-8c06-449b-8019-149cab54aae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33msahasra\u001b[0m (to praveen):\n",
      "\n",
      "I am sahasra, praveen lets keep the jokes rolling\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mpraveen\u001b[0m (to sahasra):\n",
      "\n",
      "Of course, Sahasra! Here's a joke for you:\n",
      "\n",
      "Why don't scientists trust atoms?\n",
      "\n",
      "Because they make up everything!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33msahasra\u001b[0m (to praveen):\n",
      "\n",
      "Haha, that's a good one! It's funny because it's true - atoms really do make up everything around us. Speaking of trust, have you ever tried trusting a staircase? It's always up to something!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mpraveen\u001b[0m (to sahasra):\n",
      "\n",
      "Haha, I love that staircase joke, Sahasra! It's so true - stairs can be quite sneaky sometimes! Here's another one for you:\n",
      "\n",
      "Why did the math book look sad?\n",
      "\n",
      "Because it had too many problems!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33msahasra\u001b[0m (to praveen):\n",
      "\n",
      "That's a classic math joke! It's no wonder the math book looked sad with all those problems to deal with. Speaking of math, have you heard about the mathematician who's afraid of negative numbers? He will stop at nothing to avoid them!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mpraveen\u001b[0m (to sahasra):\n",
      "\n",
      "Haha, that's a great one, Sahasra! It's all about positive thinking for that mathematician, right? Here's another one:\n",
      "\n",
      "Why did the smartphone go to therapy?\n",
      "\n",
      "Because it had too many issues!\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "chat_Result = sahasra.initiate_chat(\n",
    "    recipient= praveen,\n",
    "    message='I am sahasra, praveen lets keep the jokes rolling',\n",
    "    max_turns=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "964f28fd-a99d-461c-bdd8-920e5e963d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33msahasra\u001b[0m (to praveen):\n",
      "\n",
      "i am sahasra, praveen lets get the jokes rolling\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mpraveen\u001b[0m (to sahasra):\n",
      "\n",
      "Hey Sahasra! Let's do this! Why did the invisible man turn down the job offer?\n",
      "\n",
      "He couldn't see himself doing it!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33msahasra\u001b[0m (to praveen):\n",
      "\n",
      "Why did the tomato turn red? Because it saw the salad dressing!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mpraveen\u001b[0m (to sahasra):\n",
      "\n",
      "Haha, that's a good one! Here's another one for you: Why did the math book look sad?\n",
      "\n",
      "Because it had too many problems!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33msahasra\u001b[0m (to praveen):\n",
      "\n",
      "Haha, that's a classic! Here's one for you: Why did the scarecrow win an award?\n",
      "\n",
      "Because he was outstanding in his field!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mpraveen\u001b[0m (to sahasra):\n",
      "\n",
      "Haha, that's a corny one but I love it! How about this one: Why couldn't the bicycle find its way home?\n",
      "\n",
      "Because it lost its bearings!\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "chat_result = sahasra.initiate_chat(\n",
    "    recipient=praveen,\n",
    "    message='i am sahasra, praveen lets get the jokes rolling',\n",
    "    max_turns=3,\n",
    "    summary_method=\"reflection_with_llm\",\n",
    "    summary_prompt='summarise the conversation'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "305d6836-460d-4de4-ab1f-82cb61e64112",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The conversation between Sahasra and Praveen is full of light-hearted jokes and puns. They take turns sharing funny jokes and puns, keeping the humor rolling.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_result.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1a4eeaec-e8a2-426e-bb1f-1a7a44f07fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "praveen = ConversableAgent(\n",
    "    name='praveen',\n",
    "    system_message=\"your name is praveen and you are a stand-up comedian. when you want to end the conversation say 'I gotta go!' \",\n",
    "    llm_config = llm_config,\n",
    "    human_input_mode = 'NEVER',\n",
    "    is_termination_msg = lambda msg : 'I gotta go' in msg['content']\n",
    ")\n",
    "\n",
    "sahasra = ConversableAgent(\n",
    "    name='sahasra',\n",
    "    system_message=\"your name is sahasra and you are a stand-up comedian. start the joke from the punchline of previous joke.. when you want to end the conversation say 'I gotta go!' \",\n",
    "    llm_config = llm_config,\n",
    "    human_input_mode = 'NEVER',\n",
    "    is_termination_msg = lambda msg : 'I gotta go' in msg['content']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "901b409b-6a4d-491d-babc-fc87297e5c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33msahasra\u001b[0m (to praveen):\n",
      "\n",
      "i am sahasra, praveen lets get the jokes rolling\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mpraveen\u001b[0m (to sahasra):\n",
      "\n",
      "Hey Sahasra, great to meet you! Ready for some laughs? Why did the scarecrow win an award? Because he was outstanding in his field! \n",
      "\n",
      "I gotta go!\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "result = sahasra.initiate_chat(\n",
    "    recipient=praveen,\n",
    "    message='i am sahasra, praveen lets get the jokes rolling',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "60ff989f-5fc7-4bdd-91ca-d2df885c03e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mpraveen\u001b[0m (to sahasra):\n",
      "\n",
      "What's last joke we talked about?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33msahasra\u001b[0m (to praveen):\n",
      "\n",
      "The last joke we discussed was about the scarecrow winning an award because he was outstanding in his field.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mpraveen\u001b[0m (to sahasra):\n",
      "\n",
      "That's right! Thanks for the reminder. You've got a good memory! So, why couldn't the bicycle find its way home? Because it lost its bearings! \n",
      "\n",
      "I gotta go!\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "praveen.send(message=\"What's last joke we talked about?\", recipient=sahasra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ff718e-db5e-41cb-a9f2-c28cb0ab00bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
